{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Up and Downsampling Functions for Single Comparison Wrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_vector_model2(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer):\n",
    "    \n",
    "    '''\n",
    "    oversamples from the minoriy class (label = 1) until it is the size of the minority class (label = 0)\n",
    "\n",
    "    '''\n",
    "    #upsample data \n",
    "    X_train_col_up, y_train_up = upsample_training_data2(X_train_col, y_train)\n",
    "    \n",
    "    # perform vectorization\n",
    "    X_train_up_transformed = vectorizer.fit_transform(X_train_col_up.values.ravel())\n",
    "    X_val_transformed = vectorizer.transform(X_val_col)\n",
    "                \n",
    "    classifier.fit(X_train_up_transformed, y_train_up.values.ravel())\n",
    "    \n",
    "    #train and validate classifier\n",
    "    y_train_up_pred = classifier.predict(X_train_up_transformed)\n",
    "    y_val_pred = classifier.predict(X_val_transformed)\n",
    "    \n",
    "    y_train_up_pred_prob = classifier.predict_proba(X_train_up_transformed)\n",
    "    y_val_pred_prob = classifier.predict_proba(X_val_transformed)\n",
    "    \n",
    "    # print scores  \n",
    "    metrics_dict = compare_classification_metrics(y_train_up, y_train_up_pred, y_val, y_val_pred)\n",
    "    \n",
    "#     conf_matrix = print_confusion_matrix(y_val, y_val_pred)\n",
    "    \n",
    "    compare_predictions = compare_predictions_dataframe(y_val, y_val_pred, y_val_pred_prob)\n",
    "                     \n",
    "    return X_train_up_transformed, X_val_transformed, pd.DataFrame(y_train_up_pred), pd.DataFrame(y_val_pred), \\\n",
    "    pd.DataFrame(y_val_pred_prob), compare_predictions, metrics_dict #,conf_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_vector_model2(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer):\n",
    " \n",
    "    '''\n",
    "    downsamples from the majority class (label = 0) until it is the size of the minority class (label = 1)\n",
    "\n",
    "    '''\n",
    "   #downsample data \n",
    "    X_train_col_down, y_train_down = downsample_training_data2(X_train_col, y_train)\n",
    "\n",
    "    #perform vectorization\n",
    "    X_train_down_transformed = vectorizer.fit_transform(X_train_col_down.values.ravel())\n",
    "    X_val_transformed = vectorizer.transform(X_val_col.values.ravel())\n",
    "                \n",
    "    classifier.fit(X_train_down_transformed, y_train_down.values.ravel())\n",
    "    \n",
    "    #train and validate classifier\n",
    "    y_train_down_pred = classifier.predict(X_train_down_transformed)\n",
    "    y_val_pred = classifier.predict(X_val_transformed)\n",
    "    \n",
    "    y_train_down_pred_prob = classifier.predict_proba(X_train_down_transformed)\n",
    "    y_val_pred_prob = classifier.predict_proba(X_val_transformed)\n",
    "    \n",
    "    # print scores  \n",
    "    metrics_dict = compare_classification_metrics(y_train_down, y_train_down_pred, y_val, y_val_pred)\n",
    "    \n",
    "#     conf_matrix = print_confusion_matrix(y_val, y_val_pred.ravel())\n",
    "    \n",
    "    compare_predictions = compare_predictions_dataframe(y_val, y_val_pred, y_val_pred_prob)\n",
    "                     \n",
    "    return X_train_down_transformed, X_val_transformed, pd.DataFrame(y_train_down_pred), pd.DataFrame(y_val_pred),\\\n",
    "    pd.DataFrame(y_val_pred_prob), compare_predictions, metrics_dict #,conf_matrix      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_vector_model(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer):\n",
    "    \n",
    "    '''\n",
    "    oversamples from the minoriy class (label = 1) until it is the size of the minority class (label = 0)\n",
    "\n",
    "    '''\n",
    "    #upsample data \n",
    "    training_data = pd.DataFrame(X_train_col)\n",
    "    training_data['label']= y_train\n",
    "\n",
    "    train_0 = training_data[training_data.label==0]\n",
    "    train_1 = training_data[training_data.label==1]\n",
    "\n",
    "    train_1_up = resample(train_1, replace = True, n_samples=len(train_0), random_state=10)\n",
    "\n",
    "    train_upsampled = pd.concat([train_1_up, train_0])\n",
    "\n",
    "    X_train_col_up = train_upsampled.drop(['label'], axis = 1)\n",
    "    y_train_up = train_upsampled.label\n",
    "    \n",
    "    # perform vectorization\n",
    "    X_train_up_transformed = vectorizer.fit_transform(X_train_col_up.values.ravel())\n",
    "    X_val_transformed = vectorizer.transform(X_val_col)\n",
    "                \n",
    "    classifier.fit(X_train_up_transformed, y_train_up.values.ravel())\n",
    "    \n",
    "    #train and validate classifier\n",
    "    y_train_up_pred = classifier.predict(X_train_up_transformed)\n",
    "    y_val_pred = classifier.predict(X_val_transformed)\n",
    "    \n",
    "    y_train_up_pred_prob = classifier.predict_proba(X_train_up_transformed)\n",
    "    y_val_pred_prob = classifier.predict_proba(X_val_transformed)\n",
    "    \n",
    "    # print scores  \n",
    "    metrics_dict = compare_classification_metrics(y_train_up, y_train_up_pred, y_val, y_val_pred)\n",
    "    \n",
    "    conf_matrix = print_confusion_matrix(y_val, y_val_pred)\n",
    "    \n",
    "    compare_predictions = compare_predictions_dataframe(y_val, y_val_pred, y_val_pred_prob)\n",
    "                     \n",
    "    return X_train_up_transformed, X_val_transformed, pd.DataFrame(y_train_up_pred), pd.DataFrame(y_val_pred), pd.DataFrame(y_val_pred_prob), conf_matrix, metrics_dict, compare_predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_vector_model(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer):\n",
    " \n",
    "    '''\n",
    "    downsamples from the majority class (label = 0) until it is the size of the minority class (label = 1)\n",
    "\n",
    "    '''\n",
    "    training_data = pd.DataFrame(X_train_col)\n",
    "    training_data['label']= y_train\n",
    "    \n",
    "    train_0 = training_data[training_data.label==0]\n",
    "    train_1 = training_data[training_data.label==1]\n",
    "\n",
    "    train_0_down = resample(train_0, replace=True, n_samples=len(train_1),random_state=10)\n",
    "\n",
    "    train_downsampled = pd.concat([train_0_down, train_1])\n",
    "    \n",
    "    X_train_col_down = train_downsampled.drop(['label'], axis = 1)\n",
    "    y_train_down = train_downsampled.label\n",
    "\n",
    "    #perform vectorization\n",
    "    X_train_down_transformed = vectorizer.fit_transform(X_train_col_down.values.ravel())\n",
    "    X_val_transformed = vectorizer.transform(X_val_col)\n",
    "                \n",
    "    classifier.fit(X_train_down_transformed, y_train_down.values.ravel())\n",
    "    \n",
    "    #train and validate classifier\n",
    "    y_train_down_pred = classifier.predict(X_train_down_transformed)\n",
    "    y_val_pred = classifier.predict(X_val_transformed)\n",
    "    \n",
    "    y_train_down_pred_prob = classifier.predict_proba(X_train_down_transformed)\n",
    "    y_val_pred_prob = classAifier.predict_proba(X_val_transformed)\n",
    "    \n",
    "    # print scores  \n",
    "    metrics_dict = compare_classification_metrics(y_train_down, y_train_down_pred, y_val, y_val_pred)\n",
    "    \n",
    "    conf_matrix = print_confusion_matrix(y_val, y_val_pred)\n",
    "    \n",
    "    compare_predictions = compare_predictions_dataframe(y_val, y_val_pred, y_val_pred_prob)\n",
    "                     \n",
    "    return X_train_down_transformed, X_val_transformed, pd.DataFrame(y_train_down_pred), pd.DataFrame(y_val_pred), pd.DataFrame(y_val_pred_prob), conf_matrix, metrics_dict, compare_predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to View Classifier Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compare prediction results\n",
    "\n",
    "def compare_predictions_dataframe (y_true, y_pred, y_pred_prob):\n",
    "\n",
    "    pred_df = pd.DataFrame(y_pred_prob).copy()\n",
    "    pred_df['predicted_class'] = y_pred\n",
    "    pred_df.index = y_true.index \n",
    "    pred_df['actual_class'] = y_true\n",
    "    \n",
    "    return pred_df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### confusion matrix\n",
    "\n",
    "def print_confusion_matrix (y_true, y_pred):\n",
    "    \n",
    "    cm = pd.DataFrame(confusion_matrix((y_true, y_pred), index = ['actual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])) \n",
    "\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_report(y_true, y_pred):\n",
    "    \n",
    "    report = print(metrics.classification_report(y_true, y_pred, target_names = ['not hate', 'hate speech']))\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_classification_metrics (y_train, y_train_pred, y_val, y_val_pred):\n",
    "\n",
    "    metrics_dict = {\n",
    "    'Train Accuracy' : round(metrics.accuracy_score(y_train, y_train_pred),2),\n",
    "    'Train Precision' : round(metrics.precision_score(y_train, y_train_pred),2),\n",
    "    'Train Recall' : round(metrics.recall_score(y_train, y_train_pred),2),\n",
    "    'Train F1': round(metrics.f1_score(y_train, y_train_pred),2),\n",
    "\n",
    "    'Validation Accuracy': round(metrics.accuracy_score(y_val, y_val_pred),2),\n",
    "    'Validation Precision' : round(metrics.precision_score(y_val, y_val_pred),2),\n",
    "    'Validation Recall': round(metrics.recall_score(y_val, y_val_pred),2),\n",
    "    'Validation F1': round(metrics.f1_score(y_val, y_val_pred),2)\n",
    "    }\n",
    "    \n",
    "    return metrics_dict   #  to return as a dataframe: pd.DataFrame(metrics_dict, index=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print AUC ROC curve\n",
    "\n",
    "def graph_roc_curve(X_train_transformed, X_val_transformed, y_train, y_val):\n",
    "\n",
    "    y_val_score = log.decision_function(X_val_transformed)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, y_val_score)\n",
    "    y_train_score = log.decision_function(X_train_transformed)\n",
    "    train_fpr, train_tpr, thresholds = roc_curve(y_train, y_train_score)\n",
    "    \n",
    "    average_precision = average_precision_score(y_val, y_val_pred)\n",
    "\n",
    "    #plot curve\n",
    "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "    plt.figure(figsize=(8,6))\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.yticks([i/20.0 for i in range(21)])\n",
    "    plt.xticks([i/20.0 for i in range(21)])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic (ROC) Curve for Validation Set')\n",
    "\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "    print('Average precision-recall score RF: {}'.format(average_precision))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redoing Simple Functions to be Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_vector_model2(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer):\n",
    "    \n",
    "    '''\n",
    "    Apply the specified text vectorizer,make predictions and calculate scores with specified classifier.\n",
    "\n",
    "    No explicit correction for class imbalances is conducted in this function, \n",
    "    but up or downsampled X_train and y_train variables can be passed as arguments\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train_col: specify cleaned text column to be used for vectorization and predictions\n",
    "\n",
    "    y_train: enter as a one-dimensional vector; function transforms into an array\n",
    "\n",
    "    X_val_col: specify cleaned text column to be used for vectorization and predictions\n",
    "\n",
    "    y_val:  enter as a one-dimensional vector; function transforms into an array\n",
    "\n",
    "    vectorizer: indicate text vectorization method; uses default parameters if none are specified\n",
    "\n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "\n",
    "    '''\n",
    "    \n",
    "    X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "    X_val_transformed = vectorizer.transform(X_val_col)\n",
    "            \n",
    "    classifier.fit(X_train_transformed, y_train.values.ravel())\n",
    "    \n",
    "    y_train_pred = classifier.predict(X_train_transformed)\n",
    "    y_val_pred = classifier.predict(X_val_transformed)\n",
    "    \n",
    "    # y_train_pred_prob = model.predict_proba(X_train_transformed)\n",
    "    y_val_pred_prob = classifier.predict_proba(X_val_transformed)\n",
    "    \n",
    "    # print scores  \n",
    "    metrics_dict = compare_classification_metrics(y_train, y_train_pred, y_val, y_val_pred)\n",
    "    \n",
    "#     conf_matrix = print_confusion_matrix(y_val, y_val_pred)\n",
    "    \n",
    "    predictions = compare_predictions_dataframe(y_val, y_val_pred, y_val_pred_prob)\n",
    "                     \n",
    "    return X_train_transformed, X_val_transformed, pd.DataFrame(y_train_pred), pd.DataFrame(y_val_pred), pd.DataFrame(y_val_pred_prob), metrics_dict, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_vector_model2(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer):\n",
    "    '''\n",
    "   Returns five values: \n",
    "   \n",
    "   X_val_transformed, metrics_dict, confusion_val, pd.DataFrame(val_predictions), pd.DataFrame(val_pred_prob), pred_df\n",
    "   \n",
    "    Apply the specified text vectorizer, use SMOTE to rebalance class sizes, \n",
    "    and then make predictions and calculate scores for specified classifier\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train_col: specify cleaned text column to be used for vectorization and predictions\n",
    "\n",
    "    y_train: enter as a one-dimensional vector; function transforms into an array\n",
    "\n",
    "    X_val_col: specify cleaned text column to be used for vectorization and predictions\n",
    "\n",
    "    y_val_col:  enter as a one-dimensional vector; function transforms into an array\n",
    "\n",
    "    vectorizer: indicate text vectorization method; uses default parameters if none are specified\n",
    "\n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "\n",
    "    '''\n",
    "    \n",
    "    X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "    X_val_transformed = vectorizer.transform(X_val_col)\n",
    "    \n",
    "    smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "    \n",
    "    pipe = make_pipeline(smote, classifier) \n",
    "    \n",
    "    model = pipe.fit(X_train_transformed, y_train.values.ravel())\n",
    "    \n",
    "    y_train_pred = model.predict(X_train_transformed)\n",
    "    y_val_pred = model.predict (X_val_transformed)\n",
    "    \n",
    "#   y_train_pred_prob = model.predict_proba(X_train_transformed)\n",
    "    y_val_pred_prob = model.predict_proba(X_val_transformed)\n",
    "    \n",
    " # print scores  \n",
    "    metrics_dict = compare_classification_metrics(y_train, y_train_pred, y_val, y_val_pred)\n",
    "    \n",
    "#     conf_matrix = print_confusion_matrix(y_val, y_val_pred)\n",
    "    \n",
    "    predictions = compare_predictions_dataframe(y_val, y_val_pred, y_val_pred_prob)\n",
    "                     \n",
    "    return X_train_transformed, X_val_transformed, pd.DataFrame(y_train_pred), pd.DataFrame(y_val_pred), pd.DataFrame(y_val_pred_prob), metrics_dict, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Compare Classifiers and Vectorization Methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_vector_model(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer):\n",
    "    \n",
    "    '''\n",
    "    Apply the specified text vectorizer,make predictions and calculate scores with specified classifier.\n",
    "\n",
    "    No explicit correction for class imbalances is conducted in this function, \n",
    "    but up or downsampled X_train and y_train variables can be passed as arguments\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train_col: specify cleaned text column to be used for vectorization and predictions\n",
    "\n",
    "    y_train: enter as a one-dimensional vector; function transforms into an array\n",
    "\n",
    "    X_val_col: specify cleaned text column to be used for vectorization and predictions\n",
    "\n",
    "    y_val:  enter as a one-dimensional vector; function transforms into an array\n",
    "\n",
    "    vectorizer: indicate text vectorization method; uses default parameters if none are specified\n",
    "\n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "\n",
    "    '''\n",
    "    \n",
    "    X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "    X_val_transformed = vectorizer.transform(X_val_col)\n",
    "            \n",
    "    classifier.fit(X_train_transformed, y_train.values.ravel())\n",
    "    \n",
    "    y_train_pred = classifier.predict(X_train_transformed)\n",
    "    y_val_pred = classifier.predict (X_val_transformed)\n",
    "    \n",
    "#   y_train_pred_prob = model.predict_proba(X_train_transformed)\n",
    "    y_val_pred_prob = classifier.predict_proba(X_val_transformed)\n",
    "    \n",
    "    # print scores  \n",
    "    metrics_dict = {\n",
    "    'Train Accuracy' : round(metrics.accuracy_score(y_train, y_train_pred),2),\n",
    "    'Train Precision' : round(metrics.precision_score(y_train, y_train_pred),2),\n",
    "    'Train Recall' : round(metrics.recall_score(y_train, y_train_pred),2),\n",
    "    'Train F1': round(metrics.f1_score(y_train, y_train_pred),2),\n",
    "\n",
    "    'Validation Accuracy': round(metrics.accuracy_score(y_val, y_val_pred),2),\n",
    "    'Validation Precision' : round(metrics.precision_score(y_val, y_val_pred),2),\n",
    "    'Validation Recall': round(metrics.recall_score(y_val, y_val_pred),2),\n",
    "    'Validation F1': round(metrics.f1_score(y_val, y_val_pred),2)\n",
    "        }\n",
    "\n",
    "#     confusion_train = pd.crosstab(y_train, train_predictions, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    confusion_val = pd.DataFrame(confusion_matrix(y_val, y_val_pred), index = ['actual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])\n",
    "    \n",
    "    pred_df = pd.DataFrame(y_val_pred_prob)\n",
    "    pred_df.index = y_val.index \n",
    "    pred_df['predicted_class'] = y_val_pred\n",
    "    pred_df['actual_class'] = y_val\n",
    "    \n",
    "    return X_val_transformed, metrics_dict, confusion_val, pd.DataFrame(y_val_pred), pd.DataFrame(y_val_pred_prob), pred_df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_vector_model(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer):\n",
    "    '''\n",
    "   Returns five values: \n",
    "   \n",
    "   X_val_transformed, metrics_dict, confusion_val, pd.DataFrame(val_predictions), pd.DataFrame(val_pred_prob), pred_df\n",
    "   \n",
    "    Apply the specified text vectorizer, use SMOTE to rebalance class sizes, \n",
    "    and then make predictions and calculate scores for specified classifier\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train_col: specify cleaned text column to be used for vectorization and predictions\n",
    "\n",
    "    y_train: enter as a one-dimensional vector; function transforms into an array\n",
    "\n",
    "    X_val_col: specify cleaned text column to be used for vectorization and predictions\n",
    "\n",
    "    y_val_col:  enter as a one-dimensional vector; function transforms into an array\n",
    "\n",
    "    vectorizer: indicate text vectorization method; uses default parameters if none are specified\n",
    "\n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "\n",
    "    '''\n",
    "    \n",
    "    X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "    X_val_transformed = vectorizer.transform(X_val_col)\n",
    "    \n",
    "    smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "    \n",
    "    pipe = make_pipeline(smote, classifier) \n",
    "    \n",
    "    model = pipe.fit(X_train_transformed, y_train.values.ravel())\n",
    "    \n",
    "    y_train_pred = model.predict(X_train_transformed)\n",
    "    y_val_pred = model.predict (X_val_transformed)\n",
    "    \n",
    "#     y_train_pred_prob = model.predict_proba(X_train_transformed)\n",
    "    y_val_pred_prob = model.predict_proba(X_val_transformed)\n",
    "    \n",
    "# print scores  \n",
    "    metrics_dict = {\n",
    "    'Train Accuracy' : round(metrics.accuracy_score(y_train, y_train_pred),2),\n",
    "    'Train Precision' : round(metrics.precision_score(y_train, y_train_pred),2),\n",
    "    'Train Recall' : round(metrics.recall_score(y_train, y_train_pred),2),\n",
    "    'Train F1': round(metrics.f1_score(y_train, y_train_pred),2),\n",
    "\n",
    "    'Validation Accuracy': round(metrics.accuracy_score(y_val, y_val_pred),2),\n",
    "    'Validation Precision' : round(metrics.precision_score(y_val, y_val_pred),2),\n",
    "    'Validation Recall': round(metrics.recall_score(y_val, y_val_pred),2),\n",
    "    'Validation F1': round(metrics.f1_score(y_val, y_val_pred),2)\n",
    "        }\n",
    "\n",
    "#     confusion_train = pd.crosstab(y_train, train_predictions, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    confusion_val = pd.DataFrame(confusion_matrix(y_val, y_val_pred), index = ['actual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])\n",
    "    \n",
    "    pred_df = pd.DataFrame(y_val_pred_prob)\n",
    "    pred_df.index = y_val.index \n",
    "    pred_df['predicted_class'] = y_val_pred\n",
    "    pred_df['actual_class'] = y_val\n",
    "    \n",
    "    return X_val_transformed, metrics_dict, confusion_val, pd.DataFrame(y_val_pred), pd.DataFrame(y_val_pred_prob), pred_df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_vectorization_model(X_train_col, y_train, X_val_col, y_val, classifier, vectorization_list):\n",
    "    '''\n",
    "    Compares classification model performance using different text vectorizers,\n",
    "    (declared in 'vectorization list') outside the function.\n",
    "        \n",
    "    Parameters:\n",
    "    \n",
    "    X_train_col: cleaned text column in training set\n",
    "    \n",
    "    y_train: target variable in training set\n",
    "    \n",
    "    X_val_col: cleaned text column in validation set\n",
    "    \n",
    "    y_val: target variable in validation set \n",
    "    \n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "    \n",
    "    vectorization_list: list of tuples specficifying each name and vectorization method to be used\n",
    "\n",
    "    '''\n",
    "        \n",
    "    metrics_dict = {}\n",
    "        \n",
    "    for name, vectorizer in vectorization_list:\n",
    "                \n",
    "        X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "        X_val_transformed = vectorizer.transform (X_val_col)\n",
    "\n",
    "        classifier.fit(X_train_transformed, y_train.values.ravel())\n",
    "    \n",
    "        y_train_pred = classifier.predict (X_train_transformed)\n",
    "        y_val_pred = classifier.predict (X_val_transformed)   \n",
    "    \n",
    "        metrics_dict = {\n",
    "        'Train Accuracy' : round(metrics.accuracy_score(y_train, y_train_pred),2),\n",
    "        'Train Precision' : round(metrics.precision_score(y_train, y_train_pred),2),\n",
    "        'Train Recall' : round(metrics.recall_score(y_train, y_train_pred),2),\n",
    "        'Train F1': round(metrics.f1_score(y_train, y_train_pred),2),\n",
    "\n",
    "        'Validation Accuracy': round(metrics.accuracy_score(y_val, y_val_pred),2),\n",
    "        'Validation Precision' : round(metrics.precision_score(y_val, y_val_pred),2),\n",
    "        'Validation Recall': round(metrics.recall_score(y_val, y_val_pred),2),\n",
    "        'Validation F1': round(metrics.f1_score(y_val, y_val_pred),2)\n",
    "        }\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_compare_vectorization_model(X_train_col, y_train, X_val_col, y_val, classifier, vectorization_list):\n",
    "  \n",
    "    '''\n",
    "    Compares the performance of a single classifier using different text vectorization methods.\n",
    "\n",
    "    Uses SMOTE to rebalance class sizes before predictions are made and scored.\n",
    "\n",
    "    Parameters:\n",
    "    \n",
    "    X_train_col: cleaned text column in training set\n",
    "    \n",
    "    y_train: target variable in training set\n",
    "    \n",
    "    X_val_col: cleaned text column in validation set\n",
    "    \n",
    "    y_val: target variable in validation set \n",
    "    \n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "    \n",
    "    vectorization_list: list of tuples specficifying each name and vectorization method to be used\n",
    "\n",
    "    '''   \n",
    "    metrics_dict = {}\n",
    "        \n",
    "    for name, vectorizer in vectorization_list:\n",
    "              \n",
    "        X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "        X_val_transformed = vectorizer.transform (X_val_col)\n",
    "        \n",
    "        smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "    \n",
    "        pipe = make_pipeline(smote, classifier) \n",
    "    \n",
    "        model = pipe.fit(X_train_transformed, y_train.values.ravel())\n",
    "    \n",
    "        y_train_pred = model.predict(X_train_transformed)\n",
    "        y_val_pred = model.predict (X_val_transformed)\n",
    "    \n",
    "        metrics_dict = {\n",
    "        'Train Accuracy' : round(metrics.accuracy_score(y_train, y_train_pred),2),\n",
    "        'Train Precision' : round(metrics.precision_score(y_train, y_train_pred),2),\n",
    "        'Train Recall' : round(metrics.recall_score(y_train, y_train_pred),2),\n",
    "        'Train F1': round(metrics.f1_score(y_train, y_train_pred),2),\n",
    "\n",
    "        'Validation Accuracy': round(metrics.accuracy_score(y_val, y_val_pred),2),\n",
    "        'Validation Precision' : round(metrics.precision_score(y_val, y_val_pred),2),\n",
    "        'Validation Recall': round(metrics.recall_score(y_val, y_val_pred),2),\n",
    "        'Validation F1': round(metrics.f1_score(y_val, y_val_pred),2)\n",
    "        }\n",
    "        \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_compare_vectorization_model(X_train_col, y_train, X_val_col, y_val, classifier):\n",
    "    \n",
    "    '''\n",
    "    Adapts the 'compare vectorization model' function to work specifically for Naive Bayes by transforming the\n",
    "    sparse vector matrices to dense numpy arrays\n",
    "\n",
    "    No explicit correction for class imbalance is made in this function, \n",
    "    but up or downsampled data can be passed in as arguments.\n",
    "\n",
    "    Vectorization methods should be specified outside the function in a 'vectorization list',\n",
    "    which is a list of tuples specifying each name and vectorization method to be used.\n",
    "\n",
    "    Parameters:\n",
    "    \n",
    "    X_train_col: cleaned text column in training set\n",
    "    \n",
    "    y_train: target variable in training set\n",
    "    \n",
    "    X_val_col: cleaned text column in validation set\n",
    "    \n",
    "    y_val: target variable in validation set \n",
    "    \n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "\n",
    "    '''    \n",
    " \n",
    "    metrics_dict = {}\n",
    "\n",
    "    for name, vectorizer in vectorization_list:\n",
    "    \n",
    "        X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "        X_val_transformed = vectorizer.transform (X_val_col)\n",
    "        \n",
    "        X_train_transformed = X_train_transformed.toarray()\n",
    "        X_val_transformed = X_val_transformed.toarray()\n",
    "        \n",
    "        classifier.fit(X_train_transformed, y_train.values.ravel())\n",
    "    \n",
    "        train_predictions = classifier.predict (X_train_transformed)\n",
    "        val_predictions = classifier.predict (X_val_transformed)   \n",
    "    \n",
    "    #       print scores  \n",
    "        print ('The performance of the {} is:'.format((name)))\n",
    "        print('Train Accuracy: ' + str(round(metrics.accuracy_score(y_train, train_predictions),2)))\n",
    "        print('Train Precision: ' + str(round(metrics.precision_score(y_train, train_predictions),2)))\n",
    "        print('Train Recall: ' + str(round(metrics.recall_score(y_train, train_predictions),2)))\n",
    "        print('Train F1: ' + str(round(metrics.f1_score(y_train,train_predictions),2)))\n",
    "        print('\\n')\n",
    "        print('Validation Accuracy: ' + str(round(metrics.accuracy_score(y_val, val_predictions),2)))\n",
    "        print('Validation Precision: ' + str(round(metrics.precision_score(y_val, val_predictions),2)))\n",
    "        print('Validation Recall: ' + str(round(metrics.recall_score(y_val, val_predictions),2)))\n",
    "        print('Validation F1: ' + str(round(metrics.f1_score(y_val, val_predictions),2)))\n",
    "        print('\\n')\n",
    "        \n",
    "   \n",
    "        metrics_dict[name] = {\n",
    "        'Train Accuracy' : metrics.accuracy_score(y_train, train_predictions),\n",
    "        'Train Precision' : metrics.precision_score(y_train, train_predictions),\n",
    "        'Train Recall' : metrics.recall_score(y_train, train_predictions),\n",
    "        'Train F1': metrics.f1_score(y_train,train_predictions),\n",
    "        \n",
    "        'Validation Accuracy': metrics.accuracy_score(y_val, val_predictions),\n",
    "        'Validation Precision' : metrics.precision_score(y_val, val_predictions),\n",
    "        'Validation Recall': metrics.recall_score(y_val, val_predictions),\n",
    "        'Validation F1': metrics.f1_score(y_val, val_predictions)\n",
    "        }\n",
    "        \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_single_vectorization(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer, apply_smote = False):\n",
    "    \n",
    "    '''\n",
    "    Apply the specified text vectorizer and and then make predictions and calculate scores for specified classifier\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train_col: specify cleaned text column to be used for vectorization and predictions\n",
    "\n",
    "    y_train: enter as a one-dimensional vector; function transforms into an array\n",
    "\n",
    "    X_val_col: specify cleaned text column to be used for vectorization and predictions\n",
    "\n",
    "    y_val_col:  enter as a one-dimensional vector; function transforms into an array\n",
    "\n",
    "    vectorizer: indicate text vectorization method; uses default parameters if none are specified\n",
    "\n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "    \n",
    "    apply_smote: if True, then perform smote before running classifiction model\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if apply_smote == False:\n",
    "        \n",
    "        metrics_dict = single_vector_model(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer)\n",
    "                        \n",
    "    else:\n",
    "        \n",
    "        metrics_dict = smote_vector_model(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer)\n",
    "                \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_compare_vectorizations(X_train_col, y_train, X_val_col, y_val, classifier, vectorization_list, apply_smote = False):\n",
    "    '''\n",
    "    Compares the performance of a single classifier using different text vectorization methods.\n",
    "\n",
    "    Uses SMOTE to rebalance class sizes before predictions are made and scored.\n",
    "\n",
    "    Parameters:\n",
    "    \n",
    "    X_train_col: cleaned text column in training set\n",
    "    \n",
    "    y_train: target variable in training set\n",
    "    \n",
    "    X_val_col: cleaned text column in validation set\n",
    "    \n",
    "    y_val: target variable in validation set \n",
    "    \n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "    \n",
    "    vectorization_list: list of tuples specficifying each name and vectorization method to be used\n",
    "\n",
    "    '''       \n",
    "    if apply_smote == True:\n",
    "        \n",
    "        metrics_dict = smote_compare_vectorization_model(X_train_col, y_train, X_val_col, y_val, classifier, vectorization_list)\n",
    "                        \n",
    "    else:\n",
    "        \n",
    "        metrics_dict = compare_vectorization_model(X_train_col, y_train, X_val_col, y_val, classifier, vectorization_list)\n",
    "                \n",
    "    return metrics_dict\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    '''\n",
    "    Plots the word embeddings created by the word2vec model\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    model: variable name for word2vec model\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=250, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "#tnse_plot(model_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_vectors(sentence, model_name):\n",
    "    '''\n",
    "    Calculate the mean word embedding for every sentence in the dataset.\n",
    "        \n",
    "    Parameters:\n",
    "    \n",
    "    wordlist: list of the list of words in each sentence\n",
    "    \n",
    "    size: size of hidden layer\n",
    "    \n",
    "    model_name: name of wv2 model\n",
    "    \n",
    "    '''\n",
    "\n",
    "    sumvec = np.zeros(shape = (1,100))\n",
    "    wordcnt = 0\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word in model_name:\n",
    "            sumvec += model_name[word]\n",
    "            wordcnt +=1\n",
    "    \n",
    "    if wordcnt == 0:\n",
    "        return sumvec\n",
    "    \n",
    "    else:\n",
    "        return sumvec / wordcnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_w2v_model (X_train_w2v, y_train, X_val_w2v, y_val, classifier):\n",
    "        \n",
    "    smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "        \n",
    "    pipe = make_pipeline(smote, classifier) \n",
    "    \n",
    "    model = pipe.fit(X_train_w2v, y_train)\n",
    "    \n",
    "    train_predictions = model.predict(X_train_w2v_2)\n",
    "    val_predictions = model.predict (X_val_w2v_2)\n",
    "    \n",
    "   # print scores  \n",
    "    print('Train Accuracy: ' + str(round(metrics.accuracy_score(y_train, train_predictions),2)))\n",
    "    print('Train Precision: ' + str(round(metrics.precision_score(y_train, train_predictions),2)))\n",
    "    print('Train Recall: ' + str(round(metrics.recall_score(y_train, train_predictions),2)))\n",
    "    print('Train F1: ' + str(round(metrics.f1_score(y_train, train_predictions),2)))\n",
    "    print('\\n')\n",
    "    print('Validation Accuracy: ' + str(round(metrics.accuracy_score(y_val, val_predictions),2)))\n",
    "    print('Validation Precision: ' + str(round(metrics.precision_score(y_val, val_predictions),2)))\n",
    "    print('Validation Recall: ' + str(round(metrics.recall_score(y_val, val_predictions),2)))\n",
    "    print('Validation F1: ' + str(round(metrics.f1_score(y_val, val_predictions),2)))\n",
    "\n",
    "    confusion_test = pd.crosstab(y_val, val_predictions, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    return confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_smote_w2v_model(X_train_w2v, y_train, X_val_w2v, y_val, classifier):\n",
    "    \n",
    "    pca = decomposition.PCA(n_components=50)\n",
    "    \n",
    "    smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "        \n",
    "    pipe = make_pipeline(pca, smote, classifier) \n",
    "    \n",
    "    model = pipe.fit(X_train_w2v, y_train)\n",
    "    \n",
    "    train_predictions = model.predict(X_train_w2v)\n",
    "    val_predictions = model.predict (X_val_w2v)\n",
    "    \n",
    "   # print scores  \n",
    "    print('Train Accuracy: ' + str(round(metrics.accuracy_score(y_train, train_predictions),2)))\n",
    "    print('Train Precision: ' + str(round(metrics.precision_score(y_train, train_predictions),2)))\n",
    "    print('Train Recall: ' + str(round(metrics.recall_score(y_train, train_predictions),2)))\n",
    "    print('Train F1: ' + str(round(metrics.f1_score(y_train, train_predictions),2)))\n",
    "    print('\\n')\n",
    "    print('Validation Accuracy: ' + str(round(metrics.accuracy_score(y_val, val_predictions),2)))\n",
    "    print('Validation Precision: ' + str(round(metrics.precision_score(y_val, val_predictions),2)))\n",
    "    print('Validation Recall: ' + str(round(metrics.recall_score(y_val, val_predictions),2)))\n",
    "    print('Validation F1: ' + str(round(metrics.f1_score(y_val, val_predictions),2)))\n",
    "\n",
    "    log_confusion_test = pd.crosstab(y_val, val_predictions, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    return log_confusion_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up Wrapper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_single_vectorization2(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer, sampling = None):\n",
    "    \n",
    "    '''\n",
    "    Apply the specified text vectorizer and and then make predictions and calculate scores for specified classifier\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train_col: specify cleaned text column to be used for vectorization and predictions\n",
    "\n",
    "    y_train: enter as a one-dimensional vector; function transforms into an array\n",
    "\n",
    "    X_val_col: specify cleaned text column to be used for vectorization and predictions\n",
    "\n",
    "    y_val_col:  enter as a one-dimensional vector; function transforms into an array\n",
    "\n",
    "    vectorizer: indicate text vectorization method; uses default parameters if none are specified\n",
    "\n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "    \n",
    "    apply_smote: if True, then perform smote before running classification model\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if sampling is None:\n",
    "    \n",
    "        results = single_vector_model2(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer)\n",
    "    \n",
    "    elif sampling is 'smote':\n",
    "        \n",
    "        results = smote_vector_model2(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer)\n",
    "\n",
    "    elif sampling is 'upsample':\n",
    "        \n",
    "        results = upsample_vector_model2(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer)\n",
    "                        \n",
    "    elif sampling is 'downsample':\n",
    "        \n",
    "        results = downsample_vector_model2(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        results = print('Sampling method has been incorrectly assigned. Options are: smote, upsample or downsample')\n",
    "                \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_compare_vectorizations2(X_train_col, y_train, X_val_col, y_val, classifier, vectorization_list, sampling = None):\n",
    "\n",
    "    if sampling is None:\n",
    "            \n",
    "        metrics_dict = compare_vectorization_model(X_train_col, y_train, X_val_col, y_val, classifier, vectorization_list)\n",
    "        \n",
    "    elif sampling is 'smote':\n",
    "        \n",
    "        metrics_dict = smote_compare_vectorization_model(X_train_col, y_train, X_val_col, y_val, classifier, vectorization_list)\n",
    "                        \n",
    "    elif sampling is 'upsample':\n",
    "        \n",
    "        metrics_dict = upsample_training_data (X_train_col, y_train)\n",
    "        \n",
    "    elif sampling is 'downsample':\n",
    "        \n",
    "        metrics_dict = downsample_training_data (X_train_col, y_train)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        metrics_dict = print('Sampling method has been incorrectly assigned. Options are: smote, upsample or downsample')\n",
    "        \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling and Downsampling Full Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_training_data(X_train, y_train):\n",
    "    \n",
    "    '''\n",
    "    draws samples from the minoriy class (label = 1) until it is the size of the minority class (label = 0)\n",
    "\n",
    "    returns a single upsampled dataframe (with both X_train and y_train in one dataframe)\n",
    "\n",
    "    '''\n",
    "    training_data = X_train.copy()\n",
    "    training_data['label']= y_train\n",
    "\n",
    "    train_0 = training_data[training_data.label==0]\n",
    "    train_1 = training_data[training_data.label==1]\n",
    "\n",
    "    train_1_up = resample(train_1, \n",
    "          replace=True,    \n",
    "          n_samples=len(train_0),   \n",
    "          random_state=10)\n",
    "\n",
    "    train_upsampled = pd.concat([train_1_up, train_0])\n",
    "    \n",
    "    return train_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_training_data2(X_train_col, y_train):\n",
    "    \n",
    "    '''\n",
    "    draws samples from the minoriy class (label = 1) until it is the size of the minority class (label = 0)\n",
    "\n",
    "    returns a single upsampled dataframe (with both X_train and y_train in one dataframe)\n",
    "\n",
    "    '''\n",
    "    training_data = pd.DataFrame(X_train_col)\n",
    "    training_data['label']= y_train\n",
    "\n",
    "    train_0 = training_data[training_data.label==0]\n",
    "    train_1 = training_data[training_data.label==1]\n",
    "\n",
    "    train_1_up = resample(train_1, \n",
    "          replace=True,    \n",
    "          n_samples=len(train_0),   \n",
    "          random_state=10)\n",
    "\n",
    "    train_upsampled = pd.concat([train_1_up, train_0])\n",
    "    \n",
    "    X_train_col_up = train_upsampled.drop(['label'], axis = 1)\n",
    "    y_train_up = train_upsampled.label\n",
    "    \n",
    "    return X_train_col_up, y_train_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_training_data2(X_train_col, y_train):\n",
    "    \n",
    "    '''\n",
    "    draws samples from the majority class (label = 0) until it is the size of the minority class (label = 1)\n",
    "\n",
    "    returns a single downsampled dataframe (with both X_train and y_train in one dataframe)\n",
    "\n",
    "    '''\n",
    "    training_data = pd.DataFrame(X_train_col)\n",
    "    training_data['label']= y_train\n",
    "    \n",
    "    train_0 = training_data[training_data.label==0]\n",
    "    train_1 = training_data[training_data.label==1]\n",
    "\n",
    "    train_0_down = resample(train_0, \n",
    "          replace=True,    \n",
    "          n_samples=len(train_1),   \n",
    "          random_state=10)\n",
    "\n",
    "    train_downsampled = pd.concat([train_0_down, train_1])\n",
    "    \n",
    "    X_train_col_down = train_downsampled.drop(['label'], axis = 1)\n",
    "    y_train_down = train_downsampled.label\n",
    "    \n",
    "    return X_train_col_down, y_train_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_training_data(X_train, y_train):\n",
    "    \n",
    "    '''\n",
    "    draws samples from the majority class (label = 0) until it is the size of the minority class (label = 1)\n",
    "\n",
    "    returns a single downsampled dataframe (with both X_train and y_train in one dataframe)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    training_data = X_train.copy()\n",
    "    training_data['label']= y_train\n",
    "    \n",
    "    train_0 = training_data[training_data.label==0]\n",
    "    train_1 = training_data[training_data.label==1]\n",
    "\n",
    "    train_0_down = resample(train_0, \n",
    "          replace=True,    \n",
    "          n_samples=len(train_1),   \n",
    "          random_state=10)\n",
    "\n",
    "    train_downsampled = pd.concat([train_0_down, train_1])\n",
    "    \n",
    "    return train_downsampled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
