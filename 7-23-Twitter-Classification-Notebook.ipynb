{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import sklearn\n",
    "\n",
    "# NLTK/NLP\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import string, re\n",
    "import urllib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from nltk.collocations import *\n",
    "import gensim\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Classifiers \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Sampling\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import sklearn.decomposition as decomposition\n",
    "\n",
    "#Visualization\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import csv\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.metrics import average_precision_score, auc, roc_curve, precision_recall_curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import customized functions\n",
    "# import import_ipynb\n",
    "# from custom_functions import *\n",
    "\n",
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [sad, see, the, scene, hooligan, pre, engrus, ...\n",
       "1        [gooddyeyoung, yoyoyo, super, happy, apa, the,...\n",
       "2        [queen, evil, bihday, lnic, lnicjustanevilbday...\n",
       "3        [you, might, libtard, libtard, sjw, liberal, p...\n",
       "4          [what, are, your, goal, find, out, here, smile]\n",
       "5                                   [retweets, nuascannan]\n",
       "6                               [classic, trump, follower]\n",
       "7        [the, mixture, emotion, here, one, from, the, ...\n",
       "8        [the, meps, bear, that, travelled, sandy, hook...\n",
       "9        [just, because, affected, her, son, son, abuse...\n",
       "10       [thomas, always, say, live, dream, world, this...\n",
       "11       [when, the, local, library, try, charge, you, ...\n",
       "12       [cia, nc, sockpuppetarmy, hbgary, troll, snowd...\n",
       "13       [year, anniversary, michael, jackson, vindicat...\n",
       "14       [over, everyone, that, say, cringe, cuz, cool,...\n",
       "15       [good, morning, life, blackhair, travel, home,...\n",
       "16       [yay, let, talk, food, the, every, day, the, m...\n",
       "17       [lightroom, bull, you, will, dominate, your, b...\n",
       "18       [seek, probe, into, udtapunjab, leak, point, f...\n",
       "19                                 [two, word, billy, bob]\n",
       "20       [lovenaturebeautygardenoutdoorsprettycutelovel...\n",
       "21                  [sad, hear, that, you, say, gentleman]\n",
       "22       [rise, today, out, story, police, brutality, f...\n",
       "23                [happy, day, pretty, playa, barceloneta]\n",
       "24       [sad, aspect, rule, bigbiz, company, fleeing, ...\n",
       "25       [thoughtcast, sac, photooftheday, music, insta...\n",
       "26                             [have, bribe, people, hang]\n",
       "27       [hello, tubbytoons, fun, bekindalways, animati...\n",
       "28       [for, the, follow, thought, and, prayer, with,...\n",
       "29               [you, dare, fight, hahahaha, puppy, cute]\n",
       "                               ...                        \n",
       "31932              [and, grateful, now, that, affirmation]\n",
       "31933    [had, blast, recording, with, gaana, sunke, sa...\n",
       "31934    [the, truth, about, sexualharassment, amp, lab...\n",
       "31935    [being, white, muslim, get, accepted, most, so...\n",
       "31936    [eur, usd, clear, barrier, jump, fresh, week, ...\n",
       "31937    [bihday, trump, make, america, great, amp, saf...\n",
       "31938                            [kingd, nawf, today, day]\n",
       "31939    [xbox, one, revers, drm, angry, rant, video, x...\n",
       "31940    [the, white, establishment, can, have, blk, fo...\n",
       "31941    [bounteous, summer, harvest, all, yum, gardening]\n",
       "31942        [thankful, for, laughter, thankful, positive]\n",
       "31943    [matter, who, you, are, take, care, your, self...\n",
       "31944    [honestly, woman, there, something, wrong, wit...\n",
       "31945    [dig, deep, you, will, find, feelit, beautiful...\n",
       "31946    [college, kid, graduation, celebrate, iran, al...\n",
       "31947    [you, might, libtard, libtard, sjw, liberal, p...\n",
       "31948    [damn, this, really, made, tear, little, mustr...\n",
       "31949                                             [heaven]\n",
       "31950        [le, than, one, week, rain, already, feeling]\n",
       "31951    [superdemoreo, always, when, newgi, come, into...\n",
       "31952                                [always, filmactions]\n",
       "31953             [goodnight, everyone, earlynight, sleep]\n",
       "31954    [scapelliti, progresverebel, good, riddance, b...\n",
       "31955    [healthandfitness, rooster, simulation, want, ...\n",
       "31956                                     [congrats, lola]\n",
       "31957    [wakemeupwhen, the, left, stop, being, hypocri...\n",
       "31958    [happy, anniversary, ruby, couple, charlotte, ...\n",
       "31959             [top, story, from, featuring, christmas]\n",
       "31960    [heard, back, from, casting, call, submitted, ...\n",
       "31961    [the, diversity, officer, state, among, other,...\n",
       "Name: lemmatized_tokens, Length: 31962, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/cleaned-reshuffled.csv')\n",
    "df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "df.lem_tweet= df.lem_tweet.apply(str)\n",
    "df.stem_tweet= df.stem_tweet.apply(str)\n",
    "df.tokenized_tweet.apply(eval)\n",
    "df.stemmed_tokens.apply(eval)\n",
    "df.lemmatized_tokens.apply(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into train and test \n",
    "X_model, X_test, y_model, y_test = train_test_split(X, y, stratify = y,  test_size=0.20, random_state=123)\n",
    "\n",
    "#splitting \"model\" into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_model, y_model, test_size=0.20, random_state=123)\n",
    "\n",
    "# df_train_full = X_train.copy()\n",
    "# df_train_full['label']= y_train\n",
    "# train_full_df.to_csv('train_full_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.929854\n",
       "1    0.070146\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling and Downsampling Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>no_hash_tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lem_tweet</th>\n",
       "      <th>stem_tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20676</td>\n",
       "      <td>@user f*** this  ð¦ðº government that deli...</td>\n",
       "      <td>this government that deliberately toures #refu...</td>\n",
       "      <td>this government that deliberately toures refug...</td>\n",
       "      <td>['this', 'government', 'that', 'deliberately',...</td>\n",
       "      <td>['this', 'govern', 'that', 'deliber', 'tour', ...</td>\n",
       "      <td>['this', 'government', 'that', 'deliberately',...</td>\n",
       "      <td>this government that deliberately toures refug...</td>\n",
       "      <td>this government that deliberately toures refug...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21531</th>\n",
       "      <td>24025</td>\n",
       "      <td>despite a demoralizing 2016: may ur #newyear20...</td>\n",
       "      <td>despite demoralizing may #newyear #classism fr...</td>\n",
       "      <td>despite demoralizing may newyear classism free...</td>\n",
       "      <td>['despite', 'demoralizing', 'may', 'newyear', ...</td>\n",
       "      <td>['despit', 'demor', 'may', 'newyear', 'classis...</td>\n",
       "      <td>['despite', 'demoralizing', 'may', 'newyear', ...</td>\n",
       "      <td>despite demoralizing may newyear classism free...</td>\n",
       "      <td>despite demoralizing may newyear classism free...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13300</th>\n",
       "      <td>18145</td>\n",
       "      <td>@user #koreans &amp;amp; joseon people in japan, w...</td>\n",
       "      <td>#koreans amp joseon people japan will abuse th...</td>\n",
       "      <td>koreans amp joseon people japan will abuse the...</td>\n",
       "      <td>['koreans', 'amp', 'joseon', 'people', 'japan'...</td>\n",
       "      <td>['korean', 'amp', 'joseon', 'peopl', 'japan', ...</td>\n",
       "      <td>['korean', 'amp', 'joseon', 'people', 'japan',...</td>\n",
       "      <td>koreans amp joseon people japan will abuse the...</td>\n",
       "      <td>koreans amp joseon people japan will abuse the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18925</th>\n",
       "      <td>8506</td>\n",
       "      <td>@user @user @user @user classic ! yet you jewi...</td>\n",
       "      <td>classic yet you jewish bastards wonder why you...</td>\n",
       "      <td>classic yet you jewish bastards wonder why you...</td>\n",
       "      <td>['classic', 'yet', 'you', 'jewish', 'bastards'...</td>\n",
       "      <td>['classic', 'yet', 'you', 'jewish', 'bastard',...</td>\n",
       "      <td>['classic', 'yet', 'you', 'jewish', 'bastard',...</td>\n",
       "      <td>classic yet you jewish bastards wonder why you...</td>\n",
       "      <td>classic yet you jewish bastards wonder why you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12619</th>\n",
       "      <td>15464</td>\n",
       "      <td>@user did someone say #antisemetic ? gee (((@u...</td>\n",
       "      <td>did someone say #antisemetic gee you bit trigg...</td>\n",
       "      <td>did someone say antisemetic gee you bit triggered</td>\n",
       "      <td>['did', 'someone', 'say', 'antisemetic', 'gee'...</td>\n",
       "      <td>['did', 'someon', 'say', 'antisemet', 'gee', '...</td>\n",
       "      <td>['did', 'someone', 'say', 'antisemetic', 'gee'...</td>\n",
       "      <td>did someone say antisemetic gee you bit triggered</td>\n",
       "      <td>did someone say antisemetic gee you bit trigg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26964</th>\n",
       "      <td>28937</td>\n",
       "      <td>couldn't have said this any better nor truthfu...</td>\n",
       "      <td>couldn have said this any better nor truthfull...</td>\n",
       "      <td>couldn have said this any better nor truthfull...</td>\n",
       "      <td>['couldn', 'have', 'said', 'this', 'any', 'bet...</td>\n",
       "      <td>['couldn', 'have', 'said', 'this', 'ani', 'bet...</td>\n",
       "      <td>['couldn', 'have', 'said', 'this', 'any', 'bet...</td>\n",
       "      <td>couldn have said this any better nor truthfull...</td>\n",
       "      <td>couldn have said this any better nor truthfull...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17273</th>\n",
       "      <td>25291</td>\n",
       "      <td>@user racism stuffed into skinny jeans with a ...</td>\n",
       "      <td>racism stuffed into skinny jeans with hipster ...</td>\n",
       "      <td>racism stuffed into skinny jeans with hipster ...</td>\n",
       "      <td>['racism', 'stuffed', 'into', 'skinny', 'jeans...</td>\n",
       "      <td>['racism', 'stuf', 'into', 'skinni', 'jean', '...</td>\n",
       "      <td>['racism', 'stuffed', 'into', 'skinny', 'jean'...</td>\n",
       "      <td>racism stuffed into skinny jeans with hipster ...</td>\n",
       "      <td>racism stuffed into skinny jeans with hipster ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>12717</td>\n",
       "      <td>the end of   #me #selfie # #love #messi #cr7 #...</td>\n",
       "      <td>the end #me #selfie #love #messi #cr #religion...</td>\n",
       "      <td>the end me selfie love messi cr religion chris...</td>\n",
       "      <td>['the', 'end', 'me', 'selfie', 'love', 'messi'...</td>\n",
       "      <td>['the', 'end', 'me', 'selfi', 'love', 'messi',...</td>\n",
       "      <td>['the', 'end', 'me', 'selfie', 'love', 'messi'...</td>\n",
       "      <td>the end me selfie love messi cr religion chris...</td>\n",
       "      <td>the end me selfie love messi cr religion chris...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17875</th>\n",
       "      <td>11612</td>\n",
       "      <td>trump ally wishes mad cow disease death for ob...</td>\n",
       "      <td>trump ally wishes mad cow disease death for ob...</td>\n",
       "      <td>trump ally wishes mad cow disease death for ob...</td>\n",
       "      <td>['trump', 'ally', 'wishes', 'mad', 'cow', 'dis...</td>\n",
       "      <td>['trump', 'alli', 'wish', 'mad', 'cow', 'disea...</td>\n",
       "      <td>['trump', 'ally', 'wish', 'mad', 'cow', 'disea...</td>\n",
       "      <td>trump ally wishes mad cow disease death for ob...</td>\n",
       "      <td>trump ally wishes mad cow disease death for ob...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17184</th>\n",
       "      <td>20554</td>\n",
       "      <td>opinion:  is rife in the #lgbt community. #gay...</td>\n",
       "      <td>opinion rife the #lgbt community #gay people c...</td>\n",
       "      <td>opinion rife the lgbt community gay people can...</td>\n",
       "      <td>['opinion', 'rife', 'the', 'lgbt', 'community'...</td>\n",
       "      <td>['opinion', 'rife', 'the', 'lgbt', 'communiti'...</td>\n",
       "      <td>['opinion', 'rife', 'the', 'lgbt', 'community'...</td>\n",
       "      <td>opinion rife the lgbt community gay people can...</td>\n",
       "      <td>opinion rife the lgbt community gay people can...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8573</th>\n",
       "      <td>25151</td>\n",
       "      <td>@user #allahsoil the cold war was fought over ...</td>\n",
       "      <td>#allahsoil the cold war was fought over oil #t...</td>\n",
       "      <td>allahsoil the cold war was fought over oil tea...</td>\n",
       "      <td>['allahsoil', 'the', 'cold', 'war', 'was', 'fo...</td>\n",
       "      <td>['allahsoil', 'the', 'cold', 'war', 'was', 'fo...</td>\n",
       "      <td>['allahsoil', 'the', 'cold', 'war', 'wa', 'fou...</td>\n",
       "      <td>allahsoil the cold war was fought over oil tea...</td>\n",
       "      <td>allahsoil the cold war was fought over oil tea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26779</th>\n",
       "      <td>13585</td>\n",
       "      <td>omg, these trump suppoers are deplorable! #dum...</td>\n",
       "      <td>omg these trump suppoers are deplorable #dumpt...</td>\n",
       "      <td>omg these trump suppoers are deplorable dumptr...</td>\n",
       "      <td>['omg', 'these', 'trump', 'suppoers', 'are', '...</td>\n",
       "      <td>['omg', 'these', 'trump', 'suppoer', 'are', 'd...</td>\n",
       "      <td>['omg', 'these', 'trump', 'suppoers', 'are', '...</td>\n",
       "      <td>omg these trump suppoers are deplorable dumptr...</td>\n",
       "      <td>omg these trump suppoers are deplorable dumptr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15332</th>\n",
       "      <td>2581</td>\n",
       "      <td>sea shepherd suppoers are racist!   #antiracis...</td>\n",
       "      <td>sea shepherd suppoers are racist #antiracism #...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>['sea', 'shepherd', 'suppoers', 'are', 'racist...</td>\n",
       "      <td>['sea', 'shepherd', 'suppoer', 'are', 'racist'...</td>\n",
       "      <td>['sea', 'shepherd', 'suppoers', 'are', 'racist...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12290</th>\n",
       "      <td>6519</td>\n",
       "      <td>.@user @user while @user can use phrases like ...</td>\n",
       "      <td>while can use phrases like #sandniggers accept...</td>\n",
       "      <td>while can use phrases like sandniggers acceptable</td>\n",
       "      <td>['while', 'can', 'use', 'phrases', 'like', 'sa...</td>\n",
       "      <td>['while', 'can', 'use', 'phrase', 'like', 'san...</td>\n",
       "      <td>['while', 'can', 'use', 'phrase', 'like', 'san...</td>\n",
       "      <td>while can use phrases like sandniggers acceptable</td>\n",
       "      <td>while can use phrases like sandniggers accept</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24159</th>\n",
       "      <td>9563</td>\n",
       "      <td>@user #allahsoil enlightenment is wasted on th...</td>\n",
       "      <td>#allahsoil enlightenment wasted the wilfully b...</td>\n",
       "      <td>allahsoil enlightenment wasted the wilfully bl...</td>\n",
       "      <td>['allahsoil', 'enlightenment', 'wasted', 'the'...</td>\n",
       "      <td>['allahsoil', 'enlighten', 'wast', 'the', 'wil...</td>\n",
       "      <td>['allahsoil', 'enlightenment', 'wasted', 'the'...</td>\n",
       "      <td>allahsoil enlightenment wasted the wilfully bl...</td>\n",
       "      <td>allahsoil enlightenment wasted the wilfully bl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25895</th>\n",
       "      <td>8452</td>\n",
       "      <td>@user here comes a  #supermistict douchebag wh...</td>\n",
       "      <td>here comes #supermistict douchebag who can onl...</td>\n",
       "      <td>here comes supermistict douchebag who can only...</td>\n",
       "      <td>['here', 'comes', 'supermistict', 'douchebag',...</td>\n",
       "      <td>['here', 'come', 'supermistict', 'douchebag', ...</td>\n",
       "      <td>['here', 'come', 'supermistict', 'douchebag', ...</td>\n",
       "      <td>here comes supermistict douchebag who can only...</td>\n",
       "      <td>here comes supermistict douchebag who can only...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23934</th>\n",
       "      <td>17750</td>\n",
       "      <td>@user hidden  in #america is as rampant as bla...</td>\n",
       "      <td>hidden #america rampant blatant racism</td>\n",
       "      <td>hidden america rampant blatant racism</td>\n",
       "      <td>['hidden', 'america', 'rampant', 'blatant', 'r...</td>\n",
       "      <td>['hidden', 'america', 'rampant', 'blatant', 'r...</td>\n",
       "      <td>['hidden', 'america', 'rampant', 'blatant', 'r...</td>\n",
       "      <td>hidden america rampant blatant racism</td>\n",
       "      <td>hidden america rampant blatant rac</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29174</th>\n",
       "      <td>21389</td>\n",
       "      <td>will the alt-right promote a new kind of  gene...</td>\n",
       "      <td>will the alt right promote new kind genetics</td>\n",
       "      <td>will the alt right promote new kind genetics</td>\n",
       "      <td>['will', 'the', 'alt', 'right', 'promote', 'ne...</td>\n",
       "      <td>['will', 'the', 'alt', 'right', 'promot', 'new...</td>\n",
       "      <td>['will', 'the', 'alt', 'right', 'promote', 'ne...</td>\n",
       "      <td>will the alt right promote new kind genetics</td>\n",
       "      <td>will the alt right promote new kind genet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10474</th>\n",
       "      <td>23431</td>\n",
       "      <td>#us why weneed #empathy #ageoftrump #grassroot...</td>\n",
       "      <td>#us why weneed #empathy #ageoftrump #grassroot...</td>\n",
       "      <td>us why weneed empathy ageoftrump grassrootsact...</td>\n",
       "      <td>['us', 'why', 'weneed', 'empathy', 'ageoftrump...</td>\n",
       "      <td>['us', 'whi', 'wene', 'empathi', 'ageoftrump',...</td>\n",
       "      <td>['u', 'why', 'weneed', 'empathy', 'ageoftrump'...</td>\n",
       "      <td>us why weneed empathy ageoftrump grassrootsact...</td>\n",
       "      <td>us why weneed empathy ageoftrump grassrootsact...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30743</th>\n",
       "      <td>12301</td>\n",
       "      <td>black trump suppoer smacks down cnn repoer for...</td>\n",
       "      <td>black trump suppoer smacks down cnn repoer for...</td>\n",
       "      <td>black trump suppoer smacks down cnn repoer for...</td>\n",
       "      <td>['black', 'trump', 'suppoer', 'smacks', 'down'...</td>\n",
       "      <td>['black', 'trump', 'suppoer', 'smack', 'down',...</td>\n",
       "      <td>['black', 'trump', 'suppoer', 'smack', 'down',...</td>\n",
       "      <td>black trump suppoer smacks down cnn repoer for...</td>\n",
       "      <td>black trump suppoer smacks down cnn repoer for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6190</th>\n",
       "      <td>19407</td>\n",
       "      <td>so sick of all the pre-programmed #hillbots rh...</td>\n",
       "      <td>sick all the pre programmed #hillbots rhetoric...</td>\n",
       "      <td>sick all the pre programmed hillbots rhetoric ...</td>\n",
       "      <td>['sick', 'all', 'the', 'pre', 'programmed', 'h...</td>\n",
       "      <td>['sick', 'all', 'the', 'pre', 'program', 'hill...</td>\n",
       "      <td>['sick', 'all', 'the', 'pre', 'programmed', 'h...</td>\n",
       "      <td>sick all the pre programmed hillbots rhetoric ...</td>\n",
       "      <td>sick all the pre programmed hillbots rhetoric ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19746</th>\n",
       "      <td>406</td>\n",
       "      <td>@user when you're blocked by a  troll because ...</td>\n",
       "      <td>when you blocked troll because you promise #bl...</td>\n",
       "      <td>when you blocked troll because you promise bla...</td>\n",
       "      <td>['when', 'you', 'blocked', 'troll', 'because',...</td>\n",
       "      <td>['when', 'you', 'block', 'troll', 'becaus', 'y...</td>\n",
       "      <td>['when', 'you', 'blocked', 'troll', 'because',...</td>\n",
       "      <td>when you blocked troll because you promise bla...</td>\n",
       "      <td>when you blocked troll because you promise bla...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19581</th>\n",
       "      <td>11720</td>\n",
       "      <td>@user @user you forgot #democrate, because vet...</td>\n",
       "      <td>you forgot #democrate because vetting people f...</td>\n",
       "      <td>you forgot democrate because vetting people fr...</td>\n",
       "      <td>['you', 'forgot', 'democrate', 'because', 'vet...</td>\n",
       "      <td>['you', 'forgot', 'democr', 'becaus', 'vet', '...</td>\n",
       "      <td>['you', 'forgot', 'democrate', 'because', 'vet...</td>\n",
       "      <td>you forgot democrate because vetting people fr...</td>\n",
       "      <td>you forgot democrate because vetting people fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31767</th>\n",
       "      <td>28476</td>\n",
       "      <td>sea shepherd suppoers are racist!   #antiracis...</td>\n",
       "      <td>sea shepherd suppoers are racist #antiracism #...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>['sea', 'shepherd', 'suppoers', 'are', 'racist...</td>\n",
       "      <td>['sea', 'shepherd', 'suppoer', 'are', 'racist'...</td>\n",
       "      <td>['sea', 'shepherd', 'suppoers', 'are', 'racist...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25210</th>\n",
       "      <td>24768</td>\n",
       "      <td>porn vids web free sex</td>\n",
       "      <td>porn vids web free sex</td>\n",
       "      <td>porn vids web free sex</td>\n",
       "      <td>['porn', 'vids', 'web', 'free', 'sex']</td>\n",
       "      <td>['porn', 'vid', 'web', 'free', 'sex']</td>\n",
       "      <td>['porn', 'vids', 'web', 'free', 'sex']</td>\n",
       "      <td>porn vids web free sex</td>\n",
       "      <td>porn vids web free sex</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19800</th>\n",
       "      <td>19745</td>\n",
       "      <td>@user &amp;amp; the  #democraticpay keeps telling ...</td>\n",
       "      <td>amp the #democraticpay keeps telling that #blm...</td>\n",
       "      <td>amp the democraticpay keeps telling that blm b...</td>\n",
       "      <td>['amp', 'the', 'democraticpay', 'keeps', 'tell...</td>\n",
       "      <td>['amp', 'the', 'democraticpay', 'keep', 'tell'...</td>\n",
       "      <td>['amp', 'the', 'democraticpay', 'keep', 'telli...</td>\n",
       "      <td>amp the democraticpay keeps telling that blm b...</td>\n",
       "      <td>amp the democraticpay keeps telling that blm b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7812</th>\n",
       "      <td>1242</td>\n",
       "      <td>@user although, i am not a , a #bigot or a #mi...</td>\n",
       "      <td>although not #bigot #misogynist hope that stil...</td>\n",
       "      <td>although not bigot misogynist hope that still ...</td>\n",
       "      <td>['although', 'not', 'bigot', 'misogynist', 'ho...</td>\n",
       "      <td>['although', 'not', 'bigot', 'misogynist', 'ho...</td>\n",
       "      <td>['although', 'not', 'bigot', 'misogynist', 'ho...</td>\n",
       "      <td>although not bigot misogynist hope that still ...</td>\n",
       "      <td>although not bigot misogynist hope that still ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5074</th>\n",
       "      <td>23280</td>\n",
       "      <td>this is sooooo  or may be just funny</td>\n",
       "      <td>this sooooo may just funny</td>\n",
       "      <td>this sooooo may just funny</td>\n",
       "      <td>['this', 'sooooo', 'may', 'just', 'funny']</td>\n",
       "      <td>['this', 'sooooo', 'may', 'just', 'funni']</td>\n",
       "      <td>['this', 'sooooo', 'may', 'just', 'funny']</td>\n",
       "      <td>this sooooo may just funny</td>\n",
       "      <td>this sooooo may just funni</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6317</th>\n",
       "      <td>15050</td>\n",
       "      <td>@user âwe the peopleâ originally meant â...</td>\n",
       "      <td>the people originally meant the white landhold...</td>\n",
       "      <td>the people originally meant the white landhold...</td>\n",
       "      <td>['the', 'people', 'originally', 'meant', 'the'...</td>\n",
       "      <td>['the', 'peopl', 'origin', 'meant', 'the', 'wh...</td>\n",
       "      <td>['the', 'people', 'originally', 'meant', 'the'...</td>\n",
       "      <td>the people originally meant the white landhold...</td>\n",
       "      <td>the people originally meant the white landhold...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5988</th>\n",
       "      <td>24766</td>\n",
       "      <td>i've no problem with #universities that teach ...</td>\n",
       "      <td>problem with #universities that teach against ...</td>\n",
       "      <td>problem with universities that teach against b...</td>\n",
       "      <td>['problem', 'with', 'universities', 'that', 't...</td>\n",
       "      <td>['problem', 'with', 'univers', 'that', 'teach'...</td>\n",
       "      <td>['problem', 'with', 'university', 'that', 'tea...</td>\n",
       "      <td>problem with universities that teach against b...</td>\n",
       "      <td>problem with universities that teach against b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3721</th>\n",
       "      <td>17405</td>\n",
       "      <td>check out this new trending #funny #gif !  , p...</td>\n",
       "      <td>check out this new trending #funny #gif pixel ...</td>\n",
       "      <td>check out this new trending funny gif pixel ce...</td>\n",
       "      <td>['check', 'out', 'this', 'new', 'trending', 'f...</td>\n",
       "      <td>['check', 'out', 'this', 'new', 'trend', 'funn...</td>\n",
       "      <td>['check', 'out', 'this', 'new', 'trending', 'f...</td>\n",
       "      <td>check out this new trending funny gif pixel ce...</td>\n",
       "      <td>check out this new trending funny gif pixel ce...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28057</th>\n",
       "      <td>31774</td>\n",
       "      <td>this reminds me of this. i am   love these two...</td>\n",
       "      <td>this reminds this love these two they are gold...</td>\n",
       "      <td>this reminds this love these two they are gold...</td>\n",
       "      <td>['this', 'reminds', 'this', 'love', 'these', '...</td>\n",
       "      <td>['this', 'remind', 'this', 'love', 'these', 't...</td>\n",
       "      <td>['this', 'reminds', 'this', 'love', 'these', '...</td>\n",
       "      <td>this reminds this love these two they are gold...</td>\n",
       "      <td>this reminds this love these two they are gold...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15044</th>\n",
       "      <td>5789</td>\n",
       "      <td>livelypics: just when you think you know peop...</td>\n",
       "      <td>livelypics just when you think you know people...</td>\n",
       "      <td>livelypics just when you think you know people...</td>\n",
       "      <td>['livelypics', 'just', 'when', 'you', 'think',...</td>\n",
       "      <td>['livelyp', 'just', 'when', 'you', 'think', 'y...</td>\n",
       "      <td>['livelypics', 'just', 'when', 'you', 'think',...</td>\n",
       "      <td>livelypics just when you think you know people...</td>\n",
       "      <td>livelypics just when you think you know people...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23222</th>\n",
       "      <td>29934</td>\n",
       "      <td>trying not to shut down but maybe #pokemon wil...</td>\n",
       "      <td>trying not shut down but maybe #pokemon will h...</td>\n",
       "      <td>trying not shut down but maybe pokemon will he...</td>\n",
       "      <td>['trying', 'not', 'shut', 'down', 'but', 'mayb...</td>\n",
       "      <td>['tri', 'not', 'shut', 'down', 'but', 'mayb', ...</td>\n",
       "      <td>['trying', 'not', 'shut', 'down', 'but', 'mayb...</td>\n",
       "      <td>trying not shut down but maybe pokemon will he...</td>\n",
       "      <td>trying not shut down but maybe pokemon will he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27273</th>\n",
       "      <td>395</td>\n",
       "      <td>#first #bihday to our #puppy #eloise #sweetbab...</td>\n",
       "      <td>#first #bihday our #puppy #eloise #sweetbabins...</td>\n",
       "      <td>first bihday our puppy eloise sweetbabins dog ...</td>\n",
       "      <td>['first', 'bihday', 'our', 'puppy', 'eloise', ...</td>\n",
       "      <td>['first', 'bihday', 'our', 'puppi', 'elois', '...</td>\n",
       "      <td>['first', 'bihday', 'our', 'puppy', 'eloise', ...</td>\n",
       "      <td>first bihday our puppy eloise sweetbabins dog ...</td>\n",
       "      <td>first bihday our puppy eloise sweetbabins dog ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30304</th>\n",
       "      <td>10458</td>\n",
       "      <td>@user it was in ceain areas yeah, you not seen...</td>\n",
       "      <td>was ceain areas yeah you not seen the videos s...</td>\n",
       "      <td>was ceain areas yeah you not seen the videos s...</td>\n",
       "      <td>['was', 'ceain', 'areas', 'yeah', 'you', 'not'...</td>\n",
       "      <td>['was', 'ceain', 'area', 'yeah', 'you', 'not',...</td>\n",
       "      <td>['wa', 'ceain', 'area', 'yeah', 'you', 'not', ...</td>\n",
       "      <td>was ceain areas yeah you not seen the videos s...</td>\n",
       "      <td>was ceain areas yeah you not seen the videos s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3928</th>\n",
       "      <td>19081</td>\n",
       "      <td>so many shit talkers/bullies in the youtube co...</td>\n",
       "      <td>many shit talkers bullies the youtube comment ...</td>\n",
       "      <td>many shit talkers bullies the youtube comment ...</td>\n",
       "      <td>['many', 'shit', 'talkers', 'bullies', 'the', ...</td>\n",
       "      <td>['mani', 'shit', 'talker', 'bulli', 'the', 'yo...</td>\n",
       "      <td>['many', 'shit', 'talker', 'bully', 'the', 'yo...</td>\n",
       "      <td>many shit talkers bullies the youtube comment ...</td>\n",
       "      <td>many shit talkers bullies the youtube comment ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>13429</td>\n",
       "      <td>all ready to pay xx #saturday #daughter #love ...</td>\n",
       "      <td>all ready pay #saturday #daughter #love #pay #...</td>\n",
       "      <td>all ready pay saturday daughter love pay igers...</td>\n",
       "      <td>['all', 'ready', 'pay', 'saturday', 'daughter'...</td>\n",
       "      <td>['all', 'readi', 'pay', 'saturday', 'daughter'...</td>\n",
       "      <td>['all', 'ready', 'pay', 'saturday', 'daughter'...</td>\n",
       "      <td>all ready pay saturday daughter love pay igers...</td>\n",
       "      <td>all ready pay saturday daughter love pay igers...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9826</th>\n",
       "      <td>6122</td>\n",
       "      <td>feeling a little #mole tonight! ó¾« #food #fo...</td>\n",
       "      <td>feeling little #mole tonight #food #foodblogge...</td>\n",
       "      <td>feeling little mole tonight food foodblogger m...</td>\n",
       "      <td>['feeling', 'little', 'mole', 'tonight', 'food...</td>\n",
       "      <td>['feel', 'littl', 'mole', 'tonight', 'food', '...</td>\n",
       "      <td>['feeling', 'little', 'mole', 'tonight', 'food...</td>\n",
       "      <td>feeling little mole tonight food foodblogger m...</td>\n",
       "      <td>feeling little mole tonight food foodblogger m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11548</th>\n",
       "      <td>14426</td>\n",
       "      <td>while we're still trying to get over the shock...</td>\n",
       "      <td>while still trying get over the shock keshi de...</td>\n",
       "      <td>while still trying get over the shock keshi de...</td>\n",
       "      <td>['while', 'still', 'trying', 'get', 'over', 't...</td>\n",
       "      <td>['while', 'still', 'tri', 'get', 'over', 'the'...</td>\n",
       "      <td>['while', 'still', 'trying', 'get', 'over', 't...</td>\n",
       "      <td>while still trying get over the shock keshi de...</td>\n",
       "      <td>while still trying get over the shock keshi de...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19491</th>\n",
       "      <td>19469</td>\n",
       "      <td>fantasy aisle ð #i #fantasy #aisle #summer ...</td>\n",
       "      <td>fantasy aisle #fantasy #aisle #summer #grand #...</td>\n",
       "      <td>fantasy aisle fantasy aisle summer grand vacat...</td>\n",
       "      <td>['fantasy', 'aisle', 'fantasy', 'aisle', 'summ...</td>\n",
       "      <td>['fantasi', 'aisl', 'fantasi', 'aisl', 'summer...</td>\n",
       "      <td>['fantasy', 'aisle', 'fantasy', 'aisle', 'summ...</td>\n",
       "      <td>fantasy aisle fantasy aisle summer grand vacat...</td>\n",
       "      <td>fantasy aisle fantasy aisle summer grand vacat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31522</th>\n",
       "      <td>17794</td>\n",
       "      <td>have a wonderful f r i d a y ð¸  #love #emik...</td>\n",
       "      <td>have wonderful #love #emikagifts #jewelrydesig...</td>\n",
       "      <td>have wonderful love emikagifts jewelrydesign d...</td>\n",
       "      <td>['have', 'wonderful', 'love', 'emikagifts', 'j...</td>\n",
       "      <td>['have', 'wonder', 'love', 'emikagift', 'jewel...</td>\n",
       "      <td>['have', 'wonderful', 'love', 'emikagifts', 'j...</td>\n",
       "      <td>have wonderful love emikagifts jewelrydesign d...</td>\n",
       "      <td>have wonderful love emikagifts jewelrydesign d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>11550</td>\n",
       "      <td>who wants him to be the next commander in chie...</td>\n",
       "      <td>who wants him the next commander chief #scary ...</td>\n",
       "      <td>who wants him the next commander chief scary d...</td>\n",
       "      <td>['who', 'wants', 'him', 'the', 'next', 'comman...</td>\n",
       "      <td>['who', 'want', 'him', 'the', 'next', 'command...</td>\n",
       "      <td>['who', 'want', 'him', 'the', 'next', 'command...</td>\n",
       "      <td>who wants him the next commander chief scary d...</td>\n",
       "      <td>who wants him the next commander chief scary d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21399</th>\n",
       "      <td>18137</td>\n",
       "      <td>we beat that cock.   #beatit #penis</td>\n",
       "      <td>beat that cock #beatit #penis</td>\n",
       "      <td>beat that cock beatit penis</td>\n",
       "      <td>['beat', 'that', 'cock', 'beatit', 'penis']</td>\n",
       "      <td>['beat', 'that', 'cock', 'beatit', 'peni']</td>\n",
       "      <td>['beat', 'that', 'cock', 'beatit', 'penis']</td>\n",
       "      <td>beat that cock beatit penis</td>\n",
       "      <td>beat that cock beatit peni</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16625</th>\n",
       "      <td>14918</td>\n",
       "      <td>lik if u cri everi tim</td>\n",
       "      <td>lik cri everi tim</td>\n",
       "      <td>lik cri everi tim</td>\n",
       "      <td>['lik', 'cri', 'everi', 'tim']</td>\n",
       "      <td>['lik', 'cri', 'everi', 'tim']</td>\n",
       "      <td>['lik', 'cri', 'everi', 'tim']</td>\n",
       "      <td>lik cri everi tim</td>\n",
       "      <td>lik cri everi tim</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7084</th>\n",
       "      <td>12925</td>\n",
       "      <td>speakers dinner on eve of #ricsrural conferenc...</td>\n",
       "      <td>speakers dinner eve #ricsrural conference #bet...</td>\n",
       "      <td>speakers dinner eve ricsrural conference bette...</td>\n",
       "      <td>['speakers', 'dinner', 'eve', 'ricsrural', 'co...</td>\n",
       "      <td>['speaker', 'dinner', 'eve', 'ricsrur', 'confe...</td>\n",
       "      <td>['speaker', 'dinner', 'eve', 'ricsrural', 'con...</td>\n",
       "      <td>speakers dinner eve ricsrural conference bette...</td>\n",
       "      <td>speakers dinner eve ricsrural conference bette...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21332</th>\n",
       "      <td>1640</td>\n",
       "      <td>best facetime today; i can't wait to see my bo...</td>\n",
       "      <td>best facetime today can wait see boy wednesday...</td>\n",
       "      <td>best facetime today can wait see boy wednesday...</td>\n",
       "      <td>['best', 'facetime', 'today', 'can', 'wait', '...</td>\n",
       "      <td>['best', 'facetim', 'today', 'can', 'wait', 's...</td>\n",
       "      <td>['best', 'facetime', 'today', 'can', 'wait', '...</td>\n",
       "      <td>best facetime today can wait see boy wednesday...</td>\n",
       "      <td>best facetime today can wait see boy wednesday...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28564</th>\n",
       "      <td>18855</td>\n",
       "      <td>archery for year 5 at 1.30pm!   #archery</td>\n",
       "      <td>archery for year #archery</td>\n",
       "      <td>archery for year archery</td>\n",
       "      <td>['archery', 'for', 'year', 'archery']</td>\n",
       "      <td>['archeri', 'for', 'year', 'archeri']</td>\n",
       "      <td>['archery', 'for', 'year', 'archery']</td>\n",
       "      <td>archery for year archery</td>\n",
       "      <td>archery for year archeri</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12504</th>\n",
       "      <td>23278</td>\n",
       "      <td>#whoolo in film you can have sad endings.  #an...</td>\n",
       "      <td>#whoolo film you can have sad endings #anna torv</td>\n",
       "      <td>whoolo film you can have sad endings anna torv</td>\n",
       "      <td>['whoolo', 'film', 'you', 'can', 'have', 'sad'...</td>\n",
       "      <td>['whoolo', 'film', 'you', 'can', 'have', 'sad'...</td>\n",
       "      <td>['whoolo', 'film', 'you', 'can', 'have', 'sad'...</td>\n",
       "      <td>whoolo film you can have sad endings anna torv</td>\n",
       "      <td>whoolo film you can have sad endings anna torv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22295</th>\n",
       "      <td>17561</td>\n",
       "      <td>@user @user at what time will the gates open?</td>\n",
       "      <td>what time will the gates open</td>\n",
       "      <td>what time will the gates open</td>\n",
       "      <td>['what', 'time', 'will', 'the', 'gates', 'open']</td>\n",
       "      <td>['what', 'time', 'will', 'the', 'gate', 'open']</td>\n",
       "      <td>['what', 'time', 'will', 'the', 'gate', 'open']</td>\n",
       "      <td>what time will the gates open</td>\n",
       "      <td>what time will the gates open</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13326</th>\n",
       "      <td>6135</td>\n",
       "      <td>thank you @user for a new #chargehr after mine...</td>\n",
       "      <td>thank you for new #chargehr after mine broke #...</td>\n",
       "      <td>thank you for new chargehr after mine broke fi...</td>\n",
       "      <td>['thank', 'you', 'for', 'new', 'chargehr', 'af...</td>\n",
       "      <td>['thank', 'you', 'for', 'new', 'chargehr', 'af...</td>\n",
       "      <td>['thank', 'you', 'for', 'new', 'chargehr', 'af...</td>\n",
       "      <td>thank you for new chargehr after mine broke fi...</td>\n",
       "      <td>thank you for new chargehr after mine broke fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>21952</td>\n",
       "      <td>our thoughts and prayers goes out to everyone ...</td>\n",
       "      <td>our thoughts and prayers goes out everyone inv...</td>\n",
       "      <td>our thoughts and prayers goes out everyone inv...</td>\n",
       "      <td>['our', 'thoughts', 'and', 'prayers', 'goes', ...</td>\n",
       "      <td>['our', 'thought', 'and', 'prayer', 'goe', 'ou...</td>\n",
       "      <td>['our', 'thought', 'and', 'prayer', 'go', 'out...</td>\n",
       "      <td>our thoughts and prayers goes out everyone inv...</td>\n",
       "      <td>our thoughts and prayers goes out everyone inv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19044</th>\n",
       "      <td>25937</td>\n",
       "      <td>#film   bull up: you will dominate your bull a...</td>\n",
       "      <td>#film bull you will dominate your bull and you...</td>\n",
       "      <td>film bull you will dominate your bull and you ...</td>\n",
       "      <td>['film', 'bull', 'you', 'will', 'dominate', 'y...</td>\n",
       "      <td>['film', 'bull', 'you', 'will', 'domin', 'your...</td>\n",
       "      <td>['film', 'bull', 'you', 'will', 'dominate', 'y...</td>\n",
       "      <td>film bull you will dominate your bull and you ...</td>\n",
       "      <td>film bull you will dominate your bull and you ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31112</th>\n",
       "      <td>17229</td>\n",
       "      <td>this is horribly sad news. a fine actor with g...</td>\n",
       "      <td>this horribly sad news fine actor with great t...</td>\n",
       "      <td>this horribly sad news fine actor with great t...</td>\n",
       "      <td>['this', 'horribly', 'sad', 'news', 'fine', 'a...</td>\n",
       "      <td>['this', 'horribl', 'sad', 'news', 'fine', 'ac...</td>\n",
       "      <td>['this', 'horribly', 'sad', 'news', 'fine', 'a...</td>\n",
       "      <td>this horribly sad news fine actor with great t...</td>\n",
       "      <td>this horribly sad news fine actor with great t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14188</th>\n",
       "      <td>7012</td>\n",
       "      <td>follow your #hea and be   â¤</td>\n",
       "      <td>follow your #hea and</td>\n",
       "      <td>follow your hea and</td>\n",
       "      <td>['follow', 'your', 'hea', 'and']</td>\n",
       "      <td>['follow', 'your', 'hea', 'and']</td>\n",
       "      <td>['follow', 'your', 'hea', 'and']</td>\n",
       "      <td>follow your hea and</td>\n",
       "      <td>follow your hea and</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>31177</td>\n",
       "      <td>cops giving tickets when they didn't even see ...</td>\n",
       "      <td>cops giving tickets when they didn even see yo...</td>\n",
       "      <td>cops giving tickets when they didn even see yo...</td>\n",
       "      <td>['cops', 'giving', 'tickets', 'when', 'they', ...</td>\n",
       "      <td>['cop', 'give', 'ticket', 'when', 'they', 'did...</td>\n",
       "      <td>['cop', 'giving', 'ticket', 'when', 'they', 'd...</td>\n",
       "      <td>cops giving tickets when they didn even see yo...</td>\n",
       "      <td>cops giving tickets when they didn even see yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>23672</td>\n",
       "      <td>singapore city gallery   #riclswtravelbook #be...</td>\n",
       "      <td>singapore city gallery #riclswtravelbook #bear...</td>\n",
       "      <td>singapore city gallery riclswtravelbook bearlo...</td>\n",
       "      <td>['singapore', 'city', 'gallery', 'riclswtravel...</td>\n",
       "      <td>['singapor', 'citi', 'galleri', 'riclswtravelb...</td>\n",
       "      <td>['singapore', 'city', 'gallery', 'riclswtravel...</td>\n",
       "      <td>singapore city gallery riclswtravelbook bearlo...</td>\n",
       "      <td>singapore city gallery riclswtravelbook bearlo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8230</th>\n",
       "      <td>19584</td>\n",
       "      <td>oh man, been waiting for #wehappyfew for so lo...</td>\n",
       "      <td>man been waiting for #wehappyfew for long #xboxe</td>\n",
       "      <td>man been waiting for wehappyfew for long xboxe</td>\n",
       "      <td>['man', 'been', 'waiting', 'for', 'wehappyfew'...</td>\n",
       "      <td>['man', 'been', 'wait', 'for', 'wehappyfew', '...</td>\n",
       "      <td>['man', 'been', 'waiting', 'for', 'wehappyfew'...</td>\n",
       "      <td>man been waiting for wehappyfew for long xboxe</td>\n",
       "      <td>man been waiting for wehappyfew for long xbox</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15277</th>\n",
       "      <td>9967</td>\n",
       "      <td>schools almost over.</td>\n",
       "      <td>schools almost over</td>\n",
       "      <td>schools almost over</td>\n",
       "      <td>['schools', 'almost', 'over']</td>\n",
       "      <td>['school', 'almost', 'over']</td>\n",
       "      <td>['school', 'almost', 'over']</td>\n",
       "      <td>schools almost over</td>\n",
       "      <td>schools almost ov</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30396</th>\n",
       "      <td>2987</td>\n",
       "      <td>â #usd/cad bounces-off 0.2900, despite high...</td>\n",
       "      <td>#usd cad bounces off despite higher oil #blog ...</td>\n",
       "      <td>usd cad bounces off despite higher oil blog si...</td>\n",
       "      <td>['usd', 'cad', 'bounces', 'off', 'despite', 'h...</td>\n",
       "      <td>['usd', 'cad', 'bounc', 'off', 'despit', 'high...</td>\n",
       "      <td>['usd', 'cad', 'bounce', 'off', 'despite', 'hi...</td>\n",
       "      <td>usd cad bounces off despite higher oil blog si...</td>\n",
       "      <td>usd cad bounces off despite higher oil blog si...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37982 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  \\\n",
       "565    20676  @user f*** this  ð¦ðº government that deli...   \n",
       "21531  24025  despite a demoralizing 2016: may ur #newyear20...   \n",
       "13300  18145  @user #koreans &amp; joseon people in japan, w...   \n",
       "18925   8506  @user @user @user @user classic ! yet you jewi...   \n",
       "12619  15464  @user did someone say #antisemetic ? gee (((@u...   \n",
       "26964  28937  couldn't have said this any better nor truthfu...   \n",
       "17273  25291  @user racism stuffed into skinny jeans with a ...   \n",
       "1561   12717  the end of   #me #selfie # #love #messi #cr7 #...   \n",
       "17875  11612  trump ally wishes mad cow disease death for ob...   \n",
       "17184  20554  opinion:  is rife in the #lgbt community. #gay...   \n",
       "8573   25151  @user #allahsoil the cold war was fought over ...   \n",
       "26779  13585  omg, these trump suppoers are deplorable! #dum...   \n",
       "15332   2581  sea shepherd suppoers are racist!   #antiracis...   \n",
       "12290   6519  .@user @user while @user can use phrases like ...   \n",
       "24159   9563  @user #allahsoil enlightenment is wasted on th...   \n",
       "25895   8452  @user here comes a  #supermistict douchebag wh...   \n",
       "23934  17750  @user hidden  in #america is as rampant as bla...   \n",
       "29174  21389  will the alt-right promote a new kind of  gene...   \n",
       "10474  23431  #us why weneed #empathy #ageoftrump #grassroot...   \n",
       "30743  12301  black trump suppoer smacks down cnn repoer for...   \n",
       "6190   19407  so sick of all the pre-programmed #hillbots rh...   \n",
       "19746    406  @user when you're blocked by a  troll because ...   \n",
       "19581  11720  @user @user you forgot #democrate, because vet...   \n",
       "31767  28476  sea shepherd suppoers are racist!   #antiracis...   \n",
       "25210  24768                             porn vids web free sex   \n",
       "19800  19745  @user &amp; the  #democraticpay keeps telling ...   \n",
       "7812    1242  @user although, i am not a , a #bigot or a #mi...   \n",
       "5074   23280               this is sooooo  or may be just funny   \n",
       "6317   15050  @user âwe the peopleâ originally meant â...   \n",
       "5988   24766  i've no problem with #universities that teach ...   \n",
       "...      ...                                                ...   \n",
       "3721   17405  check out this new trending #funny #gif !  , p...   \n",
       "28057  31774  this reminds me of this. i am   love these two...   \n",
       "15044   5789   livelypics: just when you think you know peop...   \n",
       "23222  29934  trying not to shut down but maybe #pokemon wil...   \n",
       "27273    395  #first #bihday to our #puppy #eloise #sweetbab...   \n",
       "30304  10458  @user it was in ceain areas yeah, you not seen...   \n",
       "3928   19081  so many shit talkers/bullies in the youtube co...   \n",
       "269    13429  all ready to pay xx #saturday #daughter #love ...   \n",
       "9826    6122  feeling a little #mole tonight! ó¾« #food #fo...   \n",
       "11548  14426  while we're still trying to get over the shock...   \n",
       "19491  19469  fantasy aisle ð #i #fantasy #aisle #summer ...   \n",
       "31522  17794  have a wonderful f r i d a y ð¸  #love #emik...   \n",
       "750    11550  who wants him to be the next commander in chie...   \n",
       "21399  18137                we beat that cock.   #beatit #penis   \n",
       "16625  14918                             lik if u cri everi tim   \n",
       "7084   12925  speakers dinner on eve of #ricsrural conferenc...   \n",
       "21332   1640  best facetime today; i can't wait to see my bo...   \n",
       "28564  18855           archery for year 5 at 1.30pm!   #archery   \n",
       "12504  23278  #whoolo in film you can have sad endings.  #an...   \n",
       "22295  17561      @user @user at what time will the gates open?   \n",
       "13326   6135  thank you @user for a new #chargehr after mine...   \n",
       "1147   21952  our thoughts and prayers goes out to everyone ...   \n",
       "19044  25937  #film   bull up: you will dominate your bull a...   \n",
       "31112  17229  this is horribly sad news. a fine actor with g...   \n",
       "14188   7012                      follow your #hea and be   â¤   \n",
       "1034   31177  cops giving tickets when they didn't even see ...   \n",
       "2874   23672  singapore city gallery   #riclswtravelbook #be...   \n",
       "8230   19584  oh man, been waiting for #wehappyfew for so lo...   \n",
       "15277   9967                               schools almost over.   \n",
       "30396   2987   â #usd/cad bounces-off 0.2900, despite high...   \n",
       "\n",
       "                                              tidy_tweet  \\\n",
       "565    this government that deliberately toures #refu...   \n",
       "21531  despite demoralizing may #newyear #classism fr...   \n",
       "13300  #koreans amp joseon people japan will abuse th...   \n",
       "18925  classic yet you jewish bastards wonder why you...   \n",
       "12619  did someone say #antisemetic gee you bit trigg...   \n",
       "26964  couldn have said this any better nor truthfull...   \n",
       "17273  racism stuffed into skinny jeans with hipster ...   \n",
       "1561   the end #me #selfie #love #messi #cr #religion...   \n",
       "17875  trump ally wishes mad cow disease death for ob...   \n",
       "17184  opinion rife the #lgbt community #gay people c...   \n",
       "8573   #allahsoil the cold war was fought over oil #t...   \n",
       "26779  omg these trump suppoers are deplorable #dumpt...   \n",
       "15332  sea shepherd suppoers are racist #antiracism #...   \n",
       "12290  while can use phrases like #sandniggers accept...   \n",
       "24159  #allahsoil enlightenment wasted the wilfully b...   \n",
       "25895  here comes #supermistict douchebag who can onl...   \n",
       "23934             hidden #america rampant blatant racism   \n",
       "29174       will the alt right promote new kind genetics   \n",
       "10474  #us why weneed #empathy #ageoftrump #grassroot...   \n",
       "30743  black trump suppoer smacks down cnn repoer for...   \n",
       "6190   sick all the pre programmed #hillbots rhetoric...   \n",
       "19746  when you blocked troll because you promise #bl...   \n",
       "19581  you forgot #democrate because vetting people f...   \n",
       "31767  sea shepherd suppoers are racist #antiracism #...   \n",
       "25210                             porn vids web free sex   \n",
       "19800  amp the #democraticpay keeps telling that #blm...   \n",
       "7812   although not #bigot #misogynist hope that stil...   \n",
       "5074                          this sooooo may just funny   \n",
       "6317   the people originally meant the white landhold...   \n",
       "5988   problem with #universities that teach against ...   \n",
       "...                                                  ...   \n",
       "3721   check out this new trending #funny #gif pixel ...   \n",
       "28057  this reminds this love these two they are gold...   \n",
       "15044  livelypics just when you think you know people...   \n",
       "23222  trying not shut down but maybe #pokemon will h...   \n",
       "27273  #first #bihday our #puppy #eloise #sweetbabins...   \n",
       "30304  was ceain areas yeah you not seen the videos s...   \n",
       "3928   many shit talkers bullies the youtube comment ...   \n",
       "269    all ready pay #saturday #daughter #love #pay #...   \n",
       "9826   feeling little #mole tonight #food #foodblogge...   \n",
       "11548  while still trying get over the shock keshi de...   \n",
       "19491  fantasy aisle #fantasy #aisle #summer #grand #...   \n",
       "31522  have wonderful #love #emikagifts #jewelrydesig...   \n",
       "750    who wants him the next commander chief #scary ...   \n",
       "21399                      beat that cock #beatit #penis   \n",
       "16625                                  lik cri everi tim   \n",
       "7084   speakers dinner eve #ricsrural conference #bet...   \n",
       "21332  best facetime today can wait see boy wednesday...   \n",
       "28564                          archery for year #archery   \n",
       "12504   #whoolo film you can have sad endings #anna torv   \n",
       "22295                      what time will the gates open   \n",
       "13326  thank you for new #chargehr after mine broke #...   \n",
       "1147   our thoughts and prayers goes out everyone inv...   \n",
       "19044  #film bull you will dominate your bull and you...   \n",
       "31112  this horribly sad news fine actor with great t...   \n",
       "14188                               follow your #hea and   \n",
       "1034   cops giving tickets when they didn even see yo...   \n",
       "2874   singapore city gallery #riclswtravelbook #bear...   \n",
       "8230    man been waiting for #wehappyfew for long #xboxe   \n",
       "15277                                schools almost over   \n",
       "30396  #usd cad bounces off despite higher oil #blog ...   \n",
       "\n",
       "                                           no_hash_tweet  \\\n",
       "565    this government that deliberately toures refug...   \n",
       "21531  despite demoralizing may newyear classism free...   \n",
       "13300  koreans amp joseon people japan will abuse the...   \n",
       "18925  classic yet you jewish bastards wonder why you...   \n",
       "12619  did someone say antisemetic gee you bit triggered   \n",
       "26964  couldn have said this any better nor truthfull...   \n",
       "17273  racism stuffed into skinny jeans with hipster ...   \n",
       "1561   the end me selfie love messi cr religion chris...   \n",
       "17875  trump ally wishes mad cow disease death for ob...   \n",
       "17184  opinion rife the lgbt community gay people can...   \n",
       "8573   allahsoil the cold war was fought over oil tea...   \n",
       "26779  omg these trump suppoers are deplorable dumptr...   \n",
       "15332  sea shepherd suppoers are racist antiracism se...   \n",
       "12290  while can use phrases like sandniggers acceptable   \n",
       "24159  allahsoil enlightenment wasted the wilfully bl...   \n",
       "25895  here comes supermistict douchebag who can only...   \n",
       "23934              hidden america rampant blatant racism   \n",
       "29174       will the alt right promote new kind genetics   \n",
       "10474  us why weneed empathy ageoftrump grassrootsact...   \n",
       "30743  black trump suppoer smacks down cnn repoer for...   \n",
       "6190   sick all the pre programmed hillbots rhetoric ...   \n",
       "19746  when you blocked troll because you promise bla...   \n",
       "19581  you forgot democrate because vetting people fr...   \n",
       "31767  sea shepherd suppoers are racist antiracism se...   \n",
       "25210                             porn vids web free sex   \n",
       "19800  amp the democraticpay keeps telling that blm b...   \n",
       "7812   although not bigot misogynist hope that still ...   \n",
       "5074                          this sooooo may just funny   \n",
       "6317   the people originally meant the white landhold...   \n",
       "5988   problem with universities that teach against b...   \n",
       "...                                                  ...   \n",
       "3721   check out this new trending funny gif pixel ce...   \n",
       "28057  this reminds this love these two they are gold...   \n",
       "15044  livelypics just when you think you know people...   \n",
       "23222  trying not shut down but maybe pokemon will he...   \n",
       "27273  first bihday our puppy eloise sweetbabins dog ...   \n",
       "30304  was ceain areas yeah you not seen the videos s...   \n",
       "3928   many shit talkers bullies the youtube comment ...   \n",
       "269    all ready pay saturday daughter love pay igers...   \n",
       "9826   feeling little mole tonight food foodblogger m...   \n",
       "11548  while still trying get over the shock keshi de...   \n",
       "19491  fantasy aisle fantasy aisle summer grand vacat...   \n",
       "31522  have wonderful love emikagifts jewelrydesign d...   \n",
       "750    who wants him the next commander chief scary d...   \n",
       "21399                        beat that cock beatit penis   \n",
       "16625                                  lik cri everi tim   \n",
       "7084   speakers dinner eve ricsrural conference bette...   \n",
       "21332  best facetime today can wait see boy wednesday...   \n",
       "28564                           archery for year archery   \n",
       "12504     whoolo film you can have sad endings anna torv   \n",
       "22295                      what time will the gates open   \n",
       "13326  thank you for new chargehr after mine broke fi...   \n",
       "1147   our thoughts and prayers goes out everyone inv...   \n",
       "19044  film bull you will dominate your bull and you ...   \n",
       "31112  this horribly sad news fine actor with great t...   \n",
       "14188                                follow your hea and   \n",
       "1034   cops giving tickets when they didn even see yo...   \n",
       "2874   singapore city gallery riclswtravelbook bearlo...   \n",
       "8230      man been waiting for wehappyfew for long xboxe   \n",
       "15277                                schools almost over   \n",
       "30396  usd cad bounces off despite higher oil blog si...   \n",
       "\n",
       "                                         tokenized_tweet  \\\n",
       "565    ['this', 'government', 'that', 'deliberately',...   \n",
       "21531  ['despite', 'demoralizing', 'may', 'newyear', ...   \n",
       "13300  ['koreans', 'amp', 'joseon', 'people', 'japan'...   \n",
       "18925  ['classic', 'yet', 'you', 'jewish', 'bastards'...   \n",
       "12619  ['did', 'someone', 'say', 'antisemetic', 'gee'...   \n",
       "26964  ['couldn', 'have', 'said', 'this', 'any', 'bet...   \n",
       "17273  ['racism', 'stuffed', 'into', 'skinny', 'jeans...   \n",
       "1561   ['the', 'end', 'me', 'selfie', 'love', 'messi'...   \n",
       "17875  ['trump', 'ally', 'wishes', 'mad', 'cow', 'dis...   \n",
       "17184  ['opinion', 'rife', 'the', 'lgbt', 'community'...   \n",
       "8573   ['allahsoil', 'the', 'cold', 'war', 'was', 'fo...   \n",
       "26779  ['omg', 'these', 'trump', 'suppoers', 'are', '...   \n",
       "15332  ['sea', 'shepherd', 'suppoers', 'are', 'racist...   \n",
       "12290  ['while', 'can', 'use', 'phrases', 'like', 'sa...   \n",
       "24159  ['allahsoil', 'enlightenment', 'wasted', 'the'...   \n",
       "25895  ['here', 'comes', 'supermistict', 'douchebag',...   \n",
       "23934  ['hidden', 'america', 'rampant', 'blatant', 'r...   \n",
       "29174  ['will', 'the', 'alt', 'right', 'promote', 'ne...   \n",
       "10474  ['us', 'why', 'weneed', 'empathy', 'ageoftrump...   \n",
       "30743  ['black', 'trump', 'suppoer', 'smacks', 'down'...   \n",
       "6190   ['sick', 'all', 'the', 'pre', 'programmed', 'h...   \n",
       "19746  ['when', 'you', 'blocked', 'troll', 'because',...   \n",
       "19581  ['you', 'forgot', 'democrate', 'because', 'vet...   \n",
       "31767  ['sea', 'shepherd', 'suppoers', 'are', 'racist...   \n",
       "25210             ['porn', 'vids', 'web', 'free', 'sex']   \n",
       "19800  ['amp', 'the', 'democraticpay', 'keeps', 'tell...   \n",
       "7812   ['although', 'not', 'bigot', 'misogynist', 'ho...   \n",
       "5074          ['this', 'sooooo', 'may', 'just', 'funny']   \n",
       "6317   ['the', 'people', 'originally', 'meant', 'the'...   \n",
       "5988   ['problem', 'with', 'universities', 'that', 't...   \n",
       "...                                                  ...   \n",
       "3721   ['check', 'out', 'this', 'new', 'trending', 'f...   \n",
       "28057  ['this', 'reminds', 'this', 'love', 'these', '...   \n",
       "15044  ['livelypics', 'just', 'when', 'you', 'think',...   \n",
       "23222  ['trying', 'not', 'shut', 'down', 'but', 'mayb...   \n",
       "27273  ['first', 'bihday', 'our', 'puppy', 'eloise', ...   \n",
       "30304  ['was', 'ceain', 'areas', 'yeah', 'you', 'not'...   \n",
       "3928   ['many', 'shit', 'talkers', 'bullies', 'the', ...   \n",
       "269    ['all', 'ready', 'pay', 'saturday', 'daughter'...   \n",
       "9826   ['feeling', 'little', 'mole', 'tonight', 'food...   \n",
       "11548  ['while', 'still', 'trying', 'get', 'over', 't...   \n",
       "19491  ['fantasy', 'aisle', 'fantasy', 'aisle', 'summ...   \n",
       "31522  ['have', 'wonderful', 'love', 'emikagifts', 'j...   \n",
       "750    ['who', 'wants', 'him', 'the', 'next', 'comman...   \n",
       "21399        ['beat', 'that', 'cock', 'beatit', 'penis']   \n",
       "16625                     ['lik', 'cri', 'everi', 'tim']   \n",
       "7084   ['speakers', 'dinner', 'eve', 'ricsrural', 'co...   \n",
       "21332  ['best', 'facetime', 'today', 'can', 'wait', '...   \n",
       "28564              ['archery', 'for', 'year', 'archery']   \n",
       "12504  ['whoolo', 'film', 'you', 'can', 'have', 'sad'...   \n",
       "22295   ['what', 'time', 'will', 'the', 'gates', 'open']   \n",
       "13326  ['thank', 'you', 'for', 'new', 'chargehr', 'af...   \n",
       "1147   ['our', 'thoughts', 'and', 'prayers', 'goes', ...   \n",
       "19044  ['film', 'bull', 'you', 'will', 'dominate', 'y...   \n",
       "31112  ['this', 'horribly', 'sad', 'news', 'fine', 'a...   \n",
       "14188                   ['follow', 'your', 'hea', 'and']   \n",
       "1034   ['cops', 'giving', 'tickets', 'when', 'they', ...   \n",
       "2874   ['singapore', 'city', 'gallery', 'riclswtravel...   \n",
       "8230   ['man', 'been', 'waiting', 'for', 'wehappyfew'...   \n",
       "15277                      ['schools', 'almost', 'over']   \n",
       "30396  ['usd', 'cad', 'bounces', 'off', 'despite', 'h...   \n",
       "\n",
       "                                          stemmed_tokens  \\\n",
       "565    ['this', 'govern', 'that', 'deliber', 'tour', ...   \n",
       "21531  ['despit', 'demor', 'may', 'newyear', 'classis...   \n",
       "13300  ['korean', 'amp', 'joseon', 'peopl', 'japan', ...   \n",
       "18925  ['classic', 'yet', 'you', 'jewish', 'bastard',...   \n",
       "12619  ['did', 'someon', 'say', 'antisemet', 'gee', '...   \n",
       "26964  ['couldn', 'have', 'said', 'this', 'ani', 'bet...   \n",
       "17273  ['racism', 'stuf', 'into', 'skinni', 'jean', '...   \n",
       "1561   ['the', 'end', 'me', 'selfi', 'love', 'messi',...   \n",
       "17875  ['trump', 'alli', 'wish', 'mad', 'cow', 'disea...   \n",
       "17184  ['opinion', 'rife', 'the', 'lgbt', 'communiti'...   \n",
       "8573   ['allahsoil', 'the', 'cold', 'war', 'was', 'fo...   \n",
       "26779  ['omg', 'these', 'trump', 'suppoer', 'are', 'd...   \n",
       "15332  ['sea', 'shepherd', 'suppoer', 'are', 'racist'...   \n",
       "12290  ['while', 'can', 'use', 'phrase', 'like', 'san...   \n",
       "24159  ['allahsoil', 'enlighten', 'wast', 'the', 'wil...   \n",
       "25895  ['here', 'come', 'supermistict', 'douchebag', ...   \n",
       "23934  ['hidden', 'america', 'rampant', 'blatant', 'r...   \n",
       "29174  ['will', 'the', 'alt', 'right', 'promot', 'new...   \n",
       "10474  ['us', 'whi', 'wene', 'empathi', 'ageoftrump',...   \n",
       "30743  ['black', 'trump', 'suppoer', 'smack', 'down',...   \n",
       "6190   ['sick', 'all', 'the', 'pre', 'program', 'hill...   \n",
       "19746  ['when', 'you', 'block', 'troll', 'becaus', 'y...   \n",
       "19581  ['you', 'forgot', 'democr', 'becaus', 'vet', '...   \n",
       "31767  ['sea', 'shepherd', 'suppoer', 'are', 'racist'...   \n",
       "25210              ['porn', 'vid', 'web', 'free', 'sex']   \n",
       "19800  ['amp', 'the', 'democraticpay', 'keep', 'tell'...   \n",
       "7812   ['although', 'not', 'bigot', 'misogynist', 'ho...   \n",
       "5074          ['this', 'sooooo', 'may', 'just', 'funni']   \n",
       "6317   ['the', 'peopl', 'origin', 'meant', 'the', 'wh...   \n",
       "5988   ['problem', 'with', 'univers', 'that', 'teach'...   \n",
       "...                                                  ...   \n",
       "3721   ['check', 'out', 'this', 'new', 'trend', 'funn...   \n",
       "28057  ['this', 'remind', 'this', 'love', 'these', 't...   \n",
       "15044  ['livelyp', 'just', 'when', 'you', 'think', 'y...   \n",
       "23222  ['tri', 'not', 'shut', 'down', 'but', 'mayb', ...   \n",
       "27273  ['first', 'bihday', 'our', 'puppi', 'elois', '...   \n",
       "30304  ['was', 'ceain', 'area', 'yeah', 'you', 'not',...   \n",
       "3928   ['mani', 'shit', 'talker', 'bulli', 'the', 'yo...   \n",
       "269    ['all', 'readi', 'pay', 'saturday', 'daughter'...   \n",
       "9826   ['feel', 'littl', 'mole', 'tonight', 'food', '...   \n",
       "11548  ['while', 'still', 'tri', 'get', 'over', 'the'...   \n",
       "19491  ['fantasi', 'aisl', 'fantasi', 'aisl', 'summer...   \n",
       "31522  ['have', 'wonder', 'love', 'emikagift', 'jewel...   \n",
       "750    ['who', 'want', 'him', 'the', 'next', 'command...   \n",
       "21399         ['beat', 'that', 'cock', 'beatit', 'peni']   \n",
       "16625                     ['lik', 'cri', 'everi', 'tim']   \n",
       "7084   ['speaker', 'dinner', 'eve', 'ricsrur', 'confe...   \n",
       "21332  ['best', 'facetim', 'today', 'can', 'wait', 's...   \n",
       "28564              ['archeri', 'for', 'year', 'archeri']   \n",
       "12504  ['whoolo', 'film', 'you', 'can', 'have', 'sad'...   \n",
       "22295    ['what', 'time', 'will', 'the', 'gate', 'open']   \n",
       "13326  ['thank', 'you', 'for', 'new', 'chargehr', 'af...   \n",
       "1147   ['our', 'thought', 'and', 'prayer', 'goe', 'ou...   \n",
       "19044  ['film', 'bull', 'you', 'will', 'domin', 'your...   \n",
       "31112  ['this', 'horribl', 'sad', 'news', 'fine', 'ac...   \n",
       "14188                   ['follow', 'your', 'hea', 'and']   \n",
       "1034   ['cop', 'give', 'ticket', 'when', 'they', 'did...   \n",
       "2874   ['singapor', 'citi', 'galleri', 'riclswtravelb...   \n",
       "8230   ['man', 'been', 'wait', 'for', 'wehappyfew', '...   \n",
       "15277                       ['school', 'almost', 'over']   \n",
       "30396  ['usd', 'cad', 'bounc', 'off', 'despit', 'high...   \n",
       "\n",
       "                                       lemmatized_tokens  \\\n",
       "565    ['this', 'government', 'that', 'deliberately',...   \n",
       "21531  ['despite', 'demoralizing', 'may', 'newyear', ...   \n",
       "13300  ['korean', 'amp', 'joseon', 'people', 'japan',...   \n",
       "18925  ['classic', 'yet', 'you', 'jewish', 'bastard',...   \n",
       "12619  ['did', 'someone', 'say', 'antisemetic', 'gee'...   \n",
       "26964  ['couldn', 'have', 'said', 'this', 'any', 'bet...   \n",
       "17273  ['racism', 'stuffed', 'into', 'skinny', 'jean'...   \n",
       "1561   ['the', 'end', 'me', 'selfie', 'love', 'messi'...   \n",
       "17875  ['trump', 'ally', 'wish', 'mad', 'cow', 'disea...   \n",
       "17184  ['opinion', 'rife', 'the', 'lgbt', 'community'...   \n",
       "8573   ['allahsoil', 'the', 'cold', 'war', 'wa', 'fou...   \n",
       "26779  ['omg', 'these', 'trump', 'suppoers', 'are', '...   \n",
       "15332  ['sea', 'shepherd', 'suppoers', 'are', 'racist...   \n",
       "12290  ['while', 'can', 'use', 'phrase', 'like', 'san...   \n",
       "24159  ['allahsoil', 'enlightenment', 'wasted', 'the'...   \n",
       "25895  ['here', 'come', 'supermistict', 'douchebag', ...   \n",
       "23934  ['hidden', 'america', 'rampant', 'blatant', 'r...   \n",
       "29174  ['will', 'the', 'alt', 'right', 'promote', 'ne...   \n",
       "10474  ['u', 'why', 'weneed', 'empathy', 'ageoftrump'...   \n",
       "30743  ['black', 'trump', 'suppoer', 'smack', 'down',...   \n",
       "6190   ['sick', 'all', 'the', 'pre', 'programmed', 'h...   \n",
       "19746  ['when', 'you', 'blocked', 'troll', 'because',...   \n",
       "19581  ['you', 'forgot', 'democrate', 'because', 'vet...   \n",
       "31767  ['sea', 'shepherd', 'suppoers', 'are', 'racist...   \n",
       "25210             ['porn', 'vids', 'web', 'free', 'sex']   \n",
       "19800  ['amp', 'the', 'democraticpay', 'keep', 'telli...   \n",
       "7812   ['although', 'not', 'bigot', 'misogynist', 'ho...   \n",
       "5074          ['this', 'sooooo', 'may', 'just', 'funny']   \n",
       "6317   ['the', 'people', 'originally', 'meant', 'the'...   \n",
       "5988   ['problem', 'with', 'university', 'that', 'tea...   \n",
       "...                                                  ...   \n",
       "3721   ['check', 'out', 'this', 'new', 'trending', 'f...   \n",
       "28057  ['this', 'reminds', 'this', 'love', 'these', '...   \n",
       "15044  ['livelypics', 'just', 'when', 'you', 'think',...   \n",
       "23222  ['trying', 'not', 'shut', 'down', 'but', 'mayb...   \n",
       "27273  ['first', 'bihday', 'our', 'puppy', 'eloise', ...   \n",
       "30304  ['wa', 'ceain', 'area', 'yeah', 'you', 'not', ...   \n",
       "3928   ['many', 'shit', 'talker', 'bully', 'the', 'yo...   \n",
       "269    ['all', 'ready', 'pay', 'saturday', 'daughter'...   \n",
       "9826   ['feeling', 'little', 'mole', 'tonight', 'food...   \n",
       "11548  ['while', 'still', 'trying', 'get', 'over', 't...   \n",
       "19491  ['fantasy', 'aisle', 'fantasy', 'aisle', 'summ...   \n",
       "31522  ['have', 'wonderful', 'love', 'emikagifts', 'j...   \n",
       "750    ['who', 'want', 'him', 'the', 'next', 'command...   \n",
       "21399        ['beat', 'that', 'cock', 'beatit', 'penis']   \n",
       "16625                     ['lik', 'cri', 'everi', 'tim']   \n",
       "7084   ['speaker', 'dinner', 'eve', 'ricsrural', 'con...   \n",
       "21332  ['best', 'facetime', 'today', 'can', 'wait', '...   \n",
       "28564              ['archery', 'for', 'year', 'archery']   \n",
       "12504  ['whoolo', 'film', 'you', 'can', 'have', 'sad'...   \n",
       "22295    ['what', 'time', 'will', 'the', 'gate', 'open']   \n",
       "13326  ['thank', 'you', 'for', 'new', 'chargehr', 'af...   \n",
       "1147   ['our', 'thought', 'and', 'prayer', 'go', 'out...   \n",
       "19044  ['film', 'bull', 'you', 'will', 'dominate', 'y...   \n",
       "31112  ['this', 'horribly', 'sad', 'news', 'fine', 'a...   \n",
       "14188                   ['follow', 'your', 'hea', 'and']   \n",
       "1034   ['cop', 'giving', 'ticket', 'when', 'they', 'd...   \n",
       "2874   ['singapore', 'city', 'gallery', 'riclswtravel...   \n",
       "8230   ['man', 'been', 'waiting', 'for', 'wehappyfew'...   \n",
       "15277                       ['school', 'almost', 'over']   \n",
       "30396  ['usd', 'cad', 'bounce', 'off', 'despite', 'hi...   \n",
       "\n",
       "                                               lem_tweet  \\\n",
       "565    this government that deliberately toures refug...   \n",
       "21531  despite demoralizing may newyear classism free...   \n",
       "13300  koreans amp joseon people japan will abuse the...   \n",
       "18925  classic yet you jewish bastards wonder why you...   \n",
       "12619  did someone say antisemetic gee you bit triggered   \n",
       "26964  couldn have said this any better nor truthfull...   \n",
       "17273  racism stuffed into skinny jeans with hipster ...   \n",
       "1561   the end me selfie love messi cr religion chris...   \n",
       "17875  trump ally wishes mad cow disease death for ob...   \n",
       "17184  opinion rife the lgbt community gay people can...   \n",
       "8573   allahsoil the cold war was fought over oil tea...   \n",
       "26779  omg these trump suppoers are deplorable dumptr...   \n",
       "15332  sea shepherd suppoers are racist antiracism se...   \n",
       "12290  while can use phrases like sandniggers acceptable   \n",
       "24159  allahsoil enlightenment wasted the wilfully bl...   \n",
       "25895  here comes supermistict douchebag who can only...   \n",
       "23934              hidden america rampant blatant racism   \n",
       "29174       will the alt right promote new kind genetics   \n",
       "10474  us why weneed empathy ageoftrump grassrootsact...   \n",
       "30743  black trump suppoer smacks down cnn repoer for...   \n",
       "6190   sick all the pre programmed hillbots rhetoric ...   \n",
       "19746  when you blocked troll because you promise bla...   \n",
       "19581  you forgot democrate because vetting people fr...   \n",
       "31767  sea shepherd suppoers are racist antiracism se...   \n",
       "25210                             porn vids web free sex   \n",
       "19800  amp the democraticpay keeps telling that blm b...   \n",
       "7812   although not bigot misogynist hope that still ...   \n",
       "5074                          this sooooo may just funny   \n",
       "6317   the people originally meant the white landhold...   \n",
       "5988   problem with universities that teach against b...   \n",
       "...                                                  ...   \n",
       "3721   check out this new trending funny gif pixel ce...   \n",
       "28057  this reminds this love these two they are gold...   \n",
       "15044  livelypics just when you think you know people...   \n",
       "23222  trying not shut down but maybe pokemon will he...   \n",
       "27273  first bihday our puppy eloise sweetbabins dog ...   \n",
       "30304  was ceain areas yeah you not seen the videos s...   \n",
       "3928   many shit talkers bullies the youtube comment ...   \n",
       "269    all ready pay saturday daughter love pay igers...   \n",
       "9826   feeling little mole tonight food foodblogger m...   \n",
       "11548  while still trying get over the shock keshi de...   \n",
       "19491  fantasy aisle fantasy aisle summer grand vacat...   \n",
       "31522  have wonderful love emikagifts jewelrydesign d...   \n",
       "750    who wants him the next commander chief scary d...   \n",
       "21399                        beat that cock beatit penis   \n",
       "16625                                  lik cri everi tim   \n",
       "7084   speakers dinner eve ricsrural conference bette...   \n",
       "21332  best facetime today can wait see boy wednesday...   \n",
       "28564                           archery for year archery   \n",
       "12504     whoolo film you can have sad endings anna torv   \n",
       "22295                      what time will the gates open   \n",
       "13326  thank you for new chargehr after mine broke fi...   \n",
       "1147   our thoughts and prayers goes out everyone inv...   \n",
       "19044  film bull you will dominate your bull and you ...   \n",
       "31112  this horribly sad news fine actor with great t...   \n",
       "14188                                follow your hea and   \n",
       "1034   cops giving tickets when they didn even see yo...   \n",
       "2874   singapore city gallery riclswtravelbook bearlo...   \n",
       "8230      man been waiting for wehappyfew for long xboxe   \n",
       "15277                                schools almost over   \n",
       "30396  usd cad bounces off despite higher oil blog si...   \n",
       "\n",
       "                                              stem_tweet  label  \n",
       "565    this government that deliberately toures refug...      1  \n",
       "21531  despite demoralizing may newyear classism free...      1  \n",
       "13300  koreans amp joseon people japan will abuse the...      1  \n",
       "18925  classic yet you jewish bastards wonder why you...      1  \n",
       "12619      did someone say antisemetic gee you bit trigg      1  \n",
       "26964  couldn have said this any better nor truthfull...      1  \n",
       "17273  racism stuffed into skinny jeans with hipster ...      1  \n",
       "1561   the end me selfie love messi cr religion chris...      1  \n",
       "17875  trump ally wishes mad cow disease death for ob...      1  \n",
       "17184  opinion rife the lgbt community gay people can...      1  \n",
       "8573   allahsoil the cold war was fought over oil tea...      1  \n",
       "26779  omg these trump suppoers are deplorable dumptr...      1  \n",
       "15332  sea shepherd suppoers are racist antiracism se...      1  \n",
       "12290      while can use phrases like sandniggers accept      1  \n",
       "24159  allahsoil enlightenment wasted the wilfully bl...      1  \n",
       "25895  here comes supermistict douchebag who can only...      1  \n",
       "23934                 hidden america rampant blatant rac      1  \n",
       "29174          will the alt right promote new kind genet      1  \n",
       "10474  us why weneed empathy ageoftrump grassrootsact...      1  \n",
       "30743  black trump suppoer smacks down cnn repoer for...      1  \n",
       "6190   sick all the pre programmed hillbots rhetoric ...      1  \n",
       "19746  when you blocked troll because you promise bla...      1  \n",
       "19581  you forgot democrate because vetting people fr...      1  \n",
       "31767  sea shepherd suppoers are racist antiracism se...      1  \n",
       "25210                             porn vids web free sex      1  \n",
       "19800  amp the democraticpay keeps telling that blm b...      1  \n",
       "7812   although not bigot misogynist hope that still ...      1  \n",
       "5074                          this sooooo may just funni      1  \n",
       "6317   the people originally meant the white landhold...      1  \n",
       "5988   problem with universities that teach against b...      1  \n",
       "...                                                  ...    ...  \n",
       "3721   check out this new trending funny gif pixel ce...      0  \n",
       "28057  this reminds this love these two they are gold...      0  \n",
       "15044  livelypics just when you think you know people...      0  \n",
       "23222  trying not shut down but maybe pokemon will he...      0  \n",
       "27273  first bihday our puppy eloise sweetbabins dog ...      0  \n",
       "30304  was ceain areas yeah you not seen the videos s...      0  \n",
       "3928   many shit talkers bullies the youtube comment ...      0  \n",
       "269    all ready pay saturday daughter love pay igers...      0  \n",
       "9826   feeling little mole tonight food foodblogger m...      0  \n",
       "11548  while still trying get over the shock keshi de...      0  \n",
       "19491  fantasy aisle fantasy aisle summer grand vacat...      0  \n",
       "31522  have wonderful love emikagifts jewelrydesign d...      0  \n",
       "750    who wants him the next commander chief scary d...      0  \n",
       "21399                         beat that cock beatit peni      0  \n",
       "16625                                  lik cri everi tim      0  \n",
       "7084   speakers dinner eve ricsrural conference bette...      0  \n",
       "21332  best facetime today can wait see boy wednesday...      0  \n",
       "28564                           archery for year archeri      0  \n",
       "12504     whoolo film you can have sad endings anna torv      0  \n",
       "22295                      what time will the gates open      0  \n",
       "13326  thank you for new chargehr after mine broke fi...      0  \n",
       "1147   our thoughts and prayers goes out everyone inv...      0  \n",
       "19044  film bull you will dominate your bull and you ...      0  \n",
       "31112  this horribly sad news fine actor with great t...      0  \n",
       "14188                                follow your hea and      0  \n",
       "1034   cops giving tickets when they didn even see yo...      0  \n",
       "2874   singapore city gallery riclswtravelbook bearlo...      0  \n",
       "8230       man been waiting for wehappyfew for long xbox      0  \n",
       "15277                                  schools almost ov      0  \n",
       "30396  usd cad bounces off despite higher oil blog si...      0  \n",
       "\n",
       "[37982 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsample_training_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_upsampled = upsample_training_data(X_train, y_train)\n",
    "\n",
    "X_train_up = train_upsampled.drop(['label'], axis = 1)\n",
    "y_train_up = pd.DataFrame(train_upsampled.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    18991\n",
       "0    18991\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_upsampled['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_downsampled = downsample_training_data(X_train, y_train)\n",
    "\n",
    "X_train_down = train_downsampled.drop(['label'], axis = 1)\n",
    "y_train_down = pd.DataFrame(train_downsampled.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1464\n",
       "0    1464\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_downsampled['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Vectorization and Method Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=.001)\n",
    "tfidf_ngram = TfidfVectorizer(ngram_range=(1,2), min_df=.001)\n",
    "tfidf_ngram2 = TfidfVectorizer(ngram_range=(2,3),min_df=.001)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "rfc = RandomForestClassifier(random_state=10)\n",
    "nb = GaussianNB()\n",
    "svc = SVC(random_state=10)\n",
    "\n",
    "vectorization_list = [('COUNT_VECTORIZER', count_vect),\n",
    "                      ('TFIDF_VECTORIZER', tfidf_vectorizer),\n",
    "                      ('TFIDF_NGRAM_1_2', tfidf_ngram),\n",
    "                      ('TFIDF_NGRAM_2_3', tfidf_ngram2)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0e9feb973b7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m NB_compare_vectorization_model(X_train.lem_tweet, y_train, \n\u001b[0;32m----> 2\u001b[0;31m                                    X_val.lem_tweet, y_val, GaussianNB())\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-4166f9486ac3>\u001b[0m in \u001b[0;36mNB_compare_vectorization_model\u001b[0;34m(X_train_col, y_train, X_val_col, y_val, classifier)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mX_val_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val_transformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_transformed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtrain_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train_transformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[0;32m--> 191\u001b[0;31m                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;31m# boost the variance by epsilon, a small fraction of the standard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;31m# deviation of the largest dimension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_smoothing\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_refit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mvar\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m   3365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3366\u001b[0m     return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[0;32m-> 3367\u001b[0;31m                          **kwargs)\n\u001b[0m\u001b[1;32m   3368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NB_compare_vectorization_model(X_train.lem_tweet, y_train, \n",
    "                                   X_val.lem_tweet, y_val, GaussianNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.95\n",
      "Train Precision: 0.88\n",
      "Train Recall: 0.33\n",
      "Train F1: 0.48\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.95\n",
      "Validation Precision: 0.83\n",
      "Validation Recall: 0.3\n",
      "Validation F1: 0.44\n"
     ]
    }
   ],
   "source": [
    "test = wrapper_single_vectorization(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                    logreg, tfidf_vectorizer, apply_smote = False)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.89\n",
      "Train Precision: 0.38\n",
      "Train Recall: 0.85\n",
      "Train F1: 0.53\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.88\n",
      "Validation Precision: 0.31\n",
      "Validation Recall: 0.73\n",
      "Validation F1: 0.43\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4243</td>\n",
       "      <td>541</td>\n",
       "      <td>4784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>241</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4332</td>\n",
       "      <td>782</td>\n",
       "      <td>5114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0    1   All\n",
       "Actual                    \n",
       "0          4243  541  4784\n",
       "1            89  241   330\n",
       "All        4332  782  5114"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = wrapper_single_vectorization(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                    logreg, tfidf_vectorizer, apply_smote = True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'COUNT_VECTORIZER': {'Train Accuracy': 0.99,\n",
       "   'Train Precision': 0.86,\n",
       "   'Train Recall': 1.0,\n",
       "   'Train F1': 0.92,\n",
       "   'Validation Accuracy': 0.95,\n",
       "   'Validation Precision': 0.6,\n",
       "   'Validation Recall': 0.69,\n",
       "   'Validation F1': 0.64},\n",
       "  'TFIDF_VECTORIZER': {'Train Accuracy': 0.89,\n",
       "   'Train Precision': 0.38,\n",
       "   'Train Recall': 0.92,\n",
       "   'Train F1': 0.54,\n",
       "   'Validation Accuracy': 0.87,\n",
       "   'Validation Precision': 0.3,\n",
       "   'Validation Recall': 0.77,\n",
       "   'Validation F1': 0.43},\n",
       "  'TFIDF_NGRAM_1_2': {'Train Accuracy': 0.89,\n",
       "   'Train Precision': 0.4,\n",
       "   'Train Recall': 0.93,\n",
       "   'Train F1': 0.56,\n",
       "   'Validation Accuracy': 0.88,\n",
       "   'Validation Precision': 0.31,\n",
       "   'Validation Recall': 0.77,\n",
       "   'Validation F1': 0.44},\n",
       "  'TFIDF_NGRAM_2_3': {'Train Accuracy': 0.41,\n",
       "   'Train Precision': 0.1,\n",
       "   'Train Recall': 0.94,\n",
       "   'Train F1': 0.19,\n",
       "   'Validation Accuracy': 0.38,\n",
       "   'Validation Precision': 0.09,\n",
       "   'Validation Recall': 0.89,\n",
       "   'Validation F1': 0.16}},\n",
       " <20455x598 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 31135 stored elements in Compressed Sparse Row format>,\n",
       " <5114x598 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 6766 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_cw_lemm = wrapper_compare_vectorizations(X_train.lem_tweet, \n",
    "                            y_train, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', solver = 'lbfgs'), \n",
    "                                            vectorization_list, apply_smote = False)\n",
    "LR_cw_lemm\n",
    "# pd.DataFrame(LR_cw_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT_VECTORIZER</th>\n",
       "      <th>TFIDF_VECTORIZER</th>\n",
       "      <th>TFIDF_NGRAM_1_2</th>\n",
       "      <th>TFIDF_NGRAM_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation F1</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Precision</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Recall</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      COUNT_VECTORIZER  TFIDF_VECTORIZER  TFIDF_NGRAM_1_2  \\\n",
       "Train Accuracy                    0.96              0.89             0.90   \n",
       "Train F1                          0.75              0.53             0.54   \n",
       "Train Precision                   0.66              0.38             0.40   \n",
       "Train Recall                      0.87              0.85             0.86   \n",
       "Validation Accuracy               0.89              0.88             0.88   \n",
       "Validation F1                     0.44              0.43             0.45   \n",
       "Validation Precision              0.32              0.31             0.32   \n",
       "Validation Recall                 0.68              0.73             0.74   \n",
       "\n",
       "                      TFIDF_NGRAM_2_3  \n",
       "Train Accuracy                   0.42  \n",
       "Train F1                         0.18  \n",
       "Train Precision                  0.10  \n",
       "Train Recall                     0.92  \n",
       "Validation Accuracy              0.39  \n",
       "Validation F1                    0.15  \n",
       "Validation Precision             0.08  \n",
       "Validation Recall                0.86  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_cw_lemm = wrapper_compare_vectorizations(X_train.lem_tweet, \n",
    "                            y_train, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', solver = 'lbfgs'),\n",
    "                            vectorization_list, apply_smote= True)\n",
    "pd.DataFrame(LR_cw_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mycsvfile.csv','a') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow('LogisticRegression')\n",
    "    w.writerows(LR_cw_lemm.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame.from_dict(data= LR_cw_lemm)\n",
    "   .to_csv('dict_file.csv', header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression: compare vectorizers using stemming + class balances\n",
    "pd.DataFrame(wrapper_compare_vectorizations(X_train.stem_tweet, \n",
    "                            y_train, X_val.stem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', solver = 'lbfgs'),\n",
    "                            vectorization_list, apply_smote= True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regularization:\n",
    "\n",
    "- Count Vectorizer:   \n",
    "\n",
    "l2 (default), no alpha tuning: F1: 0.99, 0.66\n",
    "C = .1:  .91,  .52\n",
    "C = .2:  .96,  .57\n",
    "C = .3:  .98,  .58\n",
    "C = .01:  .67,  .39\n",
    "C = .001:  .62, .39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_up.lem_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_up.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.lem_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.98\n",
      "Train Precision: 0.99\n",
      "Train Recall: 0.8\n",
      "Train F1: 0.88\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.96\n",
      "Validation Precision: 0.86\n",
      "Validation Recall: 0.5\n",
      "Validation F1: 0.63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4756</td>\n",
       "      <td>28</td>\n",
       "      <td>4784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>164</td>\n",
       "      <td>166</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4920</td>\n",
       "      <td>194</td>\n",
       "      <td>5114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0    1   All\n",
       "Actual                    \n",
       "0          4756   28  4784\n",
       "1           164  166   330\n",
       "All        4920  194  5114"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_vector_model(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, LogisticRegression(), count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.99\n",
      "Train Precision: 0.99\n",
      "Train Recall: 0.99\n",
      "Train F1: 0.99\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.95\n",
      "Validation Precision: 0.6\n",
      "Validation Recall: 0.66\n",
      "Validation F1: 0.63\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1, 2), indices imply (37982, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mform_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1668\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                 \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   1690\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 1691\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   1692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1, 2), indices imply (37982, 2)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-e42fe3e84dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m test, test2 = single_vector_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val,\n\u001b[0;32m----> 2\u001b[0;31m                             LogisticRegression(class_weight='balanced', penalty = 'l1'), count_vect)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-61e68e24afb7>\u001b[0m in \u001b[0;36msingle_vector_model\u001b[0;34m(X_train_col, y_train, X_val_col, y_val, classifier, vectorizer)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation F1: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mconfusion_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrownames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Actual'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmargins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mconfusion_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrownames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Actual'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmargins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/pivot.py\u001b[0m in \u001b[0;36mcrosstab\u001b[0;34m(index, columns, values, rownames, colnames, aggfunc, margins, margins_name, dropna, normalize)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommon_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__dummy__'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    390\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   1669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1671\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 1691\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   1692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1, 2), indices imply (37982, 2)"
     ]
    }
   ],
   "source": [
    "single_vector_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val,\n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1'), count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.86\n",
      "Train Precision: 0.89\n",
      "Train Recall: 0.83\n",
      "Train F1: 0.86\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.89\n",
      "Validation Precision: 0.32\n",
      "Validation Recall: 0.72\n",
      "Validation F1: 0.45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4289</td>\n",
       "      <td>495</td>\n",
       "      <td>4784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>237</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4382</td>\n",
       "      <td>732</td>\n",
       "      <td>5114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0    1   All\n",
       "Actual                    \n",
       "0          4289  495  4784\n",
       "1            93  237   330\n",
       "All        4382  732  5114"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper_single_vectorization(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val,\n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1', C=.05), count_vect,\n",
    "                            apply_smote = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.86\n",
      "Train Precision: 0.89\n",
      "Train Recall: 0.83\n",
      "Train F1: 0.86\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.89\n",
      "Validation Precision: 0.32\n",
      "Validation Recall: 0.72\n",
      "Validation F1: 0.45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4290</td>\n",
       "      <td>494</td>\n",
       "      <td>4784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>237</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4383</td>\n",
       "      <td>731</td>\n",
       "      <td>5114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0    1   All\n",
       "Actual                    \n",
       "0          4290  494  4784\n",
       "1            93  237   330\n",
       "All        4383  731  5114"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper_single_vectorization(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val,\n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1', C=.05), count_vect,\n",
    "                            apply_smote = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.99\n",
      "Train Precision: 0.99\n",
      "Train Recall: 0.99\n",
      "Train F1: 0.99\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.95\n",
      "Validation Precision: 0.6\n",
      "Validation Recall: 0.66\n",
      "Validation F1: 0.63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4638</td>\n",
       "      <td>146</td>\n",
       "      <td>4784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113</td>\n",
       "      <td>217</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4751</td>\n",
       "      <td>363</td>\n",
       "      <td>5114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0    1   All\n",
       "Actual                    \n",
       "0          4638  146  4784\n",
       "1           113  217   330\n",
       "All        4751  363  5114"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper_single_vectorization(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1'), \n",
    "                            count_vect, apply_smote = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.99\n",
      "Train Precision: 0.99\n",
      "Train Recall: 0.99\n",
      "Train F1: 0.99\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.95\n",
      "Validation Precision: 0.6\n",
      "Validation Recall: 0.66\n",
      "Validation F1: 0.63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4638</td>\n",
       "      <td>146</td>\n",
       "      <td>4784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "      <td>218</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4750</td>\n",
       "      <td>364</td>\n",
       "      <td>5114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0    1   All\n",
       "Actual                    \n",
       "0          4638  146  4784\n",
       "1           112  218   330\n",
       "All        4750  364  5114"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper_single_vectorization(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1'), \n",
    "                            count_vect, apply_smote = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Comparison Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_dict, transformed_x, transformed_y = compare_vectorization_model(X_train.lem_tweet, \n",
    "                            y_train, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', solver = 'lbfgs'),\n",
    "                            vectorization_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COUNT_VECTORIZER': {'Train Accuracy': 0.99,\n",
       "  'Train Precision': 0.86,\n",
       "  'Train Recall': 1.0,\n",
       "  'Train F1': 0.92,\n",
       "  'Validation Accuracy': 0.95,\n",
       "  'Validation Precision': 0.6,\n",
       "  'Validation Recall': 0.69,\n",
       "  'Validation F1': 0.65},\n",
       " 'TFIDF_VECTORIZER': {'Train Accuracy': 0.88,\n",
       "  'Train Precision': 0.37,\n",
       "  'Train Recall': 0.91,\n",
       "  'Train F1': 0.53,\n",
       "  'Validation Accuracy': 0.87,\n",
       "  'Validation Precision': 0.3,\n",
       "  'Validation Recall': 0.75,\n",
       "  'Validation F1': 0.43},\n",
       " 'TFIDF_NGRAM_1_2': {'Train Accuracy': 0.89,\n",
       "  'Train Precision': 0.39,\n",
       "  'Train Recall': 0.92,\n",
       "  'Train F1': 0.54,\n",
       "  'Validation Accuracy': 0.87,\n",
       "  'Validation Precision': 0.31,\n",
       "  'Validation Recall': 0.76,\n",
       "  'Validation F1': 0.44},\n",
       " 'TFIDF_NGRAM_2_3': {'Train Accuracy': 0.4,\n",
       "  'Train Precision': 0.1,\n",
       "  'Train Recall': 0.94,\n",
       "  'Train F1': 0.18,\n",
       "  'Validation Accuracy': 0.37,\n",
       "  'Validation Precision': 0.08,\n",
       "  'Validation Recall': 0.9,\n",
       "  'Validation F1': 0.15}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'COUNT_VECTORIZER': {'Train Accuracy': 0.99,\n",
       "   'Train Precision': 0.99,\n",
       "   'Train Recall': 0.99,\n",
       "   'Train F1': 0.99,\n",
       "   'Validation Accuracy': 0.95,\n",
       "   'Validation Precision': 0.6,\n",
       "   'Validation Recall': 0.66,\n",
       "   'Validation F1': 0.63},\n",
       "  'TFIDF_VECTORIZER': {'Train Accuracy': 0.95,\n",
       "   'Train Precision': 0.94,\n",
       "   'Train Recall': 0.96,\n",
       "   'Train F1': 0.95,\n",
       "   'Validation Accuracy': 0.91,\n",
       "   'Validation Precision': 0.39,\n",
       "   'Validation Recall': 0.72,\n",
       "   'Validation F1': 0.51},\n",
       "  'TFIDF_NGRAM_1_2': {'Train Accuracy': 0.96,\n",
       "   'Train Precision': 0.95,\n",
       "   'Train Recall': 0.98,\n",
       "   'Train F1': 0.96,\n",
       "   'Validation Accuracy': 0.92,\n",
       "   'Validation Precision': 0.44,\n",
       "   'Validation Recall': 0.73,\n",
       "   'Validation F1': 0.55},\n",
       "  'TFIDF_NGRAM_2_3': {'Train Accuracy': 0.74,\n",
       "   'Train Precision': 0.89,\n",
       "   'Train Recall': 0.54,\n",
       "   'Train F1': 0.67,\n",
       "   'Validation Accuracy': 0.9,\n",
       "   'Validation Precision': 0.28,\n",
       "   'Validation Recall': 0.35,\n",
       "   'Validation F1': 0.31}},\n",
       " <37982x970 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 79154 stored elements in Compressed Sparse Row format>,\n",
       " <5114x970 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 6080 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper_compare_vectorizations(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1'), \n",
    "                            vectorization_list, apply_smote = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COUNT_VECTORIZER': {'Train Accuracy': 0.99,\n",
       "  'Train Precision': 0.99,\n",
       "  'Train Recall': 0.99,\n",
       "  'Train F1': 0.99,\n",
       "  'Validation Accuracy': 0.95,\n",
       "  'Validation Precision': 0.6,\n",
       "  'Validation Recall': 0.66,\n",
       "  'Validation F1': 0.63},\n",
       " 'TFIDF_VECTORIZER': {'Train Accuracy': 0.95,\n",
       "  'Train Precision': 0.94,\n",
       "  'Train Recall': 0.96,\n",
       "  'Train F1': 0.95,\n",
       "  'Validation Accuracy': 0.91,\n",
       "  'Validation Precision': 0.39,\n",
       "  'Validation Recall': 0.72,\n",
       "  'Validation F1': 0.51},\n",
       " 'TFIDF_NGRAM_1_2': {'Train Accuracy': 0.96,\n",
       "  'Train Precision': 0.95,\n",
       "  'Train Recall': 0.98,\n",
       "  'Train F1': 0.96,\n",
       "  'Validation Accuracy': 0.92,\n",
       "  'Validation Precision': 0.44,\n",
       "  'Validation Recall': 0.73,\n",
       "  'Validation F1': 0.55},\n",
       " 'TFIDF_NGRAM_2_3': {'Train Accuracy': 0.74,\n",
       "  'Train Precision': 0.89,\n",
       "  'Train Recall': 0.54,\n",
       "  'Train F1': 0.67,\n",
       "  'Validation Accuracy': 0.9,\n",
       "  'Validation Precision': 0.28,\n",
       "  'Validation Recall': 0.35,\n",
       "  'Validation F1': 0.31}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper_compare_vectorizations(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1'), \n",
    "                            vectorization_list, apply_smote = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Log Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_single_vectorization(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1'), \n",
    "                            count_vect, apply_smote = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(class_weight='balanced', penalty = 'l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countvect =  count_vect.fit_transform(X_train_up.lem_tweet)\n",
    "X_val_countvect =  count_vect.transform(X_val.lem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log = logreg.fit(X_train_countvect, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = logreg.predict(X_val_countvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(y_val, y_val_pred), index = ['actual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = log.decision_function(X_val_countvect)\n",
    "   \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_score)\n",
    "y_train_score = model_log.decision_function(X_train_countvect)\n",
    "train_fpr, train_tpr, thresholds = roc_curve(y_train_up, y_train_score)\n",
    "#Seaborns Beautiful Styling\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "plt.figure(figsize=(10,8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve for Validation Set')\n",
    "plt.legend(loc=\"lower right\")\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision = average_precision_score(y_val, y_val_pred)\n",
    "\n",
    "print('Average precision-recall score RF: {}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_predictions = []\n",
    "for item in logreg.predict_proba(X_val_countvect):\n",
    "    if item[0] <= .85:\n",
    "        weighted_predictions.append(1)\n",
    "    else:\n",
    "        weighted_predictions.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original predictions\n",
    "pd.DataFrame(confusion_matrix(y_val, y_val_pred), index = ['actual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with threshhold adjustment\n",
    "pd.DataFrame(confusion_matrix(y_val, weighted_predictions), index = ['actual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_predict_prob = log.predict_proba(X_val_countvect)\n",
    "\n",
    "y_val_predict_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(y_val_predict_prob)\n",
    "y_val = pd.DataFrame(y_val)\n",
    "y_val.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_val = pd.DataFrame(y_val)\n",
    "y_val.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['actual_class']=y_val\n",
    "pred_df['predicted_class']=y_val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[pred_df['actual_class'] != pred_df['predicted_class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tidy_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class weight = balanced + lemmatized\n",
    "compare_vectorization_model(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                   SVC(class_weight ='balanced', gamma='auto', ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upsampling + lemmatized\n",
    "compare_vectorization_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                                   SVC(class_weight ='balanced', gamma ='auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE + lemmatized \n",
    "SMOTE_compare_vectorization_model(X_train.lem_tweet, y_train, X_val.lem_tweet, \n",
    "                                    y_val, SVC(class_weight ='balanced', gamma='auto', ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Searching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfid2 =  tfidf_ngram2.fit_transform(X_train_up.lemmatized_tweet)\n",
    "X_val_tfid2 =  tfidf_ngram2.transform(X_val.lemmatized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = SVC(kernel='linear', C=1, gamma=1, class_weight ='balanced')\n",
    "\n",
    "params = {\n",
    "'C': [0.1,.2, .3, 0.8,1,1.2,1.4],\n",
    "'kernel':['linear', 'rbf'],\n",
    "'gamma' :[0.1,0.8,1,1.2,1.4]\n",
    "}\n",
    "\n",
    "svm_gs= GridSearchCV(svc, param_grid = params, cv = 3)\n",
    "\n",
    "scores = ['f1','accuracy','recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_gs.fit(X_train_tfid2, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_vector_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vectorization_model(X_train_up.lemmatized_tweet, y_train_up, X_val.lemmatized_tweet, y_val, \n",
    "                                   SVC(C=1.2, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma=1.4, kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Multiple Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with class weight balances + lemmatizing \n",
    "\n",
    "pd.DataFrame(compare_vectorization_model(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 20, \n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with upsampling + lemmatizing \n",
    "compare_vectorization_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 20,\n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with SMOTE + lemmatizing  \n",
    "SMOTE_compare_vectorization_model(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 20,\n",
    "                                   n_estimators = 100, class_weight = 'balanced', random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with upsampling + stemming\n",
    "compare_vectorization_model(X_train.stem_tweet, y_train, X_val.stem_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 20,\n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Fine-Tuning Hyperparameters: Max depth 10.... regularization??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with upsampling + lemmatizing \n",
    "compare_vectorization_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 10,\n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(compare_vectorization_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                                   RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
    "                       criterion='gini', max_depth=20, max_features='auto',\n",
    "                       max_leaf_nodes=200, min_impurity_decrease=0.0,\n",
    "                       min_impurity_split=None, min_samples_leaf=1,\n",
    "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                       n_estimators=100, n_jobs=None, oob_score=False,\n",
    "                       random_state=10, verbose=0, warm_start=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# rfc = RandomForestClassifier(n_estimators=60, max_depth=6, random_state=10, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countvect =  count_vect.fit_transform(X_train_up.lem_tweet)\n",
    "X_val_countvect =  count_vect.transform(X_val.lem_tweet)\n",
    "# X_test_countvect = count_vect.transform(X_test.lemmatized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "parameters = {'n_estimators' : [40, 60, 80, 100],\n",
    "'max_leaf_nodes' : [200, 400, 600],\n",
    "'random_state' : [10],\n",
    "'max_depth': [5, 7, 10, 20],\n",
    " 'verbose' : [0],\n",
    "'class_weight': ['balanced']\n",
    "             }\n",
    "          \n",
    "rfc_gs = GridSearchCV(\n",
    "    RandomForestClassifier(class_weight='balanced', random_state = 10), param_grid=parameters, cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.fit(X_train_countvect, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.score(X_val_countvect, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc2 = RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
    "                       criterion='gini', max_depth=20, max_features='auto',\n",
    "                       max_leaf_nodes=200, min_impurity_decrease=0.0,\n",
    "                       min_impurity_split=None, min_samples_leaf=1,\n",
    "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                       n_estimators=100, n_jobs=None, oob_score=False,\n",
    "                       random_state=10, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc2.fit (X_train_countvect, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict = rfc2.predict(X_train_countvect)\n",
    "metrics.f1_score(y_train_up, y_train_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_predict = rfc2.predict(X_val_countvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_val, y_val_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vectorization_model(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                     RandomForestClassifier(class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec = gensim.models.Word2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = time()\n",
    "\n",
    "# word2vec.build_vocab(df_tokenized_list, progress_per=10000)\n",
    "\n",
    "# print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.tokenized_tweet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train.tokenized_tweet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X-train pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.tokenized_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train['tokenized_tweet']= X_train['tokenized_tweet'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_list = list(X_train.tokenized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_token_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_sumlist = sum(X_train_token_list,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_sumlist = sum(X_train_token_list,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unique_tokens = set(X_train_token_sumlist)\n",
    "print('The unique number of words in the training dataset is: {}'.format(len(X_train_unique_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X-val pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val['tokenized_tweet']= X_val['tokenized_tweet'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_token_list = list(X_val['tokenized_tweet'])\n",
    "X_val_token_sumlist = sum(X_val_token_list,[])\n",
    "X_val_unique_tokens = set(X_val_token_sumlist)\n",
    "\n",
    "print('The unique number of words in the validation dataset is: {}'.format(len(X_val_unique_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X-test pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_token_list = list(X_test['tokenized_tweet'])\n",
    "# X_test_token_sumlist = sum(X_test_token_list,[])\n",
    "\n",
    "# X_test_unique_tokens = set(X_test_token_sumlist)\n",
    "# print('The unique number of words in the training dataset is: {}'.format(len(X_test_unique_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "t = time()\n",
    "\n",
    "w2v = gensim.models.Word2Vec(X_train_token_list, sg=1, min_count=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.train(X_train_token_list, total_examples=w2v.corpus_count, epochs=w2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v.save('w2v-min1.model')\n",
    "# w2v = gensim.models.Word2Vec.load('w2v-min1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w2v.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocab= w2v.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv['trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(['trump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(positive=['lazy','black'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.get_keras_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_X = w2v.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = X_train_token_list[1]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([w2v[w] for w in sentence if w in w2v]\n",
    "                   or [np.zeros(100)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr = np.empty((31410, 100))\n",
    "for sentence in X_train_token_list:\n",
    "    np.append(input_to_lr, np.mean([w2v[w] for w in sentence if w in w2v]\n",
    "                   or [np.zeros(100)], axis=0))\n",
    "# np.mean([w2v[w] for w in sentence if w in w2v], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp = input_to_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp_df = pd.DataFrame(X_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.fit(X_train_temp, y)\n",
    "a.score(X_train_temp, y)\n",
    "c = a.predict(X_train_temp)\n",
    "# print scores  \n",
    "print('Train Accuracy: ' + str(round(metrics.f1_score(y, c),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample, X_train_remainder, y_train_sample, y_train_remainder = train_test_split(X_train, y_train, test_size=0.99, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_RNN_sample= X_train_sample['tokenized_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_RNN_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_RNN_sample=y_train_sample\n",
    "y_RNN_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = X_RNN_sample\n",
    "# define class labels\n",
    "labels = y_RNN_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "vocab_size = 100\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.Word2Vec(df_tokenized_list, size=dimsize, window=5, min_count=50, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne_plot(w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of hidden layer (length of continuous word representation)\n",
    "dimsize= 100\n",
    "\n",
    "# model_w2v = gensim.models.Word2Vec(X_train_token_list, size= dimsize, window=5, min_count=1, workers=4)\n",
    "model_w2v = gensim.models.Word2Vec(X_train_token_list, size= dimsize,min_count=1)\n",
    "\n",
    "#create average vector for train and test from model\n",
    "#returned list of numpy arrays are then stacked \n",
    "X_train_w2v = np.concatenate([avg_word_vectors(w, dimsize, model_w2v) for w in X_train_token_list])\n",
    "X_val_w2v = np.concatenate([avg_word_vectors(w,dimsize, model_w2v) for w in X_val_token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_w2v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr = np.empty((31410, 100))\n",
    "for sentence in X_train_token_list:\n",
    "    np.append(input_to_lr, np.mean([model_w2v[w] for w in sentence if w in model_w2v]\n",
    "                   or [np.zeros(100)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_w2v_model (X_train_w2v, y_train, X_val_w2v, y_val, classifier):\n",
    "        \n",
    "    smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "        \n",
    "    pipe = make_pipeline(smote, classifier) \n",
    "    \n",
    "    model = pipe.fit(X_train_w2v, y_train)\n",
    "    \n",
    "    train_predictions = model.predict(X_train_w2v)\n",
    "    val_predictions = model.predict (X_val_w2v)\n",
    "    \n",
    "   # print scores  \n",
    "    print('Train Accuracy: ' + str(round(metrics.accuracy_score(y_train, train_predictions),2)))\n",
    "    print('Train Precision: ' + str(round(metrics.precision_score(y_train, train_predictions),2)))\n",
    "    print('Train Recall: ' + str(round(metrics.recall_score(y_train, train_predictions),2)))\n",
    "    print('Train F1: ' + str(round(metrics.f1_score(y_train, train_predictions),2)))\n",
    "    print('\\n')\n",
    "    print('Validation Accuracy: ' + str(round(metrics.accuracy_score(y_val, val_predictions),2)))\n",
    "    print('Validation Precision: ' + str(round(metrics.precision_score(y_val, val_predictions),2)))\n",
    "    print('Validation Recall: ' + str(round(metrics.recall_score(y_val, val_predictions),2)))\n",
    "    print('Validation F1: ' + str(round(metrics.f1_score(y_val, val_predictions),2)))\n",
    "\n",
    "    log_confusion_test = pd.crosstab(y_val, val_predictions, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    return log_confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model(X_train_w2v, y_train, X_val_w2v, y_val, LogisticRegression(solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model(X_train_w2v, y_train, X_val_w2v, y_val, RandomForestClassifier(n_estimators=100, max_depth= 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_smote_w2v_model (X_train_w2v, y_train, X_val_w2v, y_val, classifier):\n",
    "    \n",
    "    pca = decomposition.PCA(n_components=50)\n",
    "    \n",
    "    smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "        \n",
    "    pipe = make_pipeline(pca, smote, classifier) \n",
    "    \n",
    "    model = pipe.fit(X_train_w2v, y_train)\n",
    "    \n",
    "    train_predictions = model.predict(X_train_w2v)\n",
    "    val_predictions = model.predict (X_val_w2v)\n",
    "    \n",
    "   # print scores  \n",
    "    print('Train Accuracy: ' + str(round(metrics.accuracy_score(y_train, train_predictions),2)))\n",
    "    print('Train Precision: ' + str(round(metrics.precision_score(y_train, train_predictions),2)))\n",
    "    print('Train Recall: ' + str(round(metrics.recall_score(y_train, train_predictions),2)))\n",
    "    print('Train F1: ' + str(round(metrics.f1_score(y_train, train_predictions),2)))\n",
    "    print('\\n')\n",
    "    print('Validation Accuracy: ' + str(round(metrics.accuracy_score(y_val, val_predictions),2)))\n",
    "    print('Validation Precision: ' + str(round(metrics.precision_score(y_val, val_predictions),2)))\n",
    "    print('Validation Recall: ' + str(round(metrics.recall_score(y_val, val_predictions),2)))\n",
    "    print('Validation F1: ' + str(round(metrics.f1_score(y_val, val_predictions),2)))\n",
    "\n",
    "    log_confusion_test = pd.crosstab(y_val, val_predictions, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    return log_confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_smote_w2v_model(X_train_w2v, y_train, X_val_w2v, y_val, LogisticRegression(solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_smote_w2v_model(X_train_w2v, y_train, X_val_w2v, y_val, RandomForestClassifier(n_estimators=100, max_depth=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install glove_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install glovepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = 'glove.twitter.27B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_input_file = 'glove.twitter.27B.100d.txt'\n",
    "glove_output_file = 'glove.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, glove_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = KeyedVectors.load_word2vec_format('glove.txt.word2vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glove_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove = np.concatenate([avg_word_vectors(w, dimsize, glove_model) for w in X_train_token_list])\n",
    "X_val_glove = np.concatenate([avg_word_vectors(w, dimsize, glove_model) for w in X_val_token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove[255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove_2 = np.empty((31410, 100))\n",
    "for sentence in X_train_token_list:\n",
    "    np.append(input_to_lr, np.mean([glove_model[w] for w in sentence if w in glove_model]\n",
    "                   or [np.zeros(100)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove_2[225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Learnco "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict = {}\n",
    "with open('glove.twitter.27B.100d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in X_train_unique_tokens:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_to_lr = np.empty((31410, 100))\n",
    "for sentence in X_train_token_list:\n",
    "    np.append(input_to_lr, np.mean([w2v[w] for w in sentence if w in w2v]\n",
    "                   or [np.zeros(100)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_w2v_model (X_train_w2v, y_train, X_val_w2v, y_val, classifier):\n",
    "        \n",
    "    smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "        \n",
    "    pipe = make_pipeline(smote, classifier) \n",
    "    \n",
    "    model = pipe.fit(X_train_w2v, y_train)\n",
    "    \n",
    "    train_predictions = model.predict(X_train_w2v)\n",
    "    val_predictions = model.predict (X_val_w2v)\n",
    "    \n",
    "   # print scores  \n",
    "    print('Train Accuracy: ' + str(round(metrics.accuracy_score(y_train, train_predictions),2)))\n",
    "    print('Train Precision: ' + str(round(metrics.precision_score(y_train, train_predictions),2)))\n",
    "    print('Train Recall: ' + str(round(metrics.recall_score(y_train, train_predictions),2)))\n",
    "    print('Train F1: ' + str(round(metrics.f1_score(y_train, train_predictions),2)))\n",
    "    print('\\n')\n",
    "    print('Validation Accuracy: ' + str(round(metrics.accuracy_score(y_val, val_predictions),2)))\n",
    "    print('Validation Precision: ' + str(round(metrics.precision_score(y_val, val_predictions),2)))\n",
    "    print('Validation Recall: ' + str(round(metrics.recall_score(y_val, val_predictions),2)))\n",
    "    print('Validation F1: ' + str(round(metrics.f1_score(y_val, val_predictions),2)))\n",
    "\n",
    "    log_confusion_test = pd.crosstab(y_val, val_predictions, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    return log_confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model(X_train_glove, y_train, X_val_glove, y_val, RandomForestClassifier(n_estimators=100, max_depth=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, RandomForestClassifier(max_depth=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, RandomForestClassifier(max_depth=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, LogisticRegression(penalty ='l1', C = 10,\n",
    "                                                                            class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, LogisticRegression(penalty ='l1', C = .001,\n",
    "                                                                            class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, LogisticRegression(penalty ='l1', class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, \n",
    "                 LogisticRegression(penalty ='l2', C = 5, class_weight ={0: 5 , 1: 5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, \n",
    "                 LogisticRegression(penalty ='l2', C = .1, class_weight ={0: 5 , 1: 5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Trump Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trump_df= pd.read_csv('data/cleaned-trump-tweet.csv')\n",
    "trump_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countvect =  count_vect.fit_transform(X_train_up.lem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train_countvect, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trump = count_vect.transform(trump_df.lem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trump = X_trump.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trump.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trump_predict = logreg.predict(X_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trump_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df['predictions'] = y_trump_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trump_predict_prob = logreg.predict_proba(X_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trump_predict_prob = pd.DataFrame(y_trump_predict_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df['predict_probability'] = y_trump_predict_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df = trump_df[['tweet','predictions', 'predict_probability']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump0 = trump_df[trump_df.predictions == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump0.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df[trump_df.predictions == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump1 = trump_df[trump_df.predictions == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump1.tweet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
