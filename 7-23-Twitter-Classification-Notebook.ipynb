{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import sklearn\n",
    "\n",
    "# NLTK/NLP\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import string, re\n",
    "import urllib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from nltk.collocations import *\n",
    "import gensim\n",
    "\n",
    "# Classifiers \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Sampling\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import sklearn.decomposition as decomposition\n",
    "\n",
    "#Visualization\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import customized functions\n",
    "# import import_ipynb\n",
    "# from custom_functions import *\n",
    "\n",
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cleaned-reshuffled.csv')\n",
    "df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "df.lem_tweet= df.lem_tweet.apply(str)\n",
    "df.stem_tweet= df.stem_tweet.apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into train and test \n",
    "X_model, X_test, y_model, y_test = train_test_split(X, y, stratify = y,  test_size=0.20, random_state=123)\n",
    "\n",
    "#splitting \"model\" into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_model, y_model, test_size=0.20, random_state=123)\n",
    "\n",
    "# df_train_full = X_train.copy()\n",
    "# df_train_full['label']= y_train\n",
    "# train_full_df.to_csv('train_full_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.929854\n",
       "1    0.070146\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling and Downsampling Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>no_hash_tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lem_tweet</th>\n",
       "      <th>stem_tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20676</td>\n",
       "      <td>@user f*** this  ð¦ðº government that deli...</td>\n",
       "      <td>this government that deliberately toures #refu...</td>\n",
       "      <td>this government that deliberately toures refug...</td>\n",
       "      <td>['this', 'government', 'that', 'deliberately',...</td>\n",
       "      <td>['this', 'govern', 'that', 'deliber', 'tour', ...</td>\n",
       "      <td>['this', 'government', 'that', 'deliberately',...</td>\n",
       "      <td>this government that deliberately toures refug...</td>\n",
       "      <td>this government that deliberately toures refug...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21531</th>\n",
       "      <td>24025</td>\n",
       "      <td>despite a demoralizing 2016: may ur #newyear20...</td>\n",
       "      <td>despite demoralizing #newyear #classism free #...</td>\n",
       "      <td>despite demoralizing newyear classism free sag...</td>\n",
       "      <td>['despite', 'demoralizing', 'newyear', 'classi...</td>\n",
       "      <td>['despit', 'demor', 'newyear', 'classism', 'fr...</td>\n",
       "      <td>['despite', 'demoralizing', 'newyear', 'classi...</td>\n",
       "      <td>despite demoralizing newyear classism free sag...</td>\n",
       "      <td>despite demoralizing newyear classism free sag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13300</th>\n",
       "      <td>18145</td>\n",
       "      <td>@user #koreans &amp;amp; joseon people in japan, w...</td>\n",
       "      <td>#koreans joseon people japan will abuse claims...</td>\n",
       "      <td>koreans joseon people japan will abuse claims ...</td>\n",
       "      <td>['koreans', 'joseon', 'people', 'japan', 'will...</td>\n",
       "      <td>['korean', 'joseon', 'peopl', 'japan', 'will',...</td>\n",
       "      <td>['korean', 'joseon', 'people', 'japan', 'will'...</td>\n",
       "      <td>koreans joseon people japan will abuse claims ...</td>\n",
       "      <td>koreans joseon people japan will abuse claims ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18925</th>\n",
       "      <td>8506</td>\n",
       "      <td>@user @user @user @user classic ! yet you jewi...</td>\n",
       "      <td>classic jewish bastards wonder sooo hated world</td>\n",
       "      <td>classic jewish bastards wonder sooo hated world</td>\n",
       "      <td>['classic', 'jewish', 'bastards', 'wonder', 's...</td>\n",
       "      <td>['classic', 'jewish', 'bastard', 'wonder', 'so...</td>\n",
       "      <td>['classic', 'jewish', 'bastard', 'wonder', 'so...</td>\n",
       "      <td>classic jewish bastards wonder sooo hated world</td>\n",
       "      <td>classic jewish bastards wonder sooo hated world</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12619</th>\n",
       "      <td>15464</td>\n",
       "      <td>@user did someone say #antisemetic ? gee (((@u...</td>\n",
       "      <td>someone #antisemetic triggered</td>\n",
       "      <td>someone antisemetic triggered</td>\n",
       "      <td>['someone', 'antisemetic', 'triggered']</td>\n",
       "      <td>['someon', 'antisemet', 'trigger']</td>\n",
       "      <td>['someone', 'antisemetic', 'triggered']</td>\n",
       "      <td>someone antisemetic triggered</td>\n",
       "      <td>someone antisemetic trigg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26964</th>\n",
       "      <td>28937</td>\n",
       "      <td>couldn't have said this any better nor truthfu...</td>\n",
       "      <td>couldn have said this better truthfully donny ...</td>\n",
       "      <td>couldn have said this better truthfully donny ...</td>\n",
       "      <td>['couldn', 'have', 'said', 'this', 'better', '...</td>\n",
       "      <td>['couldn', 'have', 'said', 'this', 'better', '...</td>\n",
       "      <td>['couldn', 'have', 'said', 'this', 'better', '...</td>\n",
       "      <td>couldn have said this better truthfully donny ...</td>\n",
       "      <td>couldn have said this better truthfully donny ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17273</th>\n",
       "      <td>25291</td>\n",
       "      <td>@user racism stuffed into skinny jeans with a ...</td>\n",
       "      <td>racism stuffed into skinny jeans with hipster ...</td>\n",
       "      <td>racism stuffed into skinny jeans with hipster ...</td>\n",
       "      <td>['racism', 'stuffed', 'into', 'skinny', 'jeans...</td>\n",
       "      <td>['racism', 'stuf', 'into', 'skinni', 'jean', '...</td>\n",
       "      <td>['racism', 'stuffed', 'into', 'skinny', 'jean'...</td>\n",
       "      <td>racism stuffed into skinny jeans with hipster ...</td>\n",
       "      <td>racism stuffed into skinny jeans with hipster ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>12717</td>\n",
       "      <td>the end of   #me #selfie # #love #messi #cr7 #...</td>\n",
       "      <td>#selfie #love #messi #religion #christianity #...</td>\n",
       "      <td>selfie love messi religion christianity mecca ...</td>\n",
       "      <td>['selfie', 'love', 'messi', 'religion', 'chris...</td>\n",
       "      <td>['selfi', 'love', 'messi', 'religion', 'christ...</td>\n",
       "      <td>['selfie', 'love', 'messi', 'religion', 'chris...</td>\n",
       "      <td>selfie love messi religion christianity mecca ...</td>\n",
       "      <td>selfie love messi religion christianity mecca l</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17875</th>\n",
       "      <td>11612</td>\n",
       "      <td>trump ally wishes mad cow disease death for ob...</td>\n",
       "      <td>trump ally wishes disease death obama #unfitto...</td>\n",
       "      <td>trump ally wishes disease death obama unfittob...</td>\n",
       "      <td>['trump', 'ally', 'wishes', 'disease', 'death'...</td>\n",
       "      <td>['trump', 'alli', 'wish', 'diseas', 'death', '...</td>\n",
       "      <td>['trump', 'ally', 'wish', 'disease', 'death', ...</td>\n",
       "      <td>trump ally wishes disease death obama unfittob...</td>\n",
       "      <td>trump ally wishes disease death obama unfittob...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17184</th>\n",
       "      <td>20554</td>\n",
       "      <td>opinion:  is rife in the #lgbt community. #gay...</td>\n",
       "      <td>opinion rife #lgbt community #gay people deman...</td>\n",
       "      <td>opinion rife lgbt community gay people demand ...</td>\n",
       "      <td>['opinion', 'rife', 'lgbt', 'community', 'gay'...</td>\n",
       "      <td>['opinion', 'rife', 'lgbt', 'communiti', 'gay'...</td>\n",
       "      <td>['opinion', 'rife', 'lgbt', 'community', 'gay'...</td>\n",
       "      <td>opinion rife lgbt community gay people demand ...</td>\n",
       "      <td>opinion rife lgbt community gay people demand ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8573</th>\n",
       "      <td>25151</td>\n",
       "      <td>@user #allahsoil the cold war was fought over ...</td>\n",
       "      <td>#allahsoil cold fought over #teambts #teamsupe...</td>\n",
       "      <td>allahsoil cold fought over teambts teamsuperju...</td>\n",
       "      <td>['allahsoil', 'cold', 'fought', 'over', 'teamb...</td>\n",
       "      <td>['allahsoil', 'cold', 'fought', 'over', 'teamb...</td>\n",
       "      <td>['allahsoil', 'cold', 'fought', 'over', 'teamb...</td>\n",
       "      <td>allahsoil cold fought over teambts teamsuperju...</td>\n",
       "      <td>allahsoil cold fought over teambts teamsuperju...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26779</th>\n",
       "      <td>13585</td>\n",
       "      <td>omg, these trump suppoers are deplorable! #dum...</td>\n",
       "      <td>these trump suppoers deplorable #dumptrump #no...</td>\n",
       "      <td>these trump suppoers deplorable dumptrump notm...</td>\n",
       "      <td>['these', 'trump', 'suppoers', 'deplorable', '...</td>\n",
       "      <td>['these', 'trump', 'suppoer', 'deplor', 'dumpt...</td>\n",
       "      <td>['these', 'trump', 'suppoers', 'deplorable', '...</td>\n",
       "      <td>these trump suppoers deplorable dumptrump notm...</td>\n",
       "      <td>these trump suppoers deplorable dumptrump notm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15332</th>\n",
       "      <td>2581</td>\n",
       "      <td>sea shepherd suppoers are racist!   #antiracis...</td>\n",
       "      <td>shepherd suppoers racist #antiracism #seashepherd</td>\n",
       "      <td>shepherd suppoers racist antiracism seashepherd</td>\n",
       "      <td>['shepherd', 'suppoers', 'racist', 'antiracism...</td>\n",
       "      <td>['shepherd', 'suppoer', 'racist', 'antirac', '...</td>\n",
       "      <td>['shepherd', 'suppoers', 'racist', 'antiracism...</td>\n",
       "      <td>shepherd suppoers racist antiracism seashepherd</td>\n",
       "      <td>shepherd suppoers racist antiracism seashepherd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12290</th>\n",
       "      <td>6519</td>\n",
       "      <td>.@user @user while @user can use phrases like ...</td>\n",
       "      <td>while phrases like #sandniggers acceptable</td>\n",
       "      <td>while phrases like sandniggers acceptable</td>\n",
       "      <td>['while', 'phrases', 'like', 'sandniggers', 'a...</td>\n",
       "      <td>['while', 'phrase', 'like', 'sandnigg', 'accept']</td>\n",
       "      <td>['while', 'phrase', 'like', 'sandniggers', 'ac...</td>\n",
       "      <td>while phrases like sandniggers acceptable</td>\n",
       "      <td>while phrases like sandniggers accept</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24159</th>\n",
       "      <td>9563</td>\n",
       "      <td>@user #allahsoil enlightenment is wasted on th...</td>\n",
       "      <td>#allahsoil enlightenment wasted wilfully blind...</td>\n",
       "      <td>allahsoil enlightenment wasted wilfully blind ...</td>\n",
       "      <td>['allahsoil', 'enlightenment', 'wasted', 'wilf...</td>\n",
       "      <td>['allahsoil', 'enlighten', 'wast', 'wil', 'bli...</td>\n",
       "      <td>['allahsoil', 'enlightenment', 'wasted', 'wilf...</td>\n",
       "      <td>allahsoil enlightenment wasted wilfully blind ...</td>\n",
       "      <td>allahsoil enlightenment wasted wilfully blind ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25895</th>\n",
       "      <td>8452</td>\n",
       "      <td>@user here comes a  #supermistict douchebag wh...</td>\n",
       "      <td>here comes #supermistict douchebag only poke n...</td>\n",
       "      <td>here comes supermistict douchebag only poke no...</td>\n",
       "      <td>['here', 'comes', 'supermistict', 'douchebag',...</td>\n",
       "      <td>['here', 'come', 'supermistict', 'douchebag', ...</td>\n",
       "      <td>['here', 'come', 'supermistict', 'douchebag', ...</td>\n",
       "      <td>here comes supermistict douchebag only poke no...</td>\n",
       "      <td>here comes supermistict douchebag only poke no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23934</th>\n",
       "      <td>17750</td>\n",
       "      <td>@user hidden  in #america is as rampant as bla...</td>\n",
       "      <td>hidden #america rampant blatant racism</td>\n",
       "      <td>hidden america rampant blatant racism</td>\n",
       "      <td>['hidden', 'america', 'rampant', 'blatant', 'r...</td>\n",
       "      <td>['hidden', 'america', 'rampant', 'blatant', 'r...</td>\n",
       "      <td>['hidden', 'america', 'rampant', 'blatant', 'r...</td>\n",
       "      <td>hidden america rampant blatant racism</td>\n",
       "      <td>hidden america rampant blatant rac</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29174</th>\n",
       "      <td>21389</td>\n",
       "      <td>will the alt-right promote a new kind of  gene...</td>\n",
       "      <td>will right promote kind genetics</td>\n",
       "      <td>will right promote kind genetics</td>\n",
       "      <td>['will', 'right', 'promote', 'kind', 'genetics']</td>\n",
       "      <td>['will', 'right', 'promot', 'kind', 'genet']</td>\n",
       "      <td>['will', 'right', 'promote', 'kind', 'genetics']</td>\n",
       "      <td>will right promote kind genetics</td>\n",
       "      <td>will right promote kind genet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10474</th>\n",
       "      <td>23431</td>\n",
       "      <td>#us why weneed #empathy #ageoftrump #grassroot...</td>\n",
       "      <td>weneed #empathy #ageoftrump #grassrootsaction ...</td>\n",
       "      <td>weneed empathy ageoftrump grassrootsaction cit...</td>\n",
       "      <td>['weneed', 'empathy', 'ageoftrump', 'grassroot...</td>\n",
       "      <td>['wene', 'empathi', 'ageoftrump', 'grassrootsa...</td>\n",
       "      <td>['weneed', 'empathy', 'ageoftrump', 'grassroot...</td>\n",
       "      <td>weneed empathy ageoftrump grassrootsaction cit...</td>\n",
       "      <td>weneed empathy ageoftrump grassrootsaction cit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30743</th>\n",
       "      <td>12301</td>\n",
       "      <td>black trump suppoer smacks down cnn repoer for...</td>\n",
       "      <td>black trump suppoer smacks down repoer race ba...</td>\n",
       "      <td>black trump suppoer smacks down repoer race ba...</td>\n",
       "      <td>['black', 'trump', 'suppoer', 'smacks', 'down'...</td>\n",
       "      <td>['black', 'trump', 'suppoer', 'smack', 'down',...</td>\n",
       "      <td>['black', 'trump', 'suppoer', 'smack', 'down',...</td>\n",
       "      <td>black trump suppoer smacks down repoer race ba...</td>\n",
       "      <td>black trump suppoer smacks down repoer race ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6190</th>\n",
       "      <td>19407</td>\n",
       "      <td>so sick of all the pre-programmed #hillbots rh...</td>\n",
       "      <td>sick programmed #hillbots rhetoric enough alre...</td>\n",
       "      <td>sick programmed hillbots rhetoric enough alrea...</td>\n",
       "      <td>['sick', 'programmed', 'hillbots', 'rhetoric',...</td>\n",
       "      <td>['sick', 'program', 'hillbot', 'rhetor', 'enou...</td>\n",
       "      <td>['sick', 'programmed', 'hillbots', 'rhetoric',...</td>\n",
       "      <td>sick programmed hillbots rhetoric enough alrea...</td>\n",
       "      <td>sick programmed hillbots rhetoric enough alrea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19746</th>\n",
       "      <td>406</td>\n",
       "      <td>@user when you're blocked by a  troll because ...</td>\n",
       "      <td>when blocked troll because promise #blacklives...</td>\n",
       "      <td>when blocked troll because promise blacklivesm...</td>\n",
       "      <td>['when', 'blocked', 'troll', 'because', 'promi...</td>\n",
       "      <td>['when', 'block', 'troll', 'becaus', 'promis',...</td>\n",
       "      <td>['when', 'blocked', 'troll', 'because', 'promi...</td>\n",
       "      <td>when blocked troll because promise blacklivesm...</td>\n",
       "      <td>when blocked troll because promise blacklivesm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19581</th>\n",
       "      <td>11720</td>\n",
       "      <td>@user @user you forgot #democrate, because vet...</td>\n",
       "      <td>forgot #democrate because vetting people from ...</td>\n",
       "      <td>forgot democrate because vetting people from t...</td>\n",
       "      <td>['forgot', 'democrate', 'because', 'vetting', ...</td>\n",
       "      <td>['forgot', 'democr', 'becaus', 'vet', 'peopl',...</td>\n",
       "      <td>['forgot', 'democrate', 'because', 'vetting', ...</td>\n",
       "      <td>forgot democrate because vetting people from t...</td>\n",
       "      <td>forgot democrate because vetting people from t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31767</th>\n",
       "      <td>28476</td>\n",
       "      <td>sea shepherd suppoers are racist!   #antiracis...</td>\n",
       "      <td>shepherd suppoers racist #antiracism #seashepherd</td>\n",
       "      <td>shepherd suppoers racist antiracism seashepherd</td>\n",
       "      <td>['shepherd', 'suppoers', 'racist', 'antiracism...</td>\n",
       "      <td>['shepherd', 'suppoer', 'racist', 'antirac', '...</td>\n",
       "      <td>['shepherd', 'suppoers', 'racist', 'antiracism...</td>\n",
       "      <td>shepherd suppoers racist antiracism seashepherd</td>\n",
       "      <td>shepherd suppoers racist antiracism seashepherd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25210</th>\n",
       "      <td>24768</td>\n",
       "      <td>porn vids web free sex</td>\n",
       "      <td>porn vids free</td>\n",
       "      <td>porn vids free</td>\n",
       "      <td>['porn', 'vids', 'free']</td>\n",
       "      <td>['porn', 'vid', 'free']</td>\n",
       "      <td>['porn', 'vids', 'free']</td>\n",
       "      <td>porn vids free</td>\n",
       "      <td>porn vids fre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19800</th>\n",
       "      <td>19745</td>\n",
       "      <td>@user &amp;amp; the  #democraticpay keeps telling ...</td>\n",
       "      <td>#democraticpay keeps telling that #blm #chicag...</td>\n",
       "      <td>democraticpay keeps telling that blm chicago d...</td>\n",
       "      <td>['democraticpay', 'keeps', 'telling', 'that', ...</td>\n",
       "      <td>['democraticpay', 'keep', 'tell', 'that', 'blm...</td>\n",
       "      <td>['democraticpay', 'keep', 'telling', 'that', '...</td>\n",
       "      <td>democraticpay keeps telling that blm chicago d...</td>\n",
       "      <td>democraticpay keeps telling that blm chicago d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7812</th>\n",
       "      <td>1242</td>\n",
       "      <td>@user although, i am not a , a #bigot or a #mi...</td>\n",
       "      <td>although #bigot #misogynist hope that still qu...</td>\n",
       "      <td>although bigot misogynist hope that still qual...</td>\n",
       "      <td>['although', 'bigot', 'misogynist', 'hope', 't...</td>\n",
       "      <td>['although', 'bigot', 'misogynist', 'hope', 't...</td>\n",
       "      <td>['although', 'bigot', 'misogynist', 'hope', 't...</td>\n",
       "      <td>although bigot misogynist hope that still qual...</td>\n",
       "      <td>although bigot misogynist hope that still qual...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5074</th>\n",
       "      <td>23280</td>\n",
       "      <td>this is sooooo  or may be just funny</td>\n",
       "      <td>this sooooo just funny</td>\n",
       "      <td>this sooooo just funny</td>\n",
       "      <td>['this', 'sooooo', 'just', 'funny']</td>\n",
       "      <td>['this', 'sooooo', 'just', 'funni']</td>\n",
       "      <td>['this', 'sooooo', 'just', 'funny']</td>\n",
       "      <td>this sooooo just funny</td>\n",
       "      <td>this sooooo just funni</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6317</th>\n",
       "      <td>15050</td>\n",
       "      <td>@user âwe the peopleâ originally meant â...</td>\n",
       "      <td>people originally meant white landholding male...</td>\n",
       "      <td>people originally meant white landholding male...</td>\n",
       "      <td>['people', 'originally', 'meant', 'white', 'la...</td>\n",
       "      <td>['peopl', 'origin', 'meant', 'white', 'landhol...</td>\n",
       "      <td>['people', 'originally', 'meant', 'white', 'la...</td>\n",
       "      <td>people originally meant white landholding male...</td>\n",
       "      <td>people originally meant white landholding male...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5988</th>\n",
       "      <td>24766</td>\n",
       "      <td>i've no problem with #universities that teach ...</td>\n",
       "      <td>problem with #universities that teach against ...</td>\n",
       "      <td>problem with universities that teach against w...</td>\n",
       "      <td>['problem', 'with', 'universities', 'that', 't...</td>\n",
       "      <td>['problem', 'with', 'univers', 'that', 'teach'...</td>\n",
       "      <td>['problem', 'with', 'university', 'that', 'tea...</td>\n",
       "      <td>problem with universities that teach against w...</td>\n",
       "      <td>problem with universities that teach against w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3721</th>\n",
       "      <td>17405</td>\n",
       "      <td>check out this new trending #funny #gif !  , p...</td>\n",
       "      <td>check this trending #funny #gif pixel celebrat...</td>\n",
       "      <td>check this trending funny gif pixel celebrate ...</td>\n",
       "      <td>['check', 'this', 'trending', 'funny', 'gif', ...</td>\n",
       "      <td>['check', 'this', 'trend', 'funni', 'gif', 'pi...</td>\n",
       "      <td>['check', 'this', 'trending', 'funny', 'gif', ...</td>\n",
       "      <td>check this trending funny gif pixel celebrate ...</td>\n",
       "      <td>check this trending funny gif pixel celebrate ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28057</th>\n",
       "      <td>31774</td>\n",
       "      <td>this reminds me of this. i am   love these two...</td>\n",
       "      <td>this reminds this love these they golden #arro...</td>\n",
       "      <td>this reminds this love these they golden arrow...</td>\n",
       "      <td>['this', 'reminds', 'this', 'love', 'these', '...</td>\n",
       "      <td>['this', 'remind', 'this', 'love', 'these', 't...</td>\n",
       "      <td>['this', 'reminds', 'this', 'love', 'these', '...</td>\n",
       "      <td>this reminds this love these they golden arrow...</td>\n",
       "      <td>this reminds this love these they golden arrow ol</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15044</th>\n",
       "      <td>5789</td>\n",
       "      <td>livelypics: just when you think you know peop...</td>\n",
       "      <td>livelypics just when think know people they di...</td>\n",
       "      <td>livelypics just when think know people they di...</td>\n",
       "      <td>['livelypics', 'just', 'when', 'think', 'know'...</td>\n",
       "      <td>['livelyp', 'just', 'when', 'think', 'know', '...</td>\n",
       "      <td>['livelypics', 'just', 'when', 'think', 'know'...</td>\n",
       "      <td>livelypics just when think know people they di...</td>\n",
       "      <td>livelypics just when think know people they di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23222</th>\n",
       "      <td>29934</td>\n",
       "      <td>trying not to shut down but maybe #pokemon wil...</td>\n",
       "      <td>trying shut down maybe #pokemon will help skid</td>\n",
       "      <td>trying shut down maybe pokemon will help skid</td>\n",
       "      <td>['trying', 'shut', 'down', 'maybe', 'pokemon',...</td>\n",
       "      <td>['tri', 'shut', 'down', 'mayb', 'pokemon', 'wi...</td>\n",
       "      <td>['trying', 'shut', 'down', 'maybe', 'pokemon',...</td>\n",
       "      <td>trying shut down maybe pokemon will help skid</td>\n",
       "      <td>trying shut down maybe pokemon will help skid</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27273</th>\n",
       "      <td>395</td>\n",
       "      <td>#first #bihday to our #puppy #eloise #sweetbab...</td>\n",
       "      <td>#first #bihday #puppy #eloise #sweetbabins #do...</td>\n",
       "      <td>first bihday puppy eloise sweetbabins dog grow...</td>\n",
       "      <td>['first', 'bihday', 'puppy', 'eloise', 'sweetb...</td>\n",
       "      <td>['first', 'bihday', 'puppi', 'elois', 'sweetba...</td>\n",
       "      <td>['first', 'bihday', 'puppy', 'eloise', 'sweetb...</td>\n",
       "      <td>first bihday puppy eloise sweetbabins dog grow...</td>\n",
       "      <td>first bihday puppy eloise sweetbabins dog grow...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30304</th>\n",
       "      <td>10458</td>\n",
       "      <td>@user it was in ceain areas yeah, you not seen...</td>\n",
       "      <td>ceain areas yeah seen videos seeing little kid...</td>\n",
       "      <td>ceain areas yeah seen videos seeing little kid...</td>\n",
       "      <td>['ceain', 'areas', 'yeah', 'seen', 'videos', '...</td>\n",
       "      <td>['ceain', 'area', 'yeah', 'seen', 'video', 'se...</td>\n",
       "      <td>['ceain', 'area', 'yeah', 'seen', 'video', 'se...</td>\n",
       "      <td>ceain areas yeah seen videos seeing little kid...</td>\n",
       "      <td>ceain areas yeah seen videos seeing little kid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3928</th>\n",
       "      <td>19081</td>\n",
       "      <td>so many shit talkers/bullies in the youtube co...</td>\n",
       "      <td>many shit talkers bullies youtube comment sect...</td>\n",
       "      <td>many shit talkers bullies youtube comment sect...</td>\n",
       "      <td>['many', 'shit', 'talkers', 'bullies', 'youtub...</td>\n",
       "      <td>['mani', 'shit', 'talker', 'bulli', 'youtub', ...</td>\n",
       "      <td>['many', 'shit', 'talker', 'bully', 'youtube',...</td>\n",
       "      <td>many shit talkers bullies youtube comment sect...</td>\n",
       "      <td>many shit talkers bullies youtube comment sect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>13429</td>\n",
       "      <td>all ready to pay xx #saturday #daughter #love ...</td>\n",
       "      <td>ready #saturday #daughter #love #pay #igers #i...</td>\n",
       "      <td>ready saturday daughter love pay igers instago...</td>\n",
       "      <td>['ready', 'saturday', 'daughter', 'love', 'pay...</td>\n",
       "      <td>['readi', 'saturday', 'daughter', 'love', 'pay...</td>\n",
       "      <td>['ready', 'saturday', 'daughter', 'love', 'pay...</td>\n",
       "      <td>ready saturday daughter love pay igers instago...</td>\n",
       "      <td>ready saturday daughter love pay igers instago...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9826</th>\n",
       "      <td>6122</td>\n",
       "      <td>feeling a little #mole tonight! ó¾« #food #fo...</td>\n",
       "      <td>feeling little #mole tonight #food #foodblogge...</td>\n",
       "      <td>feeling little mole tonight food foodblogger m...</td>\n",
       "      <td>['feeling', 'little', 'mole', 'tonight', 'food...</td>\n",
       "      <td>['feel', 'littl', 'mole', 'tonight', 'food', '...</td>\n",
       "      <td>['feeling', 'little', 'mole', 'tonight', 'food...</td>\n",
       "      <td>feeling little mole tonight food foodblogger m...</td>\n",
       "      <td>feeling little mole tonight food foodblogger m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11548</th>\n",
       "      <td>14426</td>\n",
       "      <td>while we're still trying to get over the shock...</td>\n",
       "      <td>while still trying over shock keshi death shua...</td>\n",
       "      <td>while still trying over shock keshi death shua...</td>\n",
       "      <td>['while', 'still', 'trying', 'over', 'shock', ...</td>\n",
       "      <td>['while', 'still', 'tri', 'over', 'shock', 'ke...</td>\n",
       "      <td>['while', 'still', 'trying', 'over', 'shock', ...</td>\n",
       "      <td>while still trying over shock keshi death shua...</td>\n",
       "      <td>while still trying over shock keshi death shua...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19491</th>\n",
       "      <td>19469</td>\n",
       "      <td>fantasy aisle ð #i #fantasy #aisle #summer ...</td>\n",
       "      <td>fantasy aisle #fantasy #aisle #summer #grand #...</td>\n",
       "      <td>fantasy aisle fantasy aisle summer grand vacat...</td>\n",
       "      <td>['fantasy', 'aisle', 'fantasy', 'aisle', 'summ...</td>\n",
       "      <td>['fantasi', 'aisl', 'fantasi', 'aisl', 'summer...</td>\n",
       "      <td>['fantasy', 'aisle', 'fantasy', 'aisle', 'summ...</td>\n",
       "      <td>fantasy aisle fantasy aisle summer grand vacat...</td>\n",
       "      <td>fantasy aisle fantasy aisle summer grand vacat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31522</th>\n",
       "      <td>17794</td>\n",
       "      <td>have a wonderful f r i d a y ð¸  #love #emik...</td>\n",
       "      <td>have wonderful #love #emikagifts #jewelrydesig...</td>\n",
       "      <td>have wonderful love emikagifts jewelrydesign d...</td>\n",
       "      <td>['have', 'wonderful', 'love', 'emikagifts', 'j...</td>\n",
       "      <td>['have', 'wonder', 'love', 'emikagift', 'jewel...</td>\n",
       "      <td>['have', 'wonderful', 'love', 'emikagifts', 'j...</td>\n",
       "      <td>have wonderful love emikagifts jewelrydesign d...</td>\n",
       "      <td>have wonderful love emikagifts jewelrydesign d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>11550</td>\n",
       "      <td>who wants him to be the next commander in chie...</td>\n",
       "      <td>wants next commander chief #scary #disgusting ...</td>\n",
       "      <td>wants next commander chief scary disgusting tr...</td>\n",
       "      <td>['wants', 'next', 'commander', 'chief', 'scary...</td>\n",
       "      <td>['want', 'next', 'command', 'chief', 'scari', ...</td>\n",
       "      <td>['want', 'next', 'commander', 'chief', 'scary'...</td>\n",
       "      <td>wants next commander chief scary disgusting tr...</td>\n",
       "      <td>wants next commander chief scary disgusting tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21399</th>\n",
       "      <td>18137</td>\n",
       "      <td>we beat that cock.   #beatit #penis</td>\n",
       "      <td>beat that cock #beatit #penis</td>\n",
       "      <td>beat that cock beatit penis</td>\n",
       "      <td>['beat', 'that', 'cock', 'beatit', 'penis']</td>\n",
       "      <td>['beat', 'that', 'cock', 'beatit', 'peni']</td>\n",
       "      <td>['beat', 'that', 'cock', 'beatit', 'penis']</td>\n",
       "      <td>beat that cock beatit penis</td>\n",
       "      <td>beat that cock beatit peni</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16625</th>\n",
       "      <td>14918</td>\n",
       "      <td>lik if u cri everi tim</td>\n",
       "      <td>everi</td>\n",
       "      <td>everi</td>\n",
       "      <td>['everi']</td>\n",
       "      <td>['everi']</td>\n",
       "      <td>['everi']</td>\n",
       "      <td>everi</td>\n",
       "      <td>everi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7084</th>\n",
       "      <td>12925</td>\n",
       "      <td>speakers dinner on eve of #ricsrural conferenc...</td>\n",
       "      <td>speakers dinner #ricsrural conference #betterw...</td>\n",
       "      <td>speakers dinner ricsrural conference betterwri...</td>\n",
       "      <td>['speakers', 'dinner', 'ricsrural', 'conferenc...</td>\n",
       "      <td>['speaker', 'dinner', 'ricsrur', 'confer', 'be...</td>\n",
       "      <td>['speaker', 'dinner', 'ricsrural', 'conference...</td>\n",
       "      <td>speakers dinner ricsrural conference betterwri...</td>\n",
       "      <td>speakers dinner ricsrural conference betterwri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21332</th>\n",
       "      <td>1640</td>\n",
       "      <td>best facetime today; i can't wait to see my bo...</td>\n",
       "      <td>best facetime today wait wednesday when home #...</td>\n",
       "      <td>best facetime today wait wednesday when home w...</td>\n",
       "      <td>['best', 'facetime', 'today', 'wait', 'wednesd...</td>\n",
       "      <td>['best', 'facetim', 'today', 'wait', 'wednesda...</td>\n",
       "      <td>['best', 'facetime', 'today', 'wait', 'wednesd...</td>\n",
       "      <td>best facetime today wait wednesday when home w...</td>\n",
       "      <td>best facetime today wait wednesday when home w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28564</th>\n",
       "      <td>18855</td>\n",
       "      <td>archery for year 5 at 1.30pm!   #archery</td>\n",
       "      <td>archery year #archery</td>\n",
       "      <td>archery year archery</td>\n",
       "      <td>['archery', 'year', 'archery']</td>\n",
       "      <td>['archeri', 'year', 'archeri']</td>\n",
       "      <td>['archery', 'year', 'archery']</td>\n",
       "      <td>archery year archery</td>\n",
       "      <td>archery year archeri</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12504</th>\n",
       "      <td>23278</td>\n",
       "      <td>#whoolo in film you can have sad endings.  #an...</td>\n",
       "      <td>#whoolo film have endings #anna torv</td>\n",
       "      <td>whoolo film have endings anna torv</td>\n",
       "      <td>['whoolo', 'film', 'have', 'endings', 'anna', ...</td>\n",
       "      <td>['whoolo', 'film', 'have', 'end', 'anna', 'torv']</td>\n",
       "      <td>['whoolo', 'film', 'have', 'ending', 'anna', '...</td>\n",
       "      <td>whoolo film have endings anna torv</td>\n",
       "      <td>whoolo film have endings anna torv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22295</th>\n",
       "      <td>17561</td>\n",
       "      <td>@user @user at what time will the gates open?</td>\n",
       "      <td>what time will gates open</td>\n",
       "      <td>what time will gates open</td>\n",
       "      <td>['what', 'time', 'will', 'gates', 'open']</td>\n",
       "      <td>['what', 'time', 'will', 'gate', 'open']</td>\n",
       "      <td>['what', 'time', 'will', 'gate', 'open']</td>\n",
       "      <td>what time will gates open</td>\n",
       "      <td>what time will gates open</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13326</th>\n",
       "      <td>6135</td>\n",
       "      <td>thank you @user for a new #chargehr after mine...</td>\n",
       "      <td>thank #chargehr after mine broke #fitness #bec...</td>\n",
       "      <td>thank chargehr after mine broke fitness becaus...</td>\n",
       "      <td>['thank', 'chargehr', 'after', 'mine', 'broke'...</td>\n",
       "      <td>['thank', 'chargehr', 'after', 'mine', 'broke'...</td>\n",
       "      <td>['thank', 'chargehr', 'after', 'mine', 'broke'...</td>\n",
       "      <td>thank chargehr after mine broke fitness becaus...</td>\n",
       "      <td>thank chargehr after mine broke fitness becaus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>21952</td>\n",
       "      <td>our thoughts and prayers goes out to everyone ...</td>\n",
       "      <td>thoughts prayers goes everyone involved #orlan...</td>\n",
       "      <td>thoughts prayers goes everyone involved orland...</td>\n",
       "      <td>['thoughts', 'prayers', 'goes', 'everyone', 'i...</td>\n",
       "      <td>['thought', 'prayer', 'goe', 'everyon', 'invol...</td>\n",
       "      <td>['thought', 'prayer', 'go', 'everyone', 'invol...</td>\n",
       "      <td>thoughts prayers goes everyone involved orland...</td>\n",
       "      <td>thoughts prayers goes everyone involved orland...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19044</th>\n",
       "      <td>25937</td>\n",
       "      <td>#film   bull up: you will dominate your bull a...</td>\n",
       "      <td>#film bull will dominate your bull will direct...</td>\n",
       "      <td>film bull will dominate your bull will direct ...</td>\n",
       "      <td>['film', 'bull', 'will', 'dominate', 'your', '...</td>\n",
       "      <td>['film', 'bull', 'will', 'domin', 'your', 'bul...</td>\n",
       "      <td>['film', 'bull', 'will', 'dominate', 'your', '...</td>\n",
       "      <td>film bull will dominate your bull will direct ...</td>\n",
       "      <td>film bull will dominate your bull will direct ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31112</th>\n",
       "      <td>17229</td>\n",
       "      <td>this is horribly sad news. a fine actor with g...</td>\n",
       "      <td>this horribly news fine actor with great thing...</td>\n",
       "      <td>this horribly news fine actor with great thing...</td>\n",
       "      <td>['this', 'horribly', 'news', 'fine', 'actor', ...</td>\n",
       "      <td>['this', 'horribl', 'news', 'fine', 'actor', '...</td>\n",
       "      <td>['this', 'horribly', 'news', 'fine', 'actor', ...</td>\n",
       "      <td>this horribly news fine actor with great thing...</td>\n",
       "      <td>this horribly news fine actor with great thing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14188</th>\n",
       "      <td>7012</td>\n",
       "      <td>follow your #hea and be   â¤</td>\n",
       "      <td>follow your #hea</td>\n",
       "      <td>follow your hea</td>\n",
       "      <td>['follow', 'your', 'hea']</td>\n",
       "      <td>['follow', 'your', 'hea']</td>\n",
       "      <td>['follow', 'your', 'hea']</td>\n",
       "      <td>follow your hea</td>\n",
       "      <td>follow your hea</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>31177</td>\n",
       "      <td>cops giving tickets when they didn't even see ...</td>\n",
       "      <td>cops giving tickets when they didn even anythi...</td>\n",
       "      <td>cops giving tickets when they didn even anythi...</td>\n",
       "      <td>['cops', 'giving', 'tickets', 'when', 'they', ...</td>\n",
       "      <td>['cop', 'give', 'ticket', 'when', 'they', 'did...</td>\n",
       "      <td>['cop', 'giving', 'ticket', 'when', 'they', 'd...</td>\n",
       "      <td>cops giving tickets when they didn even anythi...</td>\n",
       "      <td>cops giving tickets when they didn even anythi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>23672</td>\n",
       "      <td>singapore city gallery   #riclswtravelbook #be...</td>\n",
       "      <td>singapore city gallery #riclswtravelbook #bear...</td>\n",
       "      <td>singapore city gallery riclswtravelbook bearlo...</td>\n",
       "      <td>['singapore', 'city', 'gallery', 'riclswtravel...</td>\n",
       "      <td>['singapor', 'citi', 'galleri', 'riclswtravelb...</td>\n",
       "      <td>['singapore', 'city', 'gallery', 'riclswtravel...</td>\n",
       "      <td>singapore city gallery riclswtravelbook bearlo...</td>\n",
       "      <td>singapore city gallery riclswtravelbook bearlo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8230</th>\n",
       "      <td>19584</td>\n",
       "      <td>oh man, been waiting for #wehappyfew for so lo...</td>\n",
       "      <td>been waiting #wehappyfew long #xboxe</td>\n",
       "      <td>been waiting wehappyfew long xboxe</td>\n",
       "      <td>['been', 'waiting', 'wehappyfew', 'long', 'xbo...</td>\n",
       "      <td>['been', 'wait', 'wehappyfew', 'long', 'xbox']</td>\n",
       "      <td>['been', 'waiting', 'wehappyfew', 'long', 'xbo...</td>\n",
       "      <td>been waiting wehappyfew long xboxe</td>\n",
       "      <td>been waiting wehappyfew long xbox</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15277</th>\n",
       "      <td>9967</td>\n",
       "      <td>schools almost over.</td>\n",
       "      <td>schools almost over</td>\n",
       "      <td>schools almost over</td>\n",
       "      <td>['schools', 'almost', 'over']</td>\n",
       "      <td>['school', 'almost', 'over']</td>\n",
       "      <td>['school', 'almost', 'over']</td>\n",
       "      <td>schools almost over</td>\n",
       "      <td>schools almost ov</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30396</th>\n",
       "      <td>2987</td>\n",
       "      <td>â #usd/cad bounces-off 0.2900, despite high...</td>\n",
       "      <td>#usd bounces despite higher #blog #silver #gol...</td>\n",
       "      <td>usd bounces despite higher blog silver gold forex</td>\n",
       "      <td>['usd', 'bounces', 'despite', 'higher', 'blog'...</td>\n",
       "      <td>['usd', 'bounc', 'despit', 'higher', 'blog', '...</td>\n",
       "      <td>['usd', 'bounce', 'despite', 'higher', 'blog',...</td>\n",
       "      <td>usd bounces despite higher blog silver gold forex</td>\n",
       "      <td>usd bounces despite higher blog silver gold forex</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37982 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  \\\n",
       "565    20676  @user f*** this  ð¦ðº government that deli...   \n",
       "21531  24025  despite a demoralizing 2016: may ur #newyear20...   \n",
       "13300  18145  @user #koreans &amp; joseon people in japan, w...   \n",
       "18925   8506  @user @user @user @user classic ! yet you jewi...   \n",
       "12619  15464  @user did someone say #antisemetic ? gee (((@u...   \n",
       "26964  28937  couldn't have said this any better nor truthfu...   \n",
       "17273  25291  @user racism stuffed into skinny jeans with a ...   \n",
       "1561   12717  the end of   #me #selfie # #love #messi #cr7 #...   \n",
       "17875  11612  trump ally wishes mad cow disease death for ob...   \n",
       "17184  20554  opinion:  is rife in the #lgbt community. #gay...   \n",
       "8573   25151  @user #allahsoil the cold war was fought over ...   \n",
       "26779  13585  omg, these trump suppoers are deplorable! #dum...   \n",
       "15332   2581  sea shepherd suppoers are racist!   #antiracis...   \n",
       "12290   6519  .@user @user while @user can use phrases like ...   \n",
       "24159   9563  @user #allahsoil enlightenment is wasted on th...   \n",
       "25895   8452  @user here comes a  #supermistict douchebag wh...   \n",
       "23934  17750  @user hidden  in #america is as rampant as bla...   \n",
       "29174  21389  will the alt-right promote a new kind of  gene...   \n",
       "10474  23431  #us why weneed #empathy #ageoftrump #grassroot...   \n",
       "30743  12301  black trump suppoer smacks down cnn repoer for...   \n",
       "6190   19407  so sick of all the pre-programmed #hillbots rh...   \n",
       "19746    406  @user when you're blocked by a  troll because ...   \n",
       "19581  11720  @user @user you forgot #democrate, because vet...   \n",
       "31767  28476  sea shepherd suppoers are racist!   #antiracis...   \n",
       "25210  24768                             porn vids web free sex   \n",
       "19800  19745  @user &amp; the  #democraticpay keeps telling ...   \n",
       "7812    1242  @user although, i am not a , a #bigot or a #mi...   \n",
       "5074   23280               this is sooooo  or may be just funny   \n",
       "6317   15050  @user âwe the peopleâ originally meant â...   \n",
       "5988   24766  i've no problem with #universities that teach ...   \n",
       "...      ...                                                ...   \n",
       "3721   17405  check out this new trending #funny #gif !  , p...   \n",
       "28057  31774  this reminds me of this. i am   love these two...   \n",
       "15044   5789   livelypics: just when you think you know peop...   \n",
       "23222  29934  trying not to shut down but maybe #pokemon wil...   \n",
       "27273    395  #first #bihday to our #puppy #eloise #sweetbab...   \n",
       "30304  10458  @user it was in ceain areas yeah, you not seen...   \n",
       "3928   19081  so many shit talkers/bullies in the youtube co...   \n",
       "269    13429  all ready to pay xx #saturday #daughter #love ...   \n",
       "9826    6122  feeling a little #mole tonight! ó¾« #food #fo...   \n",
       "11548  14426  while we're still trying to get over the shock...   \n",
       "19491  19469  fantasy aisle ð #i #fantasy #aisle #summer ...   \n",
       "31522  17794  have a wonderful f r i d a y ð¸  #love #emik...   \n",
       "750    11550  who wants him to be the next commander in chie...   \n",
       "21399  18137                we beat that cock.   #beatit #penis   \n",
       "16625  14918                             lik if u cri everi tim   \n",
       "7084   12925  speakers dinner on eve of #ricsrural conferenc...   \n",
       "21332   1640  best facetime today; i can't wait to see my bo...   \n",
       "28564  18855           archery for year 5 at 1.30pm!   #archery   \n",
       "12504  23278  #whoolo in film you can have sad endings.  #an...   \n",
       "22295  17561      @user @user at what time will the gates open?   \n",
       "13326   6135  thank you @user for a new #chargehr after mine...   \n",
       "1147   21952  our thoughts and prayers goes out to everyone ...   \n",
       "19044  25937  #film   bull up: you will dominate your bull a...   \n",
       "31112  17229  this is horribly sad news. a fine actor with g...   \n",
       "14188   7012                      follow your #hea and be   â¤   \n",
       "1034   31177  cops giving tickets when they didn't even see ...   \n",
       "2874   23672  singapore city gallery   #riclswtravelbook #be...   \n",
       "8230   19584  oh man, been waiting for #wehappyfew for so lo...   \n",
       "15277   9967                               schools almost over.   \n",
       "30396   2987   â #usd/cad bounces-off 0.2900, despite high...   \n",
       "\n",
       "                                              tidy_tweet  \\\n",
       "565    this government that deliberately toures #refu...   \n",
       "21531  despite demoralizing #newyear #classism free #...   \n",
       "13300  #koreans joseon people japan will abuse claims...   \n",
       "18925    classic jewish bastards wonder sooo hated world   \n",
       "12619                     someone #antisemetic triggered   \n",
       "26964  couldn have said this better truthfully donny ...   \n",
       "17273  racism stuffed into skinny jeans with hipster ...   \n",
       "1561   #selfie #love #messi #religion #christianity #...   \n",
       "17875  trump ally wishes disease death obama #unfitto...   \n",
       "17184  opinion rife #lgbt community #gay people deman...   \n",
       "8573   #allahsoil cold fought over #teambts #teamsupe...   \n",
       "26779  these trump suppoers deplorable #dumptrump #no...   \n",
       "15332  shepherd suppoers racist #antiracism #seashepherd   \n",
       "12290         while phrases like #sandniggers acceptable   \n",
       "24159  #allahsoil enlightenment wasted wilfully blind...   \n",
       "25895  here comes #supermistict douchebag only poke n...   \n",
       "23934             hidden #america rampant blatant racism   \n",
       "29174                   will right promote kind genetics   \n",
       "10474  weneed #empathy #ageoftrump #grassrootsaction ...   \n",
       "30743  black trump suppoer smacks down repoer race ba...   \n",
       "6190   sick programmed #hillbots rhetoric enough alre...   \n",
       "19746  when blocked troll because promise #blacklives...   \n",
       "19581  forgot #democrate because vetting people from ...   \n",
       "31767  shepherd suppoers racist #antiracism #seashepherd   \n",
       "25210                                     porn vids free   \n",
       "19800  #democraticpay keeps telling that #blm #chicag...   \n",
       "7812   although #bigot #misogynist hope that still qu...   \n",
       "5074                              this sooooo just funny   \n",
       "6317   people originally meant white landholding male...   \n",
       "5988   problem with #universities that teach against ...   \n",
       "...                                                  ...   \n",
       "3721   check this trending #funny #gif pixel celebrat...   \n",
       "28057  this reminds this love these they golden #arro...   \n",
       "15044  livelypics just when think know people they di...   \n",
       "23222     trying shut down maybe #pokemon will help skid   \n",
       "27273  #first #bihday #puppy #eloise #sweetbabins #do...   \n",
       "30304  ceain areas yeah seen videos seeing little kid...   \n",
       "3928   many shit talkers bullies youtube comment sect...   \n",
       "269    ready #saturday #daughter #love #pay #igers #i...   \n",
       "9826   feeling little #mole tonight #food #foodblogge...   \n",
       "11548  while still trying over shock keshi death shua...   \n",
       "19491  fantasy aisle #fantasy #aisle #summer #grand #...   \n",
       "31522  have wonderful #love #emikagifts #jewelrydesig...   \n",
       "750    wants next commander chief #scary #disgusting ...   \n",
       "21399                      beat that cock #beatit #penis   \n",
       "16625                                              everi   \n",
       "7084   speakers dinner #ricsrural conference #betterw...   \n",
       "21332  best facetime today wait wednesday when home #...   \n",
       "28564                              archery year #archery   \n",
       "12504               #whoolo film have endings #anna torv   \n",
       "22295                          what time will gates open   \n",
       "13326  thank #chargehr after mine broke #fitness #bec...   \n",
       "1147   thoughts prayers goes everyone involved #orlan...   \n",
       "19044  #film bull will dominate your bull will direct...   \n",
       "31112  this horribly news fine actor with great thing...   \n",
       "14188                                   follow your #hea   \n",
       "1034   cops giving tickets when they didn even anythi...   \n",
       "2874   singapore city gallery #riclswtravelbook #bear...   \n",
       "8230                been waiting #wehappyfew long #xboxe   \n",
       "15277                                schools almost over   \n",
       "30396  #usd bounces despite higher #blog #silver #gol...   \n",
       "\n",
       "                                           no_hash_tweet  \\\n",
       "565    this government that deliberately toures refug...   \n",
       "21531  despite demoralizing newyear classism free sag...   \n",
       "13300  koreans joseon people japan will abuse claims ...   \n",
       "18925    classic jewish bastards wonder sooo hated world   \n",
       "12619                      someone antisemetic triggered   \n",
       "26964  couldn have said this better truthfully donny ...   \n",
       "17273  racism stuffed into skinny jeans with hipster ...   \n",
       "1561   selfie love messi religion christianity mecca ...   \n",
       "17875  trump ally wishes disease death obama unfittob...   \n",
       "17184  opinion rife lgbt community gay people demand ...   \n",
       "8573   allahsoil cold fought over teambts teamsuperju...   \n",
       "26779  these trump suppoers deplorable dumptrump notm...   \n",
       "15332    shepherd suppoers racist antiracism seashepherd   \n",
       "12290          while phrases like sandniggers acceptable   \n",
       "24159  allahsoil enlightenment wasted wilfully blind ...   \n",
       "25895  here comes supermistict douchebag only poke no...   \n",
       "23934              hidden america rampant blatant racism   \n",
       "29174                   will right promote kind genetics   \n",
       "10474  weneed empathy ageoftrump grassrootsaction cit...   \n",
       "30743  black trump suppoer smacks down repoer race ba...   \n",
       "6190   sick programmed hillbots rhetoric enough alrea...   \n",
       "19746  when blocked troll because promise blacklivesm...   \n",
       "19581  forgot democrate because vetting people from t...   \n",
       "31767    shepherd suppoers racist antiracism seashepherd   \n",
       "25210                                     porn vids free   \n",
       "19800  democraticpay keeps telling that blm chicago d...   \n",
       "7812   although bigot misogynist hope that still qual...   \n",
       "5074                              this sooooo just funny   \n",
       "6317   people originally meant white landholding male...   \n",
       "5988   problem with universities that teach against w...   \n",
       "...                                                  ...   \n",
       "3721   check this trending funny gif pixel celebrate ...   \n",
       "28057  this reminds this love these they golden arrow...   \n",
       "15044  livelypics just when think know people they di...   \n",
       "23222      trying shut down maybe pokemon will help skid   \n",
       "27273  first bihday puppy eloise sweetbabins dog grow...   \n",
       "30304  ceain areas yeah seen videos seeing little kid...   \n",
       "3928   many shit talkers bullies youtube comment sect...   \n",
       "269    ready saturday daughter love pay igers instago...   \n",
       "9826   feeling little mole tonight food foodblogger m...   \n",
       "11548  while still trying over shock keshi death shua...   \n",
       "19491  fantasy aisle fantasy aisle summer grand vacat...   \n",
       "31522  have wonderful love emikagifts jewelrydesign d...   \n",
       "750    wants next commander chief scary disgusting tr...   \n",
       "21399                        beat that cock beatit penis   \n",
       "16625                                              everi   \n",
       "7084   speakers dinner ricsrural conference betterwri...   \n",
       "21332  best facetime today wait wednesday when home w...   \n",
       "28564                               archery year archery   \n",
       "12504                 whoolo film have endings anna torv   \n",
       "22295                          what time will gates open   \n",
       "13326  thank chargehr after mine broke fitness becaus...   \n",
       "1147   thoughts prayers goes everyone involved orland...   \n",
       "19044  film bull will dominate your bull will direct ...   \n",
       "31112  this horribly news fine actor with great thing...   \n",
       "14188                                    follow your hea   \n",
       "1034   cops giving tickets when they didn even anythi...   \n",
       "2874   singapore city gallery riclswtravelbook bearlo...   \n",
       "8230                  been waiting wehappyfew long xboxe   \n",
       "15277                                schools almost over   \n",
       "30396  usd bounces despite higher blog silver gold forex   \n",
       "\n",
       "                                         tokenized_tweet  \\\n",
       "565    ['this', 'government', 'that', 'deliberately',...   \n",
       "21531  ['despite', 'demoralizing', 'newyear', 'classi...   \n",
       "13300  ['koreans', 'joseon', 'people', 'japan', 'will...   \n",
       "18925  ['classic', 'jewish', 'bastards', 'wonder', 's...   \n",
       "12619            ['someone', 'antisemetic', 'triggered']   \n",
       "26964  ['couldn', 'have', 'said', 'this', 'better', '...   \n",
       "17273  ['racism', 'stuffed', 'into', 'skinny', 'jeans...   \n",
       "1561   ['selfie', 'love', 'messi', 'religion', 'chris...   \n",
       "17875  ['trump', 'ally', 'wishes', 'disease', 'death'...   \n",
       "17184  ['opinion', 'rife', 'lgbt', 'community', 'gay'...   \n",
       "8573   ['allahsoil', 'cold', 'fought', 'over', 'teamb...   \n",
       "26779  ['these', 'trump', 'suppoers', 'deplorable', '...   \n",
       "15332  ['shepherd', 'suppoers', 'racist', 'antiracism...   \n",
       "12290  ['while', 'phrases', 'like', 'sandniggers', 'a...   \n",
       "24159  ['allahsoil', 'enlightenment', 'wasted', 'wilf...   \n",
       "25895  ['here', 'comes', 'supermistict', 'douchebag',...   \n",
       "23934  ['hidden', 'america', 'rampant', 'blatant', 'r...   \n",
       "29174   ['will', 'right', 'promote', 'kind', 'genetics']   \n",
       "10474  ['weneed', 'empathy', 'ageoftrump', 'grassroot...   \n",
       "30743  ['black', 'trump', 'suppoer', 'smacks', 'down'...   \n",
       "6190   ['sick', 'programmed', 'hillbots', 'rhetoric',...   \n",
       "19746  ['when', 'blocked', 'troll', 'because', 'promi...   \n",
       "19581  ['forgot', 'democrate', 'because', 'vetting', ...   \n",
       "31767  ['shepherd', 'suppoers', 'racist', 'antiracism...   \n",
       "25210                           ['porn', 'vids', 'free']   \n",
       "19800  ['democraticpay', 'keeps', 'telling', 'that', ...   \n",
       "7812   ['although', 'bigot', 'misogynist', 'hope', 't...   \n",
       "5074                 ['this', 'sooooo', 'just', 'funny']   \n",
       "6317   ['people', 'originally', 'meant', 'white', 'la...   \n",
       "5988   ['problem', 'with', 'universities', 'that', 't...   \n",
       "...                                                  ...   \n",
       "3721   ['check', 'this', 'trending', 'funny', 'gif', ...   \n",
       "28057  ['this', 'reminds', 'this', 'love', 'these', '...   \n",
       "15044  ['livelypics', 'just', 'when', 'think', 'know'...   \n",
       "23222  ['trying', 'shut', 'down', 'maybe', 'pokemon',...   \n",
       "27273  ['first', 'bihday', 'puppy', 'eloise', 'sweetb...   \n",
       "30304  ['ceain', 'areas', 'yeah', 'seen', 'videos', '...   \n",
       "3928   ['many', 'shit', 'talkers', 'bullies', 'youtub...   \n",
       "269    ['ready', 'saturday', 'daughter', 'love', 'pay...   \n",
       "9826   ['feeling', 'little', 'mole', 'tonight', 'food...   \n",
       "11548  ['while', 'still', 'trying', 'over', 'shock', ...   \n",
       "19491  ['fantasy', 'aisle', 'fantasy', 'aisle', 'summ...   \n",
       "31522  ['have', 'wonderful', 'love', 'emikagifts', 'j...   \n",
       "750    ['wants', 'next', 'commander', 'chief', 'scary...   \n",
       "21399        ['beat', 'that', 'cock', 'beatit', 'penis']   \n",
       "16625                                          ['everi']   \n",
       "7084   ['speakers', 'dinner', 'ricsrural', 'conferenc...   \n",
       "21332  ['best', 'facetime', 'today', 'wait', 'wednesd...   \n",
       "28564                     ['archery', 'year', 'archery']   \n",
       "12504  ['whoolo', 'film', 'have', 'endings', 'anna', ...   \n",
       "22295          ['what', 'time', 'will', 'gates', 'open']   \n",
       "13326  ['thank', 'chargehr', 'after', 'mine', 'broke'...   \n",
       "1147   ['thoughts', 'prayers', 'goes', 'everyone', 'i...   \n",
       "19044  ['film', 'bull', 'will', 'dominate', 'your', '...   \n",
       "31112  ['this', 'horribly', 'news', 'fine', 'actor', ...   \n",
       "14188                          ['follow', 'your', 'hea']   \n",
       "1034   ['cops', 'giving', 'tickets', 'when', 'they', ...   \n",
       "2874   ['singapore', 'city', 'gallery', 'riclswtravel...   \n",
       "8230   ['been', 'waiting', 'wehappyfew', 'long', 'xbo...   \n",
       "15277                      ['schools', 'almost', 'over']   \n",
       "30396  ['usd', 'bounces', 'despite', 'higher', 'blog'...   \n",
       "\n",
       "                                          stemmed_tokens  \\\n",
       "565    ['this', 'govern', 'that', 'deliber', 'tour', ...   \n",
       "21531  ['despit', 'demor', 'newyear', 'classism', 'fr...   \n",
       "13300  ['korean', 'joseon', 'peopl', 'japan', 'will',...   \n",
       "18925  ['classic', 'jewish', 'bastard', 'wonder', 'so...   \n",
       "12619                 ['someon', 'antisemet', 'trigger']   \n",
       "26964  ['couldn', 'have', 'said', 'this', 'better', '...   \n",
       "17273  ['racism', 'stuf', 'into', 'skinni', 'jean', '...   \n",
       "1561   ['selfi', 'love', 'messi', 'religion', 'christ...   \n",
       "17875  ['trump', 'alli', 'wish', 'diseas', 'death', '...   \n",
       "17184  ['opinion', 'rife', 'lgbt', 'communiti', 'gay'...   \n",
       "8573   ['allahsoil', 'cold', 'fought', 'over', 'teamb...   \n",
       "26779  ['these', 'trump', 'suppoer', 'deplor', 'dumpt...   \n",
       "15332  ['shepherd', 'suppoer', 'racist', 'antirac', '...   \n",
       "12290  ['while', 'phrase', 'like', 'sandnigg', 'accept']   \n",
       "24159  ['allahsoil', 'enlighten', 'wast', 'wil', 'bli...   \n",
       "25895  ['here', 'come', 'supermistict', 'douchebag', ...   \n",
       "23934  ['hidden', 'america', 'rampant', 'blatant', 'r...   \n",
       "29174       ['will', 'right', 'promot', 'kind', 'genet']   \n",
       "10474  ['wene', 'empathi', 'ageoftrump', 'grassrootsa...   \n",
       "30743  ['black', 'trump', 'suppoer', 'smack', 'down',...   \n",
       "6190   ['sick', 'program', 'hillbot', 'rhetor', 'enou...   \n",
       "19746  ['when', 'block', 'troll', 'becaus', 'promis',...   \n",
       "19581  ['forgot', 'democr', 'becaus', 'vet', 'peopl',...   \n",
       "31767  ['shepherd', 'suppoer', 'racist', 'antirac', '...   \n",
       "25210                            ['porn', 'vid', 'free']   \n",
       "19800  ['democraticpay', 'keep', 'tell', 'that', 'blm...   \n",
       "7812   ['although', 'bigot', 'misogynist', 'hope', 't...   \n",
       "5074                 ['this', 'sooooo', 'just', 'funni']   \n",
       "6317   ['peopl', 'origin', 'meant', 'white', 'landhol...   \n",
       "5988   ['problem', 'with', 'univers', 'that', 'teach'...   \n",
       "...                                                  ...   \n",
       "3721   ['check', 'this', 'trend', 'funni', 'gif', 'pi...   \n",
       "28057  ['this', 'remind', 'this', 'love', 'these', 't...   \n",
       "15044  ['livelyp', 'just', 'when', 'think', 'know', '...   \n",
       "23222  ['tri', 'shut', 'down', 'mayb', 'pokemon', 'wi...   \n",
       "27273  ['first', 'bihday', 'puppi', 'elois', 'sweetba...   \n",
       "30304  ['ceain', 'area', 'yeah', 'seen', 'video', 'se...   \n",
       "3928   ['mani', 'shit', 'talker', 'bulli', 'youtub', ...   \n",
       "269    ['readi', 'saturday', 'daughter', 'love', 'pay...   \n",
       "9826   ['feel', 'littl', 'mole', 'tonight', 'food', '...   \n",
       "11548  ['while', 'still', 'tri', 'over', 'shock', 'ke...   \n",
       "19491  ['fantasi', 'aisl', 'fantasi', 'aisl', 'summer...   \n",
       "31522  ['have', 'wonder', 'love', 'emikagift', 'jewel...   \n",
       "750    ['want', 'next', 'command', 'chief', 'scari', ...   \n",
       "21399         ['beat', 'that', 'cock', 'beatit', 'peni']   \n",
       "16625                                          ['everi']   \n",
       "7084   ['speaker', 'dinner', 'ricsrur', 'confer', 'be...   \n",
       "21332  ['best', 'facetim', 'today', 'wait', 'wednesda...   \n",
       "28564                     ['archeri', 'year', 'archeri']   \n",
       "12504  ['whoolo', 'film', 'have', 'end', 'anna', 'torv']   \n",
       "22295           ['what', 'time', 'will', 'gate', 'open']   \n",
       "13326  ['thank', 'chargehr', 'after', 'mine', 'broke'...   \n",
       "1147   ['thought', 'prayer', 'goe', 'everyon', 'invol...   \n",
       "19044  ['film', 'bull', 'will', 'domin', 'your', 'bul...   \n",
       "31112  ['this', 'horribl', 'news', 'fine', 'actor', '...   \n",
       "14188                          ['follow', 'your', 'hea']   \n",
       "1034   ['cop', 'give', 'ticket', 'when', 'they', 'did...   \n",
       "2874   ['singapor', 'citi', 'galleri', 'riclswtravelb...   \n",
       "8230      ['been', 'wait', 'wehappyfew', 'long', 'xbox']   \n",
       "15277                       ['school', 'almost', 'over']   \n",
       "30396  ['usd', 'bounc', 'despit', 'higher', 'blog', '...   \n",
       "\n",
       "                                       lemmatized_tokens  \\\n",
       "565    ['this', 'government', 'that', 'deliberately',...   \n",
       "21531  ['despite', 'demoralizing', 'newyear', 'classi...   \n",
       "13300  ['korean', 'joseon', 'people', 'japan', 'will'...   \n",
       "18925  ['classic', 'jewish', 'bastard', 'wonder', 'so...   \n",
       "12619            ['someone', 'antisemetic', 'triggered']   \n",
       "26964  ['couldn', 'have', 'said', 'this', 'better', '...   \n",
       "17273  ['racism', 'stuffed', 'into', 'skinny', 'jean'...   \n",
       "1561   ['selfie', 'love', 'messi', 'religion', 'chris...   \n",
       "17875  ['trump', 'ally', 'wish', 'disease', 'death', ...   \n",
       "17184  ['opinion', 'rife', 'lgbt', 'community', 'gay'...   \n",
       "8573   ['allahsoil', 'cold', 'fought', 'over', 'teamb...   \n",
       "26779  ['these', 'trump', 'suppoers', 'deplorable', '...   \n",
       "15332  ['shepherd', 'suppoers', 'racist', 'antiracism...   \n",
       "12290  ['while', 'phrase', 'like', 'sandniggers', 'ac...   \n",
       "24159  ['allahsoil', 'enlightenment', 'wasted', 'wilf...   \n",
       "25895  ['here', 'come', 'supermistict', 'douchebag', ...   \n",
       "23934  ['hidden', 'america', 'rampant', 'blatant', 'r...   \n",
       "29174   ['will', 'right', 'promote', 'kind', 'genetics']   \n",
       "10474  ['weneed', 'empathy', 'ageoftrump', 'grassroot...   \n",
       "30743  ['black', 'trump', 'suppoer', 'smack', 'down',...   \n",
       "6190   ['sick', 'programmed', 'hillbots', 'rhetoric',...   \n",
       "19746  ['when', 'blocked', 'troll', 'because', 'promi...   \n",
       "19581  ['forgot', 'democrate', 'because', 'vetting', ...   \n",
       "31767  ['shepherd', 'suppoers', 'racist', 'antiracism...   \n",
       "25210                           ['porn', 'vids', 'free']   \n",
       "19800  ['democraticpay', 'keep', 'telling', 'that', '...   \n",
       "7812   ['although', 'bigot', 'misogynist', 'hope', 't...   \n",
       "5074                 ['this', 'sooooo', 'just', 'funny']   \n",
       "6317   ['people', 'originally', 'meant', 'white', 'la...   \n",
       "5988   ['problem', 'with', 'university', 'that', 'tea...   \n",
       "...                                                  ...   \n",
       "3721   ['check', 'this', 'trending', 'funny', 'gif', ...   \n",
       "28057  ['this', 'reminds', 'this', 'love', 'these', '...   \n",
       "15044  ['livelypics', 'just', 'when', 'think', 'know'...   \n",
       "23222  ['trying', 'shut', 'down', 'maybe', 'pokemon',...   \n",
       "27273  ['first', 'bihday', 'puppy', 'eloise', 'sweetb...   \n",
       "30304  ['ceain', 'area', 'yeah', 'seen', 'video', 'se...   \n",
       "3928   ['many', 'shit', 'talker', 'bully', 'youtube',...   \n",
       "269    ['ready', 'saturday', 'daughter', 'love', 'pay...   \n",
       "9826   ['feeling', 'little', 'mole', 'tonight', 'food...   \n",
       "11548  ['while', 'still', 'trying', 'over', 'shock', ...   \n",
       "19491  ['fantasy', 'aisle', 'fantasy', 'aisle', 'summ...   \n",
       "31522  ['have', 'wonderful', 'love', 'emikagifts', 'j...   \n",
       "750    ['want', 'next', 'commander', 'chief', 'scary'...   \n",
       "21399        ['beat', 'that', 'cock', 'beatit', 'penis']   \n",
       "16625                                          ['everi']   \n",
       "7084   ['speaker', 'dinner', 'ricsrural', 'conference...   \n",
       "21332  ['best', 'facetime', 'today', 'wait', 'wednesd...   \n",
       "28564                     ['archery', 'year', 'archery']   \n",
       "12504  ['whoolo', 'film', 'have', 'ending', 'anna', '...   \n",
       "22295           ['what', 'time', 'will', 'gate', 'open']   \n",
       "13326  ['thank', 'chargehr', 'after', 'mine', 'broke'...   \n",
       "1147   ['thought', 'prayer', 'go', 'everyone', 'invol...   \n",
       "19044  ['film', 'bull', 'will', 'dominate', 'your', '...   \n",
       "31112  ['this', 'horribly', 'news', 'fine', 'actor', ...   \n",
       "14188                          ['follow', 'your', 'hea']   \n",
       "1034   ['cop', 'giving', 'ticket', 'when', 'they', 'd...   \n",
       "2874   ['singapore', 'city', 'gallery', 'riclswtravel...   \n",
       "8230   ['been', 'waiting', 'wehappyfew', 'long', 'xbo...   \n",
       "15277                       ['school', 'almost', 'over']   \n",
       "30396  ['usd', 'bounce', 'despite', 'higher', 'blog',...   \n",
       "\n",
       "                                               lem_tweet  \\\n",
       "565    this government that deliberately toures refug...   \n",
       "21531  despite demoralizing newyear classism free sag...   \n",
       "13300  koreans joseon people japan will abuse claims ...   \n",
       "18925    classic jewish bastards wonder sooo hated world   \n",
       "12619                      someone antisemetic triggered   \n",
       "26964  couldn have said this better truthfully donny ...   \n",
       "17273  racism stuffed into skinny jeans with hipster ...   \n",
       "1561   selfie love messi religion christianity mecca ...   \n",
       "17875  trump ally wishes disease death obama unfittob...   \n",
       "17184  opinion rife lgbt community gay people demand ...   \n",
       "8573   allahsoil cold fought over teambts teamsuperju...   \n",
       "26779  these trump suppoers deplorable dumptrump notm...   \n",
       "15332    shepherd suppoers racist antiracism seashepherd   \n",
       "12290          while phrases like sandniggers acceptable   \n",
       "24159  allahsoil enlightenment wasted wilfully blind ...   \n",
       "25895  here comes supermistict douchebag only poke no...   \n",
       "23934              hidden america rampant blatant racism   \n",
       "29174                   will right promote kind genetics   \n",
       "10474  weneed empathy ageoftrump grassrootsaction cit...   \n",
       "30743  black trump suppoer smacks down repoer race ba...   \n",
       "6190   sick programmed hillbots rhetoric enough alrea...   \n",
       "19746  when blocked troll because promise blacklivesm...   \n",
       "19581  forgot democrate because vetting people from t...   \n",
       "31767    shepherd suppoers racist antiracism seashepherd   \n",
       "25210                                     porn vids free   \n",
       "19800  democraticpay keeps telling that blm chicago d...   \n",
       "7812   although bigot misogynist hope that still qual...   \n",
       "5074                              this sooooo just funny   \n",
       "6317   people originally meant white landholding male...   \n",
       "5988   problem with universities that teach against w...   \n",
       "...                                                  ...   \n",
       "3721   check this trending funny gif pixel celebrate ...   \n",
       "28057  this reminds this love these they golden arrow...   \n",
       "15044  livelypics just when think know people they di...   \n",
       "23222      trying shut down maybe pokemon will help skid   \n",
       "27273  first bihday puppy eloise sweetbabins dog grow...   \n",
       "30304  ceain areas yeah seen videos seeing little kid...   \n",
       "3928   many shit talkers bullies youtube comment sect...   \n",
       "269    ready saturday daughter love pay igers instago...   \n",
       "9826   feeling little mole tonight food foodblogger m...   \n",
       "11548  while still trying over shock keshi death shua...   \n",
       "19491  fantasy aisle fantasy aisle summer grand vacat...   \n",
       "31522  have wonderful love emikagifts jewelrydesign d...   \n",
       "750    wants next commander chief scary disgusting tr...   \n",
       "21399                        beat that cock beatit penis   \n",
       "16625                                              everi   \n",
       "7084   speakers dinner ricsrural conference betterwri...   \n",
       "21332  best facetime today wait wednesday when home w...   \n",
       "28564                               archery year archery   \n",
       "12504                 whoolo film have endings anna torv   \n",
       "22295                          what time will gates open   \n",
       "13326  thank chargehr after mine broke fitness becaus...   \n",
       "1147   thoughts prayers goes everyone involved orland...   \n",
       "19044  film bull will dominate your bull will direct ...   \n",
       "31112  this horribly news fine actor with great thing...   \n",
       "14188                                    follow your hea   \n",
       "1034   cops giving tickets when they didn even anythi...   \n",
       "2874   singapore city gallery riclswtravelbook bearlo...   \n",
       "8230                  been waiting wehappyfew long xboxe   \n",
       "15277                                schools almost over   \n",
       "30396  usd bounces despite higher blog silver gold forex   \n",
       "\n",
       "                                              stem_tweet  label  \n",
       "565    this government that deliberately toures refug...      1  \n",
       "21531  despite demoralizing newyear classism free sag...      1  \n",
       "13300  koreans joseon people japan will abuse claims ...      1  \n",
       "18925    classic jewish bastards wonder sooo hated world      1  \n",
       "12619                          someone antisemetic trigg      1  \n",
       "26964  couldn have said this better truthfully donny ...      1  \n",
       "17273  racism stuffed into skinny jeans with hipster ...      1  \n",
       "1561     selfie love messi religion christianity mecca l      1  \n",
       "17875  trump ally wishes disease death obama unfittob...      1  \n",
       "17184  opinion rife lgbt community gay people demand ...      1  \n",
       "8573   allahsoil cold fought over teambts teamsuperju...      1  \n",
       "26779  these trump suppoers deplorable dumptrump notm...      1  \n",
       "15332    shepherd suppoers racist antiracism seashepherd      1  \n",
       "12290              while phrases like sandniggers accept      1  \n",
       "24159  allahsoil enlightenment wasted wilfully blind ...      1  \n",
       "25895  here comes supermistict douchebag only poke no...      1  \n",
       "23934                 hidden america rampant blatant rac      1  \n",
       "29174                      will right promote kind genet      1  \n",
       "10474  weneed empathy ageoftrump grassrootsaction cit...      1  \n",
       "30743  black trump suppoer smacks down repoer race ba...      1  \n",
       "6190   sick programmed hillbots rhetoric enough alrea...      1  \n",
       "19746  when blocked troll because promise blacklivesm...      1  \n",
       "19581  forgot democrate because vetting people from t...      1  \n",
       "31767    shepherd suppoers racist antiracism seashepherd      1  \n",
       "25210                                      porn vids fre      1  \n",
       "19800  democraticpay keeps telling that blm chicago d...      1  \n",
       "7812   although bigot misogynist hope that still qual...      1  \n",
       "5074                              this sooooo just funni      1  \n",
       "6317   people originally meant white landholding male...      1  \n",
       "5988   problem with universities that teach against w...      1  \n",
       "...                                                  ...    ...  \n",
       "3721   check this trending funny gif pixel celebrate ...      0  \n",
       "28057  this reminds this love these they golden arrow ol      0  \n",
       "15044  livelypics just when think know people they di...      0  \n",
       "23222      trying shut down maybe pokemon will help skid      0  \n",
       "27273  first bihday puppy eloise sweetbabins dog grow...      0  \n",
       "30304  ceain areas yeah seen videos seeing little kid...      0  \n",
       "3928      many shit talkers bullies youtube comment sect      0  \n",
       "269    ready saturday daughter love pay igers instago...      0  \n",
       "9826   feeling little mole tonight food foodblogger m...      0  \n",
       "11548  while still trying over shock keshi death shua...      0  \n",
       "19491  fantasy aisle fantasy aisle summer grand vacat...      0  \n",
       "31522  have wonderful love emikagifts jewelrydesign d...      0  \n",
       "750    wants next commander chief scary disgusting tr...      0  \n",
       "21399                         beat that cock beatit peni      0  \n",
       "16625                                              everi      0  \n",
       "7084   speakers dinner ricsrural conference betterwri...      0  \n",
       "21332  best facetime today wait wednesday when home w...      0  \n",
       "28564                               archery year archeri      0  \n",
       "12504                 whoolo film have endings anna torv      0  \n",
       "22295                          what time will gates open      0  \n",
       "13326  thank chargehr after mine broke fitness becaus...      0  \n",
       "1147   thoughts prayers goes everyone involved orland...      0  \n",
       "19044  film bull will dominate your bull will direct ...      0  \n",
       "31112  this horribly news fine actor with great thing...      0  \n",
       "14188                                    follow your hea      0  \n",
       "1034   cops giving tickets when they didn even anythi...      0  \n",
       "2874   singapore city gallery riclswtravelbook bearlo...      0  \n",
       "8230                   been waiting wehappyfew long xbox      0  \n",
       "15277                                  schools almost ov      0  \n",
       "30396  usd bounces despite higher blog silver gold forex      0  \n",
       "\n",
       "[37982 rows x 10 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsample_training_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_upsampled = upsample_training_data(X_train, y_train)\n",
    "\n",
    "X_train_up = train_upsampled.drop(['label'], axis = 1)\n",
    "y_train_up = pd.DataFrame(train_upsampled.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    18991\n",
       "0    18991\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_upsampled['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_downsampled = downsample_training_data(X_train, y_train)\n",
    "\n",
    "X_train_down = train_downsampled.drop(['label'], axis = 1)\n",
    "y_train_down = pd.DataFrame(train_downsampled.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1464\n",
       "0    1464\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_downsampled['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Vectorization and Method Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=.001)\n",
    "tfidf_ngram = TfidfVectorizer(ngram_range=(1,2), min_df=.001)\n",
    "tfidf_ngram2 = TfidfVectorizer(ngram_range=(2,3),min_df=.001)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "rfc = RandomForestClassifier(random_state=10)\n",
    "nb = GaussianNB()\n",
    "svc = SVC(random_state=10)\n",
    "\n",
    "vectorization_list = [('COUNT_VECTORIZER', count_vect),\n",
    "                      ('TFIDF_VECTORIZER', tfidf_vectorizer),\n",
    "                      ('TFIDF_NGRAM_1_2', tfidf_ngram),\n",
    "                      ('TFIDF_NGRAM_2_3', tfidf_ngram2)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of the COUNT_VECTORIZER is:\n",
      "Train Accuracy: 0.93\n",
      "Train Precision: 0.51\n",
      "Train Recall: 1.0\n",
      "Train F1: 0.68\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.85\n",
      "Validation Precision: 0.24\n",
      "Validation Recall: 0.63\n",
      "The performance of the COUNT_VECTORIZER is:\n",
      "Train Accuracy: 0.93\n",
      "Train Precision: 0.51\n",
      "Train Recall: 1.0\n",
      "Train F1: 0.68\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.85\n",
      "Validation Precision: 0.24\n",
      "Validation Recall: 0.63\n",
      "Validation F1: 0.34\n",
      "\n",
      "\n",
      "Validation F1: 0.34\n",
      "\n",
      "\n",
      "The performance of the TFIDF_VECTORIZER is:\n",
      "Train Accuracy: 0.6\n",
      "Train Precision: 0.15\n",
      "Train Recall: 1.0\n",
      "Train F1: 0.26\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.58\n",
      "Validation Precision: 0.12\n",
      "Validation Recall: 0.88\n",
      "Validation F1: 0.21\n",
      "\n",
      "\n",
      "The performance of the TFIDF_VECTORIZER is:\n",
      "Train Accuracy: 0.6\n",
      "Train Precision: 0.15\n",
      "Train Recall: 1.0\n",
      "Train F1: 0.26\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.58\n",
      "Validation Precision: 0.12\n",
      "Validation Recall: 0.88\n",
      "Validation F1: 0.21\n",
      "\n",
      "\n",
      "The performance of the TFIDF_NGRAM_1_2 is:\n",
      "Train Accuracy: 0.62\n",
      "Train Precision: 0.16\n",
      "Train Recall: 1.0\n",
      "Train F1: 0.28\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.6\n",
      "Validation Precision: 0.13\n",
      "Validation Recall: 0.89\n",
      "Validation F1: 0.22\n",
      "\n",
      "\n",
      "The performance of the TFIDF_NGRAM_1_2 is:\n",
      "Train Accuracy: 0.62\n",
      "Train Precision: 0.16\n",
      "Train Recall: 1.0\n",
      "Train F1: 0.28\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.6\n",
      "Validation Precision: 0.13\n",
      "Validation Recall: 0.89\n",
      "Validation F1: 0.22\n",
      "\n",
      "\n",
      "The performance of the TFIDF_NGRAM_2_3 is:\n",
      "Train Accuracy: 0.22\n",
      "Train Precision: 0.08\n",
      "Train Recall: 1.0\n",
      "Train F1: 0.16\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.2\n",
      "Validation Precision: 0.07\n",
      "Validation Recall: 1.0\n",
      "Validation F1: 0.14\n",
      "\n",
      "\n",
      "The performance of the TFIDF_NGRAM_2_3 is:\n",
      "Train Accuracy: 0.22\n",
      "Train Precision: 0.08\n",
      "Train Recall: 1.0\n",
      "Train F1: 0.16\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.2\n",
      "Validation Precision: 0.07\n",
      "Validation Recall: 1.0\n",
      "Validation F1: 0.14\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'COUNT_VECTORIZER': {'Train Accuracy': 0.9324370569542899,\n",
       "  'Train Precision': 0.5144061841180604,\n",
       "  'Train Recall': 1.0,\n",
       "  'Train F1': 0.6793503480278421,\n",
       "  'Validation Accuracy': 0.8461087211576066,\n",
       "  'Validation Precision': 0.23765786452353616,\n",
       "  'Validation Recall': 0.6272727272727273,\n",
       "  'Validation F1': 0.3447127393838468},\n",
       " 'TFIDF_VECTORIZER': {'Train Accuracy': 0.6011733072598386,\n",
       "  'Train Precision': 0.15215131989191436,\n",
       "  'Train Recall': 1.0,\n",
       "  'Train F1': 0.26411690420349987,\n",
       "  'Validation Accuracy': 0.5803676183026985,\n",
       "  'Validation Precision': 0.12166666666666667,\n",
       "  'Validation Recall': 0.8848484848484849,\n",
       "  'Validation F1': 0.21391941391941394},\n",
       " 'TFIDF_NGRAM_1_2': {'Train Accuracy': 0.6244927890491322,\n",
       "  'Train Precision': 0.1600874794969929,\n",
       "  'Train Recall': 1.0,\n",
       "  'Train F1': 0.2759920821943633,\n",
       "  'Validation Accuracy': 0.6018771998435667,\n",
       "  'Validation Precision': 0.12783595113438045,\n",
       "  'Validation Recall': 0.8878787878787879,\n",
       "  'Validation F1': 0.22349351639969486},\n",
       " 'TFIDF_NGRAM_2_3': {'Train Accuracy': 0.22468834025910536,\n",
       "  'Train Precision': 0.08451192056803095,\n",
       "  'Train Recall': 1.0,\n",
       "  'Train F1': 0.15585245116303828,\n",
       "  'Validation Accuracy': 0.20219006648416113,\n",
       "  'Validation Precision': 0.07463702359346643,\n",
       "  'Validation Recall': 0.996969696969697,\n",
       "  'Validation F1': 0.13887716336006756}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'COUNT_VECTORIZER': {'Train Accuracy': 0.9324370569542899,\n",
       "  'Train Precision': 0.5144061841180604,\n",
       "  'Train Recall': 1.0,\n",
       "  'Train F1': 0.6793503480278421,\n",
       "  'Validation Accuracy': 0.8461087211576066,\n",
       "  'Validation Precision': 0.23765786452353616,\n",
       "  'Validation Recall': 0.6272727272727273,\n",
       "  'Validation F1': 0.3447127393838468},\n",
       " 'TFIDF_VECTORIZER': {'Train Accuracy': 0.6011733072598386,\n",
       "  'Train Precision': 0.15215131989191436,\n",
       "  'Train Recall': 1.0,\n",
       "  'Train F1': 0.26411690420349987,\n",
       "  'Validation Accuracy': 0.5803676183026985,\n",
       "  'Validation Precision': 0.12166666666666667,\n",
       "  'Validation Recall': 0.8848484848484849,\n",
       "  'Validation F1': 0.21391941391941394},\n",
       " 'TFIDF_NGRAM_1_2': {'Train Accuracy': 0.6244927890491322,\n",
       "  'Train Precision': 0.1600874794969929,\n",
       "  'Train Recall': 1.0,\n",
       "  'Train F1': 0.2759920821943633,\n",
       "  'Validation Accuracy': 0.6018771998435667,\n",
       "  'Validation Precision': 0.12783595113438045,\n",
       "  'Validation Recall': 0.8878787878787879,\n",
       "  'Validation F1': 0.22349351639969486},\n",
       " 'TFIDF_NGRAM_2_3': {'Train Accuracy': 0.22468834025910536,\n",
       "  'Train Precision': 0.08451192056803095,\n",
       "  'Train Recall': 1.0,\n",
       "  'Train F1': 0.15585245116303828,\n",
       "  'Validation Accuracy': 0.20219006648416113,\n",
       "  'Validation Precision': 0.07463702359346643,\n",
       "  'Validation Recall': 0.996969696969697,\n",
       "  'Validation F1': 0.13887716336006756}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_compare_vectorization_model(X_train.lem_tweet, y_train, \n",
    "                                   X_val.lem_tweet, y_val, GaussianNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.88\n",
      "Train Precision: 0.35\n",
      "Train Recall: 0.86\n",
      "Train F1: 0.5\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.86\n",
      "Validation Precision: 0.27\n",
      "Validation Recall: 0.72\n",
      "Validation F1: 0.4\n",
      "Train Accuracy: 0.88\n",
      "Train Precision: 0.35\n",
      "Train Recall: 0.86\n",
      "Train F1: 0.5\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.86\n",
      "Validation Precision: 0.27\n",
      "Validation Recall: 0.72\n",
      "Validation F1: 0.4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4148</td>\n",
       "      <td>636</td>\n",
       "      <td>4784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>239</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4239</td>\n",
       "      <td>875</td>\n",
       "      <td>5114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0    1   All\n",
       "Actual                    \n",
       "0          4148  636  4784\n",
       "1            91  239   330\n",
       "All        4239  875  5114"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4148</td>\n",
       "      <td>636</td>\n",
       "      <td>4784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>239</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4239</td>\n",
       "      <td>875</td>\n",
       "      <td>5114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0    1   All\n",
       "Actual                    \n",
       "0          4148  636  4784\n",
       "1            91  239   330\n",
       "All        4239  875  5114"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMOTE_vector_model(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, tfidf_vectorizer, logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression: compare vectorizers with class weight balances + lemmatizing\n",
    "LR_cw_lemm = compare_vectorization_model(X_train.lem_tweet, \n",
    "                            y_train, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', solver = 'lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT_VECTORIZER</th>\n",
       "      <th>TFIDF_VECTORIZER</th>\n",
       "      <th>TFIDF_NGRAM_1_2</th>\n",
       "      <th>TFIDF_NGRAM_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation F1</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Precision</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Recall</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      COUNT_VECTORIZER  TFIDF_VECTORIZER  TFIDF_NGRAM_1_2  \\\n",
       "Train Accuracy                    0.99              0.88             0.88   \n",
       "Train F1                          0.92              0.51             0.51   \n",
       "Train Precision                   0.85              0.36             0.36   \n",
       "Train Recall                      0.99              0.88             0.88   \n",
       "Validation Accuracy               0.95              0.86             0.86   \n",
       "Validation F1                     0.65              0.41             0.41   \n",
       "Validation Precision              0.61              0.28             0.28   \n",
       "Validation Recall                 0.69              0.72             0.72   \n",
       "\n",
       "                      TFIDF_NGRAM_2_3  \n",
       "Train Accuracy                   0.24  \n",
       "Train F1                         0.16  \n",
       "Train Precision                  0.09  \n",
       "Train Recall                     0.99  \n",
       "Validation Accuracy              0.22  \n",
       "Validation F1                    0.14  \n",
       "Validation Precision             0.08  \n",
       "Validation Recall                0.98  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(LR_cw_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression: compare vectorizers with SMOTE + lemmatizing\n",
    "LR_smote_lemm = SMOTE_compare_vectorization_model(X_train.lem_tweet, y_train, X_val.lem_tweet, \n",
    "                                    y_val, LogisticRegression(class_weight='balanced', solver= 'lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COUNT_VECTORIZER': {'Train Accuracy': 0.99,\n",
       "  'Train Precision': 0.85,\n",
       "  'Train Recall': 0.99,\n",
       "  'Train F1': 0.92,\n",
       "  'Validation Accuracy': 0.95,\n",
       "  'Validation Precision': 0.61,\n",
       "  'Validation Recall': 0.69,\n",
       "  'Validation F1': 0.65},\n",
       " 'TFIDF_VECTORIZER': {'Train Accuracy': 0.88,\n",
       "  'Train Precision': 0.36,\n",
       "  'Train Recall': 0.88,\n",
       "  'Train F1': 0.51,\n",
       "  'Validation Accuracy': 0.86,\n",
       "  'Validation Precision': 0.28,\n",
       "  'Validation Recall': 0.72,\n",
       "  'Validation F1': 0.41},\n",
       " 'TFIDF_NGRAM_1_2': {'Train Accuracy': 0.88,\n",
       "  'Train Precision': 0.36,\n",
       "  'Train Recall': 0.88,\n",
       "  'Train F1': 0.51,\n",
       "  'Validation Accuracy': 0.86,\n",
       "  'Validation Precision': 0.28,\n",
       "  'Validation Recall': 0.72,\n",
       "  'Validation F1': 0.41},\n",
       " 'TFIDF_NGRAM_2_3': {'Train Accuracy': 0.24,\n",
       "  'Train Precision': 0.09,\n",
       "  'Train Recall': 0.99,\n",
       "  'Train F1': 0.16,\n",
       "  'Validation Accuracy': 0.22,\n",
       "  'Validation Precision': 0.08,\n",
       "  'Validation Recall': 0.98,\n",
       "  'Validation F1': 0.14}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'COUNT_VECTORIZER': {'Train Accuracy': 0.99,\n",
       "  'Train Precision': 0.85,\n",
       "  'Train Recall': 0.99,\n",
       "  'Train F1': 0.92,\n",
       "  'Validation Accuracy': 0.95,\n",
       "  'Validation Precision': 0.61,\n",
       "  'Validation Recall': 0.69,\n",
       "  'Validation F1': 0.65},\n",
       " 'TFIDF_VECTORIZER': {'Train Accuracy': 0.88,\n",
       "  'Train Precision': 0.36,\n",
       "  'Train Recall': 0.88,\n",
       "  'Train F1': 0.51,\n",
       "  'Validation Accuracy': 0.86,\n",
       "  'Validation Precision': 0.28,\n",
       "  'Validation Recall': 0.72,\n",
       "  'Validation F1': 0.41},\n",
       " 'TFIDF_NGRAM_1_2': {'Train Accuracy': 0.88,\n",
       "  'Train Precision': 0.36,\n",
       "  'Train Recall': 0.88,\n",
       "  'Train F1': 0.51,\n",
       "  'Validation Accuracy': 0.86,\n",
       "  'Validation Precision': 0.28,\n",
       "  'Validation Recall': 0.72,\n",
       "  'Validation F1': 0.41},\n",
       " 'TFIDF_NGRAM_2_3': {'Train Accuracy': 0.24,\n",
       "  'Train Precision': 0.09,\n",
       "  'Train Recall': 0.99,\n",
       "  'Train F1': 0.16,\n",
       "  'Validation Accuracy': 0.22,\n",
       "  'Validation Precision': 0.08,\n",
       "  'Validation Recall': 0.98,\n",
       "  'Validation F1': 0.14}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_smote_lemm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COUNT_VECTORIZER': {'Train Accuracy': 0.99,\n",
       "  'Train Precision': 0.99,\n",
       "  'Train Recall': 1.0,\n",
       "  'Train F1': 0.99,\n",
       "  'Validation Accuracy': 0.95,\n",
       "  'Validation Precision': 0.62,\n",
       "  'Validation Recall': 0.69,\n",
       "  'Validation F1': 0.65},\n",
       " 'TFIDF_VECTORIZER': {'Train Accuracy': 0.93,\n",
       "  'Train Precision': 0.93,\n",
       "  'Train Recall': 0.94,\n",
       "  'Train F1': 0.93,\n",
       "  'Validation Accuracy': 0.91,\n",
       "  'Validation Precision': 0.38,\n",
       "  'Validation Recall': 0.76,\n",
       "  'Validation F1': 0.51},\n",
       " 'TFIDF_NGRAM_1_2': {'Train Accuracy': 0.94,\n",
       "  'Train Precision': 0.93,\n",
       "  'Train Recall': 0.95,\n",
       "  'Train F1': 0.94,\n",
       "  'Validation Accuracy': 0.91,\n",
       "  'Validation Precision': 0.4,\n",
       "  'Validation Recall': 0.76,\n",
       "  'Validation F1': 0.52},\n",
       " 'TFIDF_NGRAM_2_3': {'Train Accuracy': 0.65,\n",
       "  'Train Precision': 0.92,\n",
       "  'Train Recall': 0.32,\n",
       "  'Train F1': 0.48,\n",
       "  'Validation Accuracy': 0.93,\n",
       "  'Validation Precision': 0.38,\n",
       "  'Validation Recall': 0.23,\n",
       "  'Validation F1': 0.28}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression: compare vectorizers with upsampling + lemmatizing\n",
    "compare_vectorization_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                                   LogisticRegression(class_weight='balanced', solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT_VECTORIZER</th>\n",
       "      <th>TFIDF_VECTORIZER</th>\n",
       "      <th>TFIDF_NGRAM_1_2</th>\n",
       "      <th>TFIDF_NGRAM_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation F1</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Precision</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Recall</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      COUNT_VECTORIZER  TFIDF_VECTORIZER  TFIDF_NGRAM_1_2  \\\n",
       "Train Accuracy                    0.99              0.88             0.88   \n",
       "Train F1                          0.92              0.50             0.50   \n",
       "Train Precision                   0.86              0.35             0.35   \n",
       "Train Recall                      1.00              0.86             0.86   \n",
       "Validation Accuracy               0.95              0.86             0.86   \n",
       "Validation F1                     0.65              0.40             0.40   \n",
       "Validation Precision              0.62              0.28             0.28   \n",
       "Validation Recall                 0.68              0.72             0.71   \n",
       "\n",
       "                      TFIDF_NGRAM_2_3  \n",
       "Train Accuracy                   0.24  \n",
       "Train F1                         0.16  \n",
       "Train Precision                  0.09  \n",
       "Train Recall                     0.99  \n",
       "Validation Accuracy              0.21  \n",
       "Validation F1                    0.14  \n",
       "Validation Precision             0.07  \n",
       "Validation Recall                0.98  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT_VECTORIZER</th>\n",
       "      <th>TFIDF_VECTORIZER</th>\n",
       "      <th>TFIDF_NGRAM_1_2</th>\n",
       "      <th>TFIDF_NGRAM_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation F1</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Precision</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Recall</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      COUNT_VECTORIZER  TFIDF_VECTORIZER  TFIDF_NGRAM_1_2  \\\n",
       "Train Accuracy                    0.99              0.88             0.88   \n",
       "Train F1                          0.92              0.50             0.50   \n",
       "Train Precision                   0.86              0.35             0.35   \n",
       "Train Recall                      1.00              0.86             0.86   \n",
       "Validation Accuracy               0.95              0.86             0.86   \n",
       "Validation F1                     0.65              0.40             0.40   \n",
       "Validation Precision              0.62              0.28             0.28   \n",
       "Validation Recall                 0.68              0.72             0.71   \n",
       "\n",
       "                      TFIDF_NGRAM_2_3  \n",
       "Train Accuracy                   0.24  \n",
       "Train F1                         0.16  \n",
       "Train Precision                  0.09  \n",
       "Train Recall                     0.99  \n",
       "Validation Accuracy              0.21  \n",
       "Validation F1                    0.14  \n",
       "Validation Precision             0.07  \n",
       "Validation Recall                0.98  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression: compare vectorizers using stemming + class balances\n",
    "pd.DataFrame(compare_vectorization_model(X_train.stem_tweet, y_train, X_val.stem_tweet, \n",
    "                                    y_val, LogisticRegression(class_weight='balanced', solver='lbfgs')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regularization:\n",
    "\n",
    "- Count Vectorizer:   \n",
    "\n",
    "l2 (default), no alpha tuning: F1: 0.99, 0.66\n",
    "C = .1:  .91,  .52\n",
    "C = .2:  .96,  .57\n",
    "C = .3:  .98,  .58\n",
    "C = .01:  .67,  .39\n",
    "C = .001:  .62, .39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.91\n",
      "Train Precision: 0.93\n",
      "Train Recall: 0.9\n",
      "Train F1: 0.91\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.92\n",
      "Validation Precision: 0.41\n",
      "Validation Recall: 0.69\n",
      "Validation F1: 0.51\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4455</td>\n",
       "      <td>329</td>\n",
       "      <td>4784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>228</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4557</td>\n",
       "      <td>557</td>\n",
       "      <td>5114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0    1   All\n",
       "Actual                    \n",
       "0          4455  329  4784\n",
       "1           102  228   330\n",
       "All        4557  557  5114"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_vector_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, count_vect, \n",
    "                   LogisticRegression(penalty = 'l1', C = .1,  class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countvect =  count_vect.fit_transform(X_train_up.lem_tweet)\n",
    "X_val_countvect =  count_vect.transform(X_val.lem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(class_weight='balanced', penalty = 'l1', C = .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train_countvect, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_predict = logreg.predict(X_val_countvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score RF: 0.4491201439676215\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, auc, roc_curve, precision_recall_curve\n",
    "average_precision = average_precision_score(y_val, y_val_predict)\n",
    "\n",
    "print('Average precision-recall score RF: {}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_val_predict)\n",
    "# plot no skill\n",
    "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(fpr, tpr, marker='.')\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '2-class Precision-Recall curve: AP=0.45')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_val, y_val_predict)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted 0</th>\n",
       "      <th>predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual 0</th>\n",
       "      <td>4647</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual 1</th>\n",
       "      <td>103</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          predicted 0  predicted 1\n",
       "actual 0         4647          137\n",
       "actual 1          103          227"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_val, y_val_predict), index = ['actual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = []\n",
    "for item in logreg.predict_proba(X_val_countvect):\n",
    "    if item[0] <= .40:\n",
    "        predicts.append(1)\n",
    "    else:\n",
    "        predicts.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted 0</th>\n",
       "      <th>predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual 0</th>\n",
       "      <td>4686</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual 1</th>\n",
       "      <td>116</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          predicted 0  predicted 1\n",
       "actual 0         4686           98\n",
       "actual 1          116          214"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with threshhold adjustment\n",
    "pd.DataFrame(confusion_matrix(y_val, predicts), index = ['actual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class weight = balanced + lemmatized\n",
    "compare_vectorization_model(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                   SVC(class_weight ='balanced', gamma='auto', ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upsampling + lemmatized\n",
    "compare_vectorization_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                                   SVC(class_weight ='balanced', gamma ='auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE + lemmatized \n",
    "SMOTE_compare_vectorization_model(X_train.lem_tweet, y_train, X_val.lem_tweet, \n",
    "                                    y_val, SVC(class_weight ='balanced', gamma='auto', ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Searching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfid2 =  tfidf_ngram2.fit_transform(X_train_up.lemmatized_tweet)\n",
    "X_val_tfid2 =  tfidf_ngram2.transform(X_val.lemmatized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = SVC(kernel='linear', C=1, gamma=1, class_weight ='balanced')\n",
    "\n",
    "params = {\n",
    "'C': [0.1,.2, .3, 0.8,1,1.2,1.4],\n",
    "'kernel':['linear', 'rbf'],\n",
    "'gamma' :[0.1,0.8,1,1.2,1.4]\n",
    "}\n",
    "\n",
    "svm_gs= GridSearchCV(svc, param_grid = params, cv = 3)\n",
    "\n",
    "scores = ['f1','accuracy','recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_gs.fit(X_train_tfid2, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_vector_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vectorization_model(X_train_up.lemmatized_tweet, y_train_up, X_val.lemmatized_tweet, y_val, \n",
    "                                   SVC(C=1.2, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma=1.4, kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Multiple Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with class weight balances + lemmatizing \n",
    "pd.DataFrame(compare_vectorization_model(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 20, \n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with upsampling + lemmatizing \n",
    "compare_vectorization_model(X_train_up.lemmatized_tweet, y_train_up, X_val.lemmatized_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 20,\n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with SMOTE + lemmatizing  \n",
    "SMOTE_compare_vectorization_model(X_train.lemmatized_tweet, y_train, X_val.lemmatized_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 20,\n",
    "                                   n_estimators = 100, class_weight = 'balanced', random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with upsampling + stemming\n",
    "compare_vectorization_model(X_train.stemmed_tweet_meta, y_train, X_val.stemmed_tweet_meta, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 20,\n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Fine-Tuning Hyperparameters: Max depth 10.... regularization??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with upsampling + lemmatizing \n",
    "compare_vectorization_model(X_train_up.lemmatized_tweet, y_train_up, X_val.lemmatized_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 10,\n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vectorization_model(X_train_up.lemmatized_tweet, y_train_up, X_val.lemmatized_tweet, y_val, \n",
    "                                   RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
    "                       criterion='gini', max_depth=20, max_features='auto',\n",
    "                       max_leaf_nodes=200, min_impurity_decrease=0.0,\n",
    "                       min_impurity_split=None, min_samples_leaf=1,\n",
    "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                       n_estimators=100, n_jobs=None, oob_score=False,\n",
    "                       random_state=10, verbose=0, warm_start=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# rfc = RandomForestClassifier(n_estimators=60, max_depth=6, random_state=10, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countvect =  count_vect.fit_transform(X_train_up.lem_tweet)\n",
    "X_val_countvect =  count_vect.transform(X_val.lem_tweet)\n",
    "# X_test_countvect = count_vect.transform(X_test.lemmatized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "parameters = {'n_estimators' : [40, 60, 80, 100],\n",
    "'max_leaf_nodes' : [200, 400, 600],\n",
    "'random_state' : [10],\n",
    "'max_depth': [5, 7, 10, 20],\n",
    " 'verbose' : [0],\n",
    "'class_weight': ['balanced']\n",
    "             }\n",
    "          \n",
    "rfc_gs = GridSearchCV(\n",
    "    RandomForestClassifier(class_weight='balanced', random_state = 10), param_grid=parameters, cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.fit(X_train_countvect, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.score(X_val_countvect, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc2 = RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
    "                       criterion='gini', max_depth=20, max_features='auto',\n",
    "                       max_leaf_nodes=200, min_impurity_decrease=0.0,\n",
    "                       min_impurity_split=None, min_samples_leaf=1,\n",
    "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                       n_estimators=100, n_jobs=None, oob_score=False,\n",
    "                       random_state=10, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_countvect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4066454c3fd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train_countvect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_up\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_countvect' is not defined"
     ]
    }
   ],
   "source": [
    "rfc2.fit (X_train_countvect, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict = rfc2.predict(X_train_countvect)\n",
    "metrics.f1_score(y_train_up, y_train_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_predict = rfc2.predict(X_val_countvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_val, y_val_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vectorization_model(X_train.lemmatized_tweet, y_train, X_val.lemmatized_tweet, y_val, \n",
    "                     RandomForestClassifier(class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec = gensim.models.Word2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = time()\n",
    "\n",
    "# word2vec.build_vocab(df_tokenized_list, progress_per=10000)\n",
    "\n",
    "# print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X-train pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.tokenized_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_list = list(X_train['tokenized_tweet'])\n",
    "X_train_token_sumlist = sum(X_train_token_list,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unique_tokens = set(X_train_token_sumlist)\n",
    "print('The unique number of words in the training dataset is: {}'.format(len(X_train_unique_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X-val pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val_token_list = list(X_val['tokenized_tweet'])\n",
    "# X_val_token_sumlist = sum(X_val_token_list,[])\n",
    "# X_val_unique_tokens = set(X_val_token_sumlist)\n",
    "\n",
    "# print('The unique number of words in the validation dataset is: {}'.format(len(X_val_unique_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X-test pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_token_list = list(X_test['tokenized_tweet'])\n",
    "# X_test_token_sumlist = sum(X_test_token_list,[])\n",
    "\n",
    "# X_test_unique_tokens = set(X_test_token_sumlist)\n",
    "# print('The unique number of words in the training dataset is: {}'.format(len(X_test_unique_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "t = time()\n",
    "\n",
    "w2v = gensim.models.Word2Vec(X_train_token_list, sg=1, min_count=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.train(X_train_token_list, total_examples=w2v.corpus_count, epochs=w2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v.save('w2v-min1.model')\n",
    "# w2v = gensim.models.Word2Vec.load('w2v-min1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w2v.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocab= w2v.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv['trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(['trump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(positive=['lazy','black'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.get_keras_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_X = w2v.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = df_tokenized_list[1]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([w2v[w] for w in sentence if w in w2v]\n",
    "                   or [np.zeros(100)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.mean([w2v[w] for w in sentence if w in w2v]  or np.zeros(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr = np.empty((31410, 100))\n",
    "for sentence in X_train_token_list:\n",
    "    np.append(input_to_lr, np.mean([w2v[w] for w in sentence if w in w2v]\n",
    "                   or [np.zeros(100)], axis=0))\n",
    "# np.mean([w2v[w] for w in sentence if w in w2v], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp = input_to_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp_df = pd.DataFrame(X_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.fit(X_train_temp, y)\n",
    "a.score(X_train_temp, y)\n",
    "c = a.predict(X_train_temp)\n",
    "# print scores  \n",
    "print('Train Accuracy: ' + str(round(metrics.f1_score(y, c),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample, X_train_remainder, y_train_sample, y_train_remainder = train_test_split(X_train, y_train, test_size=0.99, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_RNN_sample= X_train_sample['tokenized_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_RNN_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_RNN_sample=y_train_sample\n",
    "y_RNN_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = X_RNN_sample\n",
    "# define class labels\n",
    "labels = y_RNN_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "vocab_size = 100\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.Word2Vec(df_tokenized_list, size=dimsize, window=5, min_count=50, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne_plot(w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of hidden layer (length of continuous word representation)\n",
    "dimsize= 100\n",
    "\n",
    "# model_w2v = gensim.models.Word2Vec(X_train_token_list, size= dimsize, window=5, min_count=1, workers=4)\n",
    "model_w2v = gensim.models.Word2Vec(X_train_token_list, size= dimsize,min_count=1)\n",
    "\n",
    "\n",
    "\n",
    "#create average vector for train and test from model\n",
    "#returned list of numpy arrays are then stacked \n",
    "X_train_w2v = np.concatenate([avg_word_vectors(w, dimsize, model_w2v) for w in X_train_token_list])\n",
    "X_val_w2v = np.concatenate([avg_word_vectors(w,dimsize, model_w2v) for w in X_val_token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_w2v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr = np.empty((31410, 100))\n",
    "for sentence in X_train_token_list:\n",
    "    np.append(input_to_lr, np.mean([model_w2v[w] for w in sentence if w in model_w2v]\n",
    "                   or [np.zeros(100)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_w2v_model (X_train_w2v, y_train, X_val_w2v, y_val, classifier):\n",
    "        \n",
    "    smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "        \n",
    "    pipe = make_pipeline(smote, classifier) \n",
    "    \n",
    "    model = pipe.fit(X_train_w2v, y_train)\n",
    "    \n",
    "    train_predictions = model.predict(X_train_w2v)\n",
    "    val_predictions = model.predict (X_val_w2v)\n",
    "    \n",
    "   # print scores  \n",
    "    print('Train Accuracy: ' + str(round(metrics.accuracy_score(y_train, train_predictions),2)))\n",
    "    print('Train Precision: ' + str(round(metrics.precision_score(y_train, train_predictions),2)))\n",
    "    print('Train Recall: ' + str(round(metrics.recall_score(y_train, train_predictions),2)))\n",
    "    print('Train F1: ' + str(round(metrics.f1_score(y_train, train_predictions),2)))\n",
    "    print('\\n')\n",
    "    print('Validation Accuracy: ' + str(round(metrics.accuracy_score(y_val, val_predictions),2)))\n",
    "    print('Validation Precision: ' + str(round(metrics.precision_score(y_val, val_predictions),2)))\n",
    "    print('Validation Recall: ' + str(round(metrics.recall_score(y_val, val_predictions),2)))\n",
    "    print('Validation F1: ' + str(round(metrics.f1_score(y_val, val_predictions),2)))\n",
    "\n",
    "    log_confusion_test = pd.crosstab(y_val, val_predictions, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    return log_confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model(X_train_w2v, y_train, X_val_w2v, y_val, LogisticRegression(solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model(X_train_w2v, y_train, X_val_w2v, y_val, RandomForestClassifier(n_estimators=100, max_depth= 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_smote_w2v_model (X_train_w2v, y_train, X_val_w2v, y_val, classifier):\n",
    "    \n",
    "    pca = decomposition.PCA(n_components=50)\n",
    "    \n",
    "    smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "        \n",
    "    pipe = make_pipeline(pca, smote, classifier) \n",
    "    \n",
    "    model = pipe.fit(X_train_w2v, y_train)\n",
    "    \n",
    "    train_predictions = model.predict(X_train_w2v)\n",
    "    val_predictions = model.predict (X_val_w2v)\n",
    "    \n",
    "   # print scores  \n",
    "    print('Train Accuracy: ' + str(round(metrics.accuracy_score(y_train, train_predictions),2)))\n",
    "    print('Train Precision: ' + str(round(metrics.precision_score(y_train, train_predictions),2)))\n",
    "    print('Train Recall: ' + str(round(metrics.recall_score(y_train, train_predictions),2)))\n",
    "    print('Train F1: ' + str(round(metrics.f1_score(y_train, train_predictions),2)))\n",
    "    print('\\n')\n",
    "    print('Validation Accuracy: ' + str(round(metrics.accuracy_score(y_val, val_predictions),2)))\n",
    "    print('Validation Precision: ' + str(round(metrics.precision_score(y_val, val_predictions),2)))\n",
    "    print('Validation Recall: ' + str(round(metrics.recall_score(y_val, val_predictions),2)))\n",
    "    print('Validation F1: ' + str(round(metrics.f1_score(y_val, val_predictions),2)))\n",
    "\n",
    "    log_confusion_test = pd.crosstab(y_val, val_predictions, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    return log_confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_smote_w2v_model(X_train_w2v, y_train, X_val_w2v, y_val, LogisticRegression(solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_smote_w2v_model(X_train_w2v, y_train, X_val_w2v, y_val, RandomForestClassifier(n_estimators=100, max_depth=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install glove_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install glovepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = 'glove.twitter.27B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_input_file = 'glove.twitter.27B.100d.txt'\n",
    "glove_output_file = 'glove.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, glove_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = KeyedVectors.load_word2vec_format('glove.txt.word2vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glove_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove = np.concatenate([avg_word_vectors(w, dimsize, glove_model) for w in X_train_token_list])\n",
    "X_val_glove = np.concatenate([avg_word_vectors(w, dimsize, glove_model) for w in X_val_token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove[255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove_2 = np.empty((31410, 100))\n",
    "for sentence in X_train_token_list:\n",
    "    np.append(input_to_lr, np.mean([glove_model[w] for w in sentence if w in glove_model]\n",
    "                   or [np.zeros(100)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove_2[225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Learnco "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict = {}\n",
    "with open('glove.twitter.27B.100d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in X_train_unique_tokens:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_to_lr = np.empty((31410, 100))\n",
    "for sentence in X_train_token_list:\n",
    "    np.append(input_to_lr, np.mean([w2v[w] for w in sentence if w in w2v]\n",
    "                   or [np.zeros(100)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_smote_w2v_model(X_train_glove, y_train, X_val_glove, y_val, RandomForestClassifier(n_estimators=100, max_depth=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_w2v_model (X_train_w2v, y_train, X_val_w2v, y_val, classifier):\n",
    "        \n",
    "    smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "        \n",
    "    pipe = make_pipeline(smote, classifier) \n",
    "    \n",
    "    model = pipe.fit(X_train_w2v, y_train)\n",
    "    \n",
    "    train_predictions = model.predict(X_train_w2v)\n",
    "    val_predictions = model.predict (X_val_w2v)\n",
    "    \n",
    "   # print scores  \n",
    "    print('Train Accuracy: ' + str(round(metrics.accuracy_score(y_train, train_predictions),2)))\n",
    "    print('Train Precision: ' + str(round(metrics.precision_score(y_train, train_predictions),2)))\n",
    "    print('Train Recall: ' + str(round(metrics.recall_score(y_train, train_predictions),2)))\n",
    "    print('Train F1: ' + str(round(metrics.f1_score(y_train, train_predictions),2)))\n",
    "    print('\\n')\n",
    "    print('Validation Accuracy: ' + str(round(metrics.accuracy_score(y_val, val_predictions),2)))\n",
    "    print('Validation Precision: ' + str(round(metrics.precision_score(y_val, val_predictions),2)))\n",
    "    print('Validation Recall: ' + str(round(metrics.recall_score(y_val, val_predictions),2)))\n",
    "    print('Validation F1: ' + str(round(metrics.f1_score(y_val, val_predictions),2)))\n",
    "\n",
    "    log_confusion_test = pd.crosstab(y_val, val_predictions, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    return log_confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, RandomForestClassifier(max_depth=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, RandomForestClassifier(max_depth=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, LogisticRegression(penalty ='l1', C = 10,\n",
    "                                                                            class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, LogisticRegression(penalty ='l1', C = .001,\n",
    "                                                                            class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, LogisticRegression(penalty ='l1', class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, \n",
    "                 LogisticRegression(penalty ='l2', C = 5, class_weight ={0: 5 , 1: 5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, \n",
    "                 LogisticRegression(penalty ='l2', C = .1, class_weight ={0: 5 , 1: 5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Trump Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>no_hash_tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lem_tweet</th>\n",
       "      <th>stem_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>So Robert Mueller has now asked for his long t...</td>\n",
       "      <td>Robert Mueller asked long time Never Trumper l...</td>\n",
       "      <td>Robert Mueller asked long time Never Trumper l...</td>\n",
       "      <td>['Robert', 'Mueller', 'asked', 'long', 'time',...</td>\n",
       "      <td>['robert', 'mueller', 'ask', 'long', 'time', '...</td>\n",
       "      <td>['Robert', 'Mueller', 'asked', 'long', 'time',...</td>\n",
       "      <td>Robert Mueller asked long time Never Trumper l...</td>\n",
       "      <td>robert mueller asked long time never trumper l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The young leaders here today (@TPUSA) are part...</td>\n",
       "      <td>young leaders here today part movement unlike ...</td>\n",
       "      <td>young leaders here today part movement unlike ...</td>\n",
       "      <td>['young', 'leaders', 'here', 'today', 'part', ...</td>\n",
       "      <td>['young', 'leader', 'here', 'today', 'part', '...</td>\n",
       "      <td>['young', 'leader', 'here', 'today', 'part', '...</td>\n",
       "      <td>young leaders here today part movement unlike ...</td>\n",
       "      <td>young leaders here today part movement unlike ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Congratulations to our new Secretary of Defens...</td>\n",
       "      <td>Congratulations Secretary Defense Mark twitter</td>\n",
       "      <td>Congratulations Secretary Defense Mark twitter</td>\n",
       "      <td>['Congratulations', 'Secretary', 'Defense', 'M...</td>\n",
       "      <td>['congratul', 'secretari', 'defens', 'mark', '...</td>\n",
       "      <td>['Congratulations', 'Secretary', 'Defense', 'M...</td>\n",
       "      <td>Congratulations Secretary Defense Mark twitter</td>\n",
       "      <td>congratulations secretary defense mark twitt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Oh well, we still have the great @seanhannity ...</td>\n",
       "      <td>well still have great hear really strong show ...</td>\n",
       "      <td>well still have great hear really strong show ...</td>\n",
       "      <td>['well', 'still', 'have', 'great', 'hear', 're...</td>\n",
       "      <td>['well', 'still', 'have', 'great', 'hear', 're...</td>\n",
       "      <td>['well', 'still', 'have', 'great', 'hear', 're...</td>\n",
       "      <td>well still have great hear really strong show ...</td>\n",
       "      <td>well still have great hear really strong show ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>THANK YOU @TPUSA! #MAGApic.twitter.com/1eqR2Im8dQ</td>\n",
       "      <td>THANK #MAGApic twitter</td>\n",
       "      <td>THANK MAGApic twitter</td>\n",
       "      <td>['THANK', 'MAGApic', 'twitter']</td>\n",
       "      <td>['thank', 'magap', 'twitter']</td>\n",
       "      <td>['THANK', 'MAGApic', 'twitter']</td>\n",
       "      <td>THANK MAGApic twitter</td>\n",
       "      <td>thank magapic twitt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              tweet  \\\n",
       "0           0  So Robert Mueller has now asked for his long t...   \n",
       "1           1  The young leaders here today (@TPUSA) are part...   \n",
       "2           2  Congratulations to our new Secretary of Defens...   \n",
       "3           3  Oh well, we still have the great @seanhannity ...   \n",
       "4           4  THANK YOU @TPUSA! #MAGApic.twitter.com/1eqR2Im8dQ   \n",
       "\n",
       "                                          tidy_tweet  \\\n",
       "0  Robert Mueller asked long time Never Trumper l...   \n",
       "1  young leaders here today part movement unlike ...   \n",
       "2     Congratulations Secretary Defense Mark twitter   \n",
       "3  well still have great hear really strong show ...   \n",
       "4                             THANK #MAGApic twitter   \n",
       "\n",
       "                                       no_hash_tweet  \\\n",
       "0  Robert Mueller asked long time Never Trumper l...   \n",
       "1  young leaders here today part movement unlike ...   \n",
       "2     Congratulations Secretary Defense Mark twitter   \n",
       "3  well still have great hear really strong show ...   \n",
       "4                              THANK MAGApic twitter   \n",
       "\n",
       "                                     tokenized_tweet  \\\n",
       "0  ['Robert', 'Mueller', 'asked', 'long', 'time',...   \n",
       "1  ['young', 'leaders', 'here', 'today', 'part', ...   \n",
       "2  ['Congratulations', 'Secretary', 'Defense', 'M...   \n",
       "3  ['well', 'still', 'have', 'great', 'hear', 're...   \n",
       "4                    ['THANK', 'MAGApic', 'twitter']   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  ['robert', 'mueller', 'ask', 'long', 'time', '...   \n",
       "1  ['young', 'leader', 'here', 'today', 'part', '...   \n",
       "2  ['congratul', 'secretari', 'defens', 'mark', '...   \n",
       "3  ['well', 'still', 'have', 'great', 'hear', 're...   \n",
       "4                      ['thank', 'magap', 'twitter']   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  ['Robert', 'Mueller', 'asked', 'long', 'time',...   \n",
       "1  ['young', 'leader', 'here', 'today', 'part', '...   \n",
       "2  ['Congratulations', 'Secretary', 'Defense', 'M...   \n",
       "3  ['well', 'still', 'have', 'great', 'hear', 're...   \n",
       "4                    ['THANK', 'MAGApic', 'twitter']   \n",
       "\n",
       "                                           lem_tweet  \\\n",
       "0  Robert Mueller asked long time Never Trumper l...   \n",
       "1  young leaders here today part movement unlike ...   \n",
       "2     Congratulations Secretary Defense Mark twitter   \n",
       "3  well still have great hear really strong show ...   \n",
       "4                              THANK MAGApic twitter   \n",
       "\n",
       "                                          stem_tweet  \n",
       "0  robert mueller asked long time never trumper l...  \n",
       "1  young leaders here today part movement unlike ...  \n",
       "2       congratulations secretary defense mark twitt  \n",
       "3  well still have great hear really strong show ...  \n",
       "4                                thank magapic twitt  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_df= pd.read_csv('data/cleaned-trump-tweet.csv')\n",
    "trump_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     robert mueller asked long time never trumper l...\n",
       "1     young leaders here today part movement unlike ...\n",
       "2          congratulations secretary defense mark twitt\n",
       "3     well still have great hear really strong show ...\n",
       "4                                   thank magapic twitt\n",
       "5     just watched eric swalwell asked endless softb...\n",
       "6     just back only hear last minute change allowin...\n",
       "7     budget deal gives great victories military vet...\n",
       "8     know over many years chairman police athletic ...\n",
       "9     saddened learn recent passing morgenthau truly...\n",
       "10    leaving turning point will speaking some great...\n",
       "11    when wall southern border that crumbling falli...\n",
       "12    almost minnesota because america hating anti s...\n",
       "13    tariffs remittance fees above guatemala been g...\n",
       "14    guatemala which been forming caravans sending ...\n",
       "15    farmers starting great again after years downw...\n",
       "16                                   keep america great\n",
       "17    congratulations boris johnson becoming prime m...\n",
       "18    best newest military almost totally rebuilt fr...\n",
       "19    newest poll only favor starting ridiculous imp...\n",
       "Name: stem_tweet, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_df.stem_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countvect =  count_vect.fit_transform(X_train_up.lem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train_countvect, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trump = count_vect.transform(trump_df.lem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trump = X_trump.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 283)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trump.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37982, 9)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_up.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20455, 9)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
