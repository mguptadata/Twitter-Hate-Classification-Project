{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling and Downsampling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_training_data(X_train, y_train):\n",
    "    \n",
    "    '''\n",
    "    draws samples from the minoriy class (label = 1) until it is the size of the minority class (label = 0)\n",
    "\n",
    "    returns a single upsampled dataframe (with both X_train and y_train in one dataframe)\n",
    "\n",
    "    '''\n",
    "    training_data = X_train.copy()\n",
    "    training_data['label']= y_train\n",
    "\n",
    "    train_0 = training_data[training_data.label==0]\n",
    "    train_1 = training_data[training_data.label==1]\n",
    "\n",
    "    train_1_up = resample(train_1, \n",
    "          replace=True,    \n",
    "          n_samples=len(train_0),   \n",
    "          random_state=10)\n",
    "\n",
    "    train_upsampled = pd.concat([train_1_up, train_0])\n",
    "    \n",
    "    return train_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_training_data(X_train, y_train):\n",
    "    \n",
    "    '''\n",
    "    draws samples from the majority class (label = 0) until it is the size of the minority class (label = 1)\n",
    "\n",
    "    returns a single downsampled dataframe (with both X_train and y_train in one dataframe)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    training_data = X_train.copy()\n",
    "    training_data['label']= y_train\n",
    "    \n",
    "    train_0 = training_data[training_data.label==0]\n",
    "    train_1 = training_data[training_data.label==1]\n",
    "\n",
    "    train_0_down = resample(train_0, \n",
    "          replace=True,    \n",
    "          n_samples=len(train_1),   \n",
    "          random_state=10)\n",
    "\n",
    "    train_downsampled = pd.concat([train_0_down, train_1])\n",
    "    \n",
    "    return train_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Test Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_compare_vectorization_model(X_train_col, y_train, X_val_col, y_val, classifier):\n",
    "    \n",
    "    '''\n",
    "    Adapts the 'compare vectorization model' function to work specifically for Naive Bayes by transforming the\n",
    "    sparse vector matrices to dense numpy arrays\n",
    "\n",
    "    No explicit correction for class imbalance is made in this function, \n",
    "    but up or downsampled data can be passed in as arguments.\n",
    "\n",
    "    Vectorization methods should be specified outside the function in a 'vectorization list',\n",
    "    which is a list of tuples specifying each name and vectorization method to be used.\n",
    "\n",
    "    Parameters:\n",
    "    \n",
    "    X_train_col: cleaned text column in training set\n",
    "    \n",
    "    y_train: target variable in training set\n",
    "    \n",
    "    X_val_col: cleaned text column in validation set\n",
    "    \n",
    "    y_val: target variable in validation set \n",
    "    \n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "\n",
    "    '''    \n",
    " \n",
    "    metrics_dict = {}\n",
    "\n",
    "    for name, vectorizer in vectorization_list:\n",
    "    \n",
    "        X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "        X_val_transformed = vectorizer.transform (X_val_col)\n",
    "        \n",
    "        X_train_transformed = X_train_transformed.toarray()\n",
    "        X_val_transformed = X_val_transformed.toarray()\n",
    "        \n",
    "        classifier.fit(X_train_transformed, y_train.values.ravel())\n",
    "    \n",
    "        train_predictions = classifier.predict (X_train_transformed)\n",
    "        val_predictions = classifier.predict (X_val_transformed)   \n",
    "    \n",
    "    #       print scores  \n",
    "        print ('The performance of the {} is:'.format((name)))\n",
    "        print('Train Accuracy: ' + str(round(metrics.accuracy_score(y_train, train_predictions),2)))\n",
    "        print('Train Precision: ' + str(round(metrics.precision_score(y_train, train_predictions),2)))\n",
    "        print('Train Recall: ' + str(round(metrics.recall_score(y_train, train_predictions),2)))\n",
    "        print('Train F1: ' + str(round(metrics.f1_score(y_train,train_predictions),2)))\n",
    "        print('\\n')\n",
    "        print('Validation Accuracy: ' + str(round(metrics.accuracy_score(y_val, val_predictions),2)))\n",
    "        print('Validation Precision: ' + str(round(metrics.precision_score(y_val, val_predictions),2)))\n",
    "        print('Validation Recall: ' + str(round(metrics.recall_score(y_val, val_predictions),2)))\n",
    "        print('Validation F1: ' + str(round(metrics.f1_score(y_val, val_predictions),2)))\n",
    "        print('\\n')\n",
    "        \n",
    "   \n",
    "        metrics_dict[name] = {\n",
    "        'Train Accuracy' : metrics.accuracy_score(y_train, train_predictions),\n",
    "        'Train Precision' : metrics.precision_score(y_train, train_predictions),\n",
    "        'Train Recall' : metrics.recall_score(y_train, train_predictions),\n",
    "        'Train F1': metrics.f1_score(y_train,train_predictions),\n",
    "        \n",
    "        'Validation Accuracy': metrics.accuracy_score(y_val, val_predictions),\n",
    "        'Validation Precision' : metrics.precision_score(y_val, val_predictions),\n",
    "        'Validation Recall': metrics.recall_score(y_val, val_predictions),\n",
    "        'Validation F1': metrics.f1_score(y_val, val_predictions)\n",
    "        }\n",
    "        \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_vector_model(X_train_col, y_train, X_val_col, y_val, vectorizer, classifier):\n",
    "    \n",
    "    '''\n",
    "    Apply the specified text vectorizer,make predictions and calculate scores with specified classifier.\n",
    "\n",
    "    No explicit correction for class imbalances is conducted in this function, \n",
    "    but up or downsampled X_train and y_train variables can be passed as arguments\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train_col: specify cleaned text column to be used for vectorization and predictions\n",
    "\n",
    "    y_train: enter as a one-dimensional vector; function transforms into an array\n",
    "\n",
    "    X_val_col: specify cleaned text column to be used for vectorization and predictions\n",
    "\n",
    "    y_val:  enter as a one-dimensional vector; function transforms into an array\n",
    "\n",
    "    vectorizer: indicate text vectorization method; uses default parameters if none are specified\n",
    "\n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "\n",
    "    '''\n",
    "    \n",
    "    X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "    X_val_transformed = vectorizer.transform(X_val_col)\n",
    "            \n",
    "    classifier.fit(X_train_transformed, y_train.values.ravel())\n",
    "    \n",
    "    train_predictions = classifier.predict(X_train_transformed)\n",
    "    val_predictions = classifier.predict (X_val_transformed)\n",
    "    \n",
    "   # print scores  \n",
    "    print('Train Accuracy: ' + str(round(metrics.accuracy_score(y_train, train_predictions),2)))\n",
    "    print('Train Precision: ' + str(round(metrics.precision_score(y_train, train_predictions),2)))\n",
    "    print('Train Recall: ' + str(round(metrics.recall_score(y_train, train_predictions),2)))\n",
    "    print('Train F1: ' + str(round(metrics.f1_score(y_train, train_predictions),2)))\n",
    "    print('\\n')\n",
    "    print('Validation Accuracy: ' + str(round(metrics.accuracy_score(y_val, val_predictions),2)))\n",
    "    print('Validation Precision: ' + str(round(metrics.precision_score(y_val, val_predictions),2)))\n",
    "    print('Validation Recall: ' + str(round(metrics.recall_score(y_val, val_predictions),2)))\n",
    "    print('Validation F1: ' + str(round(metrics.f1_score(y_val, val_predictions),2)))\n",
    "\n",
    "    log_confusion_test = pd.crosstab(y_val, val_predictions, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    return log_confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMOTE_vector_model(X_train_col, y_train, X_val_col, y_val, vectorizer, classifier):\n",
    "    '''\n",
    "    Apply the specified text vectorizer, use SMOTE to rebalance class sizes, \n",
    "    and then make predictions and calculate scores for specified classifier\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    X_train_col: specify cleaned text column to be used for vectorization and predictions\n",
    "\n",
    "    y_train: enter as a one-dimensional vector; function transforms into an array\n",
    "\n",
    "    X_val_col: specify cleaned text column to be used for vectorization and predictions\n",
    "\n",
    "    y_val_col:  enter as a one-dimensional vector; function transforms into an array\n",
    "\n",
    "    vectorizer: indicate text vectorization method; uses default parameters if none are specified\n",
    "\n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "\n",
    "    '''\n",
    "    \n",
    "    X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "    X_val_transformed = vectorizer.transform(X_val_col)\n",
    "    \n",
    "    smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "    \n",
    "    pipe = make_pipeline(smote, classifier) \n",
    "    \n",
    "    model = pipe.fit(X_train_transformed, y_train.values.ravel())\n",
    "    \n",
    "    train_predictions = model.predict(X_train_transformed)\n",
    "    val_predictions = model.predict (X_val_transformed)\n",
    "    \n",
    "    # print scores  \n",
    "    print('Train Accuracy: ' + str(round(metrics.accuracy_score(y_train, train_predictions),2)))\n",
    "    print('Train Precision: ' + str(round(metrics.precision_score(y_train, train_predictions),2)))\n",
    "    print('Train Recall: ' + str(round(metrics.recall_score(y_train, train_predictions),2)))\n",
    "    print('Train F1: ' + str(round(metrics.f1_score(y_train, train_predictions),2)))\n",
    "    print('\\n')\n",
    "    print('Validation Accuracy: ' + str(round(metrics.accuracy_score(y_val, val_predictions),2)))\n",
    "    print('Validation Precision: ' + str(round(metrics.precision_score(y_val, val_predictions),2)))\n",
    "    print('Validation Recall: ' + str(round(metrics.recall_score(y_val, val_predictions),2)))\n",
    "    print('Validation F1: ' + str(round(metrics.f1_score(y_val, val_predictions),2)))\n",
    "\n",
    "    log_confusion_test = pd.crosstab(y_val, val_predictions, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    return log_confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_vectorization_model(X_train_col, y_train, X_val_col, y_val, classifier):\n",
    "    '''\n",
    "    Compares classification model performance using different text vectorizers,\n",
    "    (declared in 'vectorization list') outside the function.\n",
    "        \n",
    "    Parameters:\n",
    "    \n",
    "    X_train_col: cleaned text column in training set\n",
    "    \n",
    "    y_train: target variable in training set\n",
    "    \n",
    "    X_val_col: cleaned text column in validation set\n",
    "    \n",
    "    y_val: target variable in validation set \n",
    "    \n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "    \n",
    "    '''\n",
    "        \n",
    "    metrics_dict = {}\n",
    "        \n",
    "    for name, vectorizer in vectorization_list:\n",
    "                \n",
    "        X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "        X_val_transformed = vectorizer.transform (X_val_col)\n",
    "\n",
    "        classifier.fit(X_train_transformed, y_train.values.ravel())\n",
    "    \n",
    "        train_predictions = classifier.predict (X_train_transformed)\n",
    "        val_predictions = classifier.predict (X_val_transformed)   \n",
    "    \n",
    "        metrics_dict[name] = {\n",
    "        'Train Accuracy' : round(metrics.accuracy_score(y_train, train_predictions),2),\n",
    "        'Train Precision' : round(metrics.precision_score(y_train, train_predictions),2),\n",
    "        'Train Recall' : round(metrics.recall_score(y_train, train_predictions),2),\n",
    "        'Train F1': round(metrics.f1_score(y_train,train_predictions),2),\n",
    "        \n",
    "        'Validation Accuracy': round(metrics.accuracy_score(y_val, val_predictions),2),\n",
    "        'Validation Precision' : round(metrics.precision_score(y_val, val_predictions),2),\n",
    "        'Validation Recall': round(metrics.recall_score(y_val, val_predictions),2),\n",
    "        'Validation F1': round(metrics.f1_score(y_val, val_predictions),2)\n",
    "        }\n",
    "        \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMOTE_compare_vectorization_model(X_train_col, y_train, X_val_col, y_val, classifier):\n",
    "  \n",
    "    '''\n",
    "    Compares the performance of a single classifier using different text vectorization methods.\n",
    "\n",
    "    Uses SMOTE to rebalance class sizes before predictions are made and scored.\n",
    "\n",
    "    Vectorization methods should be specified outside the function in a 'vectorization list',\n",
    "    which is a list of tuples specifying each name and vectorization method to be used.\n",
    "\n",
    "    Parameters:\n",
    "    \n",
    "    X_train_col: cleaned text column in training set\n",
    "    \n",
    "    y_train: target variable in training set\n",
    "    \n",
    "    X_val_col: cleaned text column in validation set\n",
    "    \n",
    "    y_val: target variable in validation set \n",
    "    \n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "\n",
    "    '''   \n",
    "    metrics_dict = {}\n",
    "        \n",
    "    for name, vectorizer in vectorization_list:\n",
    "              \n",
    "        X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "        X_val_transformed = vectorizer.transform (X_val_col)\n",
    "        \n",
    "        smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "    \n",
    "        pipe = make_pipeline(smote, classifier) \n",
    "    \n",
    "        model = pipe.fit(X_train_transformed, y_train.values.ravel())\n",
    "    \n",
    "        train_predictions = model.predict(X_train_transformed)\n",
    "        val_predictions = model.predict (X_val_transformed)\n",
    "    \n",
    "        metrics_dict[name] = {\n",
    "        'Train Accuracy' : round(metrics.accuracy_score(y_train, train_predictions),2),\n",
    "        'Train Precision' : round(metrics.precision_score(y_train, train_predictions),2),\n",
    "        'Train Recall' : round(metrics.recall_score(y_train, train_predictions),2),\n",
    "        'Train F1': round(metrics.f1_score(y_train,train_predictions),2),\n",
    "        \n",
    "        'Validation Accuracy': round(metrics.accuracy_score(y_val, val_predictions),2),\n",
    "        'Validation Precision' : round(metrics.precision_score(y_val, val_predictions),2),\n",
    "        'Validation Recall': round(metrics.recall_score(y_val, val_predictions),2),\n",
    "        'Validation F1': round(metrics.f1_score(y_val, val_predictions),2)\n",
    "        }\n",
    "        \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMOTE_compare_vectorization_model2(X_train_col, y_train, X_val_col, y_val, classifier, smote = False):\n",
    "  \n",
    "    '''\n",
    "    Compares the performance of a single classifier using different text vectorization methods.\n",
    "\n",
    "    Uses SMOTE to rebalance class sizes before predictions are made and scored.\n",
    "\n",
    "    Vectorization methods should be specified outside the function in a 'vectorization list',\n",
    "    which is a list of tuples specifying each name and vectorization method to be used.\n",
    "\n",
    "    Parameters:\n",
    "    \n",
    "    X_train_col: cleaned text column in training set\n",
    "    y_train: target variable in training set    \n",
    "    X_val_col: cleaned text column in validation set\n",
    "    y_val: target variable in validation set \n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "\n",
    "    '''   \n",
    "    metrics_dict = {}\n",
    "    \n",
    "    for name, vectorizer in vectorization_list:\n",
    "\n",
    "        if smote == True:\n",
    "                      \n",
    "            X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "            X_val_transformed = vectorizer.transform (X_val_col)\n",
    "        \n",
    "            smote = SMOTE(random_state = 1, sampling_strategy = 'not majority')\n",
    "    \n",
    "            pipe = make_pipeline(smote, classifier) \n",
    "    \n",
    "            model = pipe.fit(X_train_transformed, y_train.values.ravel())\n",
    "    \n",
    "            train_predictions = model.predict(X_train_transformed)\n",
    "            val_predictions = model.predict (X_val_transformed)\n",
    "            \n",
    "            metrics_dict[name] = {\n",
    "            'Train Accuracy' : round(metrics.accuracy_score(y_train, train_predictions),2),\n",
    "            'Train Precision' : round(metrics.precision_score(y_train, train_predictions),2),\n",
    "            'Train Recall' : round(metrics.recall_score(y_train, train_predictions),2),\n",
    "            'Train F1': round(metrics.f1_score(y_train,train_predictions),2),\n",
    "        \n",
    "            'Validation Accuracy': round(metrics.accuracy_score(y_val, val_predictions),2),\n",
    "            'Validation Precision' : round(metrics.precision_score(y_val, val_predictions),2),\n",
    "            'Validation Recall': round(metrics.recall_score(y_val, val_predictions),2),\n",
    "            'Validation F1': round(metrics.f1_score(y_val, val_predictions),2)\n",
    "            }\n",
    "                            \n",
    "        else:\n",
    "                                \n",
    "            X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "            X_val_transformed = vectorizer.transform (X_val_col)\n",
    "\n",
    "            classifier.fit(X_train_transformed, y_train.values.ravel())\n",
    "    \n",
    "            train_predictions = classifier.predict (X_train_transformed)\n",
    "            val_predictions = classifier.predict (X_val_transformed) \n",
    "            \n",
    "            metrics_dict[name] = {\n",
    "            'Train Accuracy' : round(metrics.accuracy_score(y_train, train_predictions),2),\n",
    "            'Train Precision' : round(metrics.precision_score(y_train, train_predictions),2),\n",
    "            'Train Recall' : round(metrics.recall_score(y_train, train_predictions),2),\n",
    "            'Train F1': round(metrics.f1_score(y_train,train_predictions),2),\n",
    "        \n",
    "            'Validation Accuracy': round(metrics.accuracy_score(y_val, val_predictions),2),\n",
    "            'Validation Precision' : round(metrics.precision_score(y_val, val_predictions),2),\n",
    "            'Validation Recall': round(metrics.recall_score(y_val, val_predictions),2),\n",
    "            'Validation F1': round(metrics.f1_score(y_val, val_predictions),2)\n",
    "            }\n",
    "            \n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMOTE_compare_vectorization_model3(X_train_col, y_train, X_val_col, y_val, classifier, smote = False):\n",
    "  \n",
    "    '''\n",
    "    Compares the performance of a single classifier using different text vectorization methods.\n",
    "\n",
    "    Uses SMOTE to rebalance class sizes before predictions are made and scored.\n",
    "\n",
    "    Vectorization methods should be specified outside the function in a 'vectorization list',\n",
    "    which is a list of tuples specifying each name and vectorization method to be used.\n",
    "\n",
    "    Parameters:\n",
    "    \n",
    "    X_train_col: cleaned text column in training set\n",
    "    y_train: target variable in training set    \n",
    "    X_val_col: cleaned text column in validation set\n",
    "    y_val: target variable in validation set \n",
    "    classifier: name of classifier; uses default parameters if none are specified\n",
    "\n",
    "    '''   \n",
    "    metrics_dict = {}\n",
    "    \n",
    "    for name, vectorizer in vectorization_list:\n",
    "\n",
    "        if smote == True:\n",
    "                      \n",
    "            X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "            X_val_transformed = vectorizer.transform (X_val_col)\n",
    "        \n",
    "            smote = SMOTE(random_state = 1, sampling_strategy = 'not majority')\n",
    "    \n",
    "            pipe = make_pipeline(smote, classifier) \n",
    "    \n",
    "            model = pipe.fit(X_train_transformed, y_train.values.ravel())\n",
    "    \n",
    "            train_predictions = model.predict(X_train_transformed)\n",
    "            val_predictions = model.predict (X_val_transformed)\n",
    "            \n",
    "            metrics_dict[name] = {\n",
    "            'Train Accuracy' : round(metrics.accuracy_score(y_train, train_predictions),2),\n",
    "            'Train Precision' : round(metrics.precision_score(y_train, train_predictions),2),\n",
    "            'Train Recall' : round(metrics.recall_score(y_train, train_predictions),2),\n",
    "            'Train F1': round(metrics.f1_score(y_train,train_predictions),2),\n",
    "        \n",
    "            'Validation Accuracy': round(metrics.accuracy_score(y_val, val_predictions),2),\n",
    "            'Validation Precision' : round(metrics.precision_score(y_val, val_predictions),2),\n",
    "            'Validation Recall': round(metrics.recall_score(y_val, val_predictions),2),\n",
    "            'Validation F1': round(metrics.f1_score(y_val, val_predictions),2)\n",
    "            }\n",
    "                            \n",
    "        else:\n",
    "                                \n",
    "            X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "            X_val_transformed = vectorizer.transform (X_val_col)\n",
    "\n",
    "            classifier.fit(X_train_transformed, y_train.values.ravel())\n",
    "    \n",
    "            train_predictions = classifier.predict (X_train_transformed)\n",
    "            val_predictions = classifier.predict (X_val_transformed) \n",
    "            \n",
    "            metrics_dict[name] = {\n",
    "            'Train Accuracy' : round(metrics.accuracy_score(y_train, train_predictions),2),\n",
    "            'Train Precision' : round(metrics.precision_score(y_train, train_predictions),2),\n",
    "            'Train Recall' : round(metrics.recall_score(y_train, train_predictions),2),\n",
    "            'Train F1': round(metrics.f1_score(y_train,train_predictions),2),\n",
    "        \n",
    "            'Validation Accuracy': round(metrics.accuracy_score(y_val, val_predictions),2),\n",
    "            'Validation Precision' : round(metrics.precision_score(y_val, val_predictions),2),\n",
    "            'Validation Recall': round(metrics.recall_score(y_val, val_predictions),2),\n",
    "            'Validation F1': round(metrics.f1_score(y_val, val_predictions),2)\n",
    "            }\n",
    "            \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_classify (X_train_col, y_train, X_val_col, y_val):\n",
    "            \n",
    "    X_train_transformed = vectorizer.fit_transform(X_train_col)\n",
    "    X_val_transformed = vectorizer.transform (X_val_col)\n",
    "\n",
    "    smote = SMOTE(random_state = 1, sampling_strategy = 'not majority')\n",
    "\n",
    "    pipe = make_pipeline(smote, classifier) \n",
    "\n",
    "    model = pipe.fit(X_train_transformed, y_train.values.ravel())\n",
    "\n",
    "    train_predictions = model.predict(X_train_transformed)\n",
    "    val_predictions = model.predict (X_val_transformed)\n",
    "\n",
    "    metrics_dict[name] = {\n",
    "    'Train Accuracy' : round(metrics.accuracy_score(y_train, train_predictions),2),\n",
    "    'Train Precision' : round(metrics.precision_score(y_train, train_predictions),2),\n",
    "    'Train Recall' : round(metrics.recall_score(y_train, train_predictions),2),\n",
    "    'Train F1': round(metrics.f1_score(y_train,train_predictions),2),\n",
    "\n",
    "    'Validation Accuracy': round(metrics.accuracy_score(y_val, val_predictions),2),\n",
    "    'Validation Precision' : round(metrics.precision_score(y_val, val_predictions),2),\n",
    "    'Validation Recall': round(metrics.recall_score(y_val, val_predictions),2),\n",
    "    'Validation F1': round(metrics.f1_score(y_val, val_predictions),2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    '''\n",
    "    Plots the word embeddings created by the word2vec model\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    model: variable name for word2vec model\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=250, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "#tnse_plot(model_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_vectors(wordlist, size, model_name):\n",
    "    '''\n",
    "    Calculate the mean word embedding for every sentence in the dataset.\n",
    "        \n",
    "    Parameters:\n",
    "    \n",
    "    wordlist: list of the list of words in each sentence\n",
    "    \n",
    "    size: size of hidden layer\n",
    "    \n",
    "    model_name: name of wv2 model\n",
    "    \n",
    "    '''\n",
    "\n",
    "    sumvec = np.zeros(shape = (1,size))\n",
    "    wordcnt = 0\n",
    "    \n",
    "    for word in wordlist:\n",
    "        if word in model_name:\n",
    "            sumvec += model_name[w]\n",
    "            wordcnt +=1\n",
    "    \n",
    "    if wordcnt == 0:\n",
    "        return sumvec\n",
    "    \n",
    "    else:\n",
    "        return sumvec / wordcnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_smote_w2v_model_testcopy(X_train_w2v, y_train, X_val_w2v, y_val, classifier):\n",
    "    '''\n",
    "    Conducts PCA (number of components = 10) and SMOTE to correct target class imbalance before \n",
    "    testing specified classifer with mean embeddings from w2v model. \n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    X_train_w2v: variable containing mean word embedding array for training data\n",
    "    \n",
    "    X_val_w2v: variable containing mean word embedding array for training data\n",
    "\n",
    "    '''    \n",
    "   \n",
    "    smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "    \n",
    "    pca = decomposition.PCA(n_components=10)\n",
    "    \n",
    "    pipe = make_pipeline(pca, smote, classifier) \n",
    "    \n",
    "    model = pipe.fit(X_train_w2v, y_train)\n",
    "    \n",
    "    train_predictions = model.predict(X_train_w2v)\n",
    "    val_predictions = model.predict (X_val_w2v)\n",
    "    \n",
    "   # print scores  \n",
    "    print('Train Accuracy: ' + str(round(metrics.accuracy_score(y_train, train_predictions),2)))\n",
    "    print('Train Precision: ' + str(round(metrics.precision_score(y_train, train_predictions),2)))\n",
    "    print('Train Recall: ' + str(round(metrics.recall_score(y_train, train_predictions),2)))\n",
    "    print('Train F1: ' + str(round(metrics.f1_score(y_train, train_predictions),2)))\n",
    "    print('\\n')\n",
    "    print('Validation Accuracy: ' + str(round(metrics.accuracy_score(y_val, val_predictions),2)))\n",
    "    print('Validation Precision: ' + str(round(metrics.precision_score(y_val, val_predictions),2)))\n",
    "    print('Validation Recall: ' + str(round(metrics.recall_score(y_val, val_predictions),2)))\n",
    "    print('Validation F1: ' + str(round(metrics.f1_score(y_val, val_predictions),2)))\n",
    "\n",
    "    log_confusion_test = pd.crosstab(y_val, val_predictions, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    return log_confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
