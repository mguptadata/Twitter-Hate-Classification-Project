{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import sklearn\n",
    "\n",
    "# NLTK/NLP\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import string, re\n",
    "import urllib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from nltk.collocations import *\n",
    "import gensim\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Classifiers \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Sampling\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import sklearn.decomposition as decomposition\n",
    "\n",
    "#Visualization\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import csv\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.metrics import average_precision_score, auc, roc_curve, precision_recall_curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cleaned-reshuffled.pkl', 'rb') as f:\n",
    "\tdf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>no_hash_tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lem_tweet</th>\n",
       "      <th>stem_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29727</td>\n",
       "      <td>0</td>\n",
       "      <td>sad to see the scenes of hooligans pre #engrus...</td>\n",
       "      <td>sad to see the scenes of hooligans pre #engrus...</td>\n",
       "      <td>sad to see the scenes of hooligans pre engrus ...</td>\n",
       "      <td>[sad, to, see, the, scenes, of, hooligans, pre...</td>\n",
       "      <td>[sad, to, see, the, scene, of, hooligan, pre, ...</td>\n",
       "      <td>[sad, to, see, the, scene, of, hooligan, pre, ...</td>\n",
       "      <td>sad to see the scenes of hooligans pre engrus ...</td>\n",
       "      <td>sad to see the scenes of hooligans pre engrus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14466</td>\n",
       "      <td>0</td>\n",
       "      <td>#gooddyeyoung #yoyoyo  !! super happy to be ap...</td>\n",
       "      <td>#gooddyeyoung #yoyoyo super happy to be apa of...</td>\n",
       "      <td>gooddyeyoung yoyoyo super happy to be apa of t...</td>\n",
       "      <td>[gooddyeyoung, yoyoyo, super, happy, to, be, a...</td>\n",
       "      <td>[gooddyeyoung, yoyoyo, super, happi, to, be, a...</td>\n",
       "      <td>[gooddyeyoung, yoyoyo, super, happy, to, be, a...</td>\n",
       "      <td>gooddyeyoung yoyoyo super happy to be apa of t...</td>\n",
       "      <td>gooddyeyoung yoyoyo super happy to be apa of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18194</td>\n",
       "      <td>0</td>\n",
       "      <td>queen evil's bihdayð#lnic #lnicjustanevilbd...</td>\n",
       "      <td>queen evil s bihday #lnic #lnicjustanevilbday ...</td>\n",
       "      <td>queen evil s bihday lnic lnicjustanevilbday bi...</td>\n",
       "      <td>[queen, evil, s, bihday, lnic, lnicjustanevilb...</td>\n",
       "      <td>[queen, evil, s, bihday, lnic, lnicjustanevilb...</td>\n",
       "      <td>[queen, evil, s, bihday, lnic, lnicjustanevilb...</td>\n",
       "      <td>queen evil s bihday lnic lnicjustanevilbday bi...</td>\n",
       "      <td>queen evil s bihday lnic lnicjustanevilbday bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18283</td>\n",
       "      <td>1</td>\n",
       "      <td>@user you might be a libtard if... #libtard  #...</td>\n",
       "      <td>you might be a libtard if #libtard #sjw #liber...</td>\n",
       "      <td>you might be a libtard if libtard sjw liberal ...</td>\n",
       "      <td>[you, might, be, a, libtard, if, libtard, sjw,...</td>\n",
       "      <td>[you, might, be, a, libtard, if, libtard, sjw,...</td>\n",
       "      <td>[you, might, be, a, libtard, if, libtard, sjw,...</td>\n",
       "      <td>you might be a libtard if libtard sjw liberal ...</td>\n",
       "      <td>you might be a libtard if libtard sjw liberal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25845</td>\n",
       "      <td>0</td>\n",
       "      <td>what are your goals? find out here...   #smile...</td>\n",
       "      <td>what are your goals find out here #smile</td>\n",
       "      <td>what are your goals find out here smile</td>\n",
       "      <td>[what, are, your, goals, find, out, here, smile]</td>\n",
       "      <td>[what, are, your, goal, find, out, here, smile]</td>\n",
       "      <td>[what, are, your, goal, find, out, here, smile]</td>\n",
       "      <td>what are your goals find out here smile</td>\n",
       "      <td>what are your goals find out here smil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label                                              tweet  \\\n",
       "0  29727      0  sad to see the scenes of hooligans pre #engrus...   \n",
       "1  14466      0  #gooddyeyoung #yoyoyo  !! super happy to be ap...   \n",
       "2  18194      0  queen evil's bihdayð#lnic #lnicjustanevilbd...   \n",
       "3  18283      1  @user you might be a libtard if... #libtard  #...   \n",
       "4  25845      0  what are your goals? find out here...   #smile...   \n",
       "\n",
       "                                          tidy_tweet  \\\n",
       "0  sad to see the scenes of hooligans pre #engrus...   \n",
       "1  #gooddyeyoung #yoyoyo super happy to be apa of...   \n",
       "2  queen evil s bihday #lnic #lnicjustanevilbday ...   \n",
       "3  you might be a libtard if #libtard #sjw #liber...   \n",
       "4           what are your goals find out here #smile   \n",
       "\n",
       "                                       no_hash_tweet  \\\n",
       "0  sad to see the scenes of hooligans pre engrus ...   \n",
       "1  gooddyeyoung yoyoyo super happy to be apa of t...   \n",
       "2  queen evil s bihday lnic lnicjustanevilbday bi...   \n",
       "3  you might be a libtard if libtard sjw liberal ...   \n",
       "4            what are your goals find out here smile   \n",
       "\n",
       "                                     tokenized_tweet  \\\n",
       "0  [sad, to, see, the, scenes, of, hooligans, pre...   \n",
       "1  [gooddyeyoung, yoyoyo, super, happy, to, be, a...   \n",
       "2  [queen, evil, s, bihday, lnic, lnicjustanevilb...   \n",
       "3  [you, might, be, a, libtard, if, libtard, sjw,...   \n",
       "4   [what, are, your, goals, find, out, here, smile]   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  [sad, to, see, the, scene, of, hooligan, pre, ...   \n",
       "1  [gooddyeyoung, yoyoyo, super, happi, to, be, a...   \n",
       "2  [queen, evil, s, bihday, lnic, lnicjustanevilb...   \n",
       "3  [you, might, be, a, libtard, if, libtard, sjw,...   \n",
       "4    [what, are, your, goal, find, out, here, smile]   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  [sad, to, see, the, scene, of, hooligan, pre, ...   \n",
       "1  [gooddyeyoung, yoyoyo, super, happy, to, be, a...   \n",
       "2  [queen, evil, s, bihday, lnic, lnicjustanevilb...   \n",
       "3  [you, might, be, a, libtard, if, libtard, sjw,...   \n",
       "4    [what, are, your, goal, find, out, here, smile]   \n",
       "\n",
       "                                           lem_tweet  \\\n",
       "0  sad to see the scenes of hooligans pre engrus ...   \n",
       "1  gooddyeyoung yoyoyo super happy to be apa of t...   \n",
       "2  queen evil s bihday lnic lnicjustanevilbday bi...   \n",
       "3  you might be a libtard if libtard sjw liberal ...   \n",
       "4            what are your goals find out here smile   \n",
       "\n",
       "                                          stem_tweet  \n",
       "0  sad to see the scenes of hooligans pre engrus ...  \n",
       "1  gooddyeyoung yoyoyo super happy to be apa of t...  \n",
       "2  queen evil s bihday lnic lnicjustanevilbday bi...  \n",
       "3  you might be a libtard if libtard sjw liberal ...  \n",
       "4             what are your goals find out here smil  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Val / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into train and test \n",
    "X_model, X_test, y_model, y_test = train_test_split(X, y, stratify = y,  test_size=0.20, random_state=123)\n",
    "\n",
    "#splitting \"model\" into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_model, y_model, test_size=0.20, random_state=123)\n",
    "\n",
    "# df_train_full = X_train.copy()\n",
    "# df_train_full['label']= y_train\n",
    "# train_full_df.to_csv('train_full_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.929854\n",
       "1    0.070146\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling and Downsampling Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>no_hash_tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lem_tweet</th>\n",
       "      <th>stem_tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20676</td>\n",
       "      <td>@user f*** this  ð¦ðº government that deli...</td>\n",
       "      <td>f this government that deliberately toures #re...</td>\n",
       "      <td>f this government that deliberately toures ref...</td>\n",
       "      <td>[f, this, government, that, deliberately, tour...</td>\n",
       "      <td>[f, this, govern, that, deliber, tour, refuge,...</td>\n",
       "      <td>[f, this, government, that, deliberately, tour...</td>\n",
       "      <td>f this government that deliberately toures ref...</td>\n",
       "      <td>f this government that deliberately toures ref...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21531</th>\n",
       "      <td>24025</td>\n",
       "      <td>despite a demoralizing 2016: may ur #newyear20...</td>\n",
       "      <td>despite a demoralizing may ur #newyear be #cla...</td>\n",
       "      <td>despite a demoralizing may ur newyear be class...</td>\n",
       "      <td>[despite, a, demoralizing, may, ur, newyear, b...</td>\n",
       "      <td>[despit, a, demor, may, ur, newyear, be, class...</td>\n",
       "      <td>[despite, a, demoralizing, may, ur, newyear, b...</td>\n",
       "      <td>despite a demoralizing may ur newyear be class...</td>\n",
       "      <td>despite a demoralizing may ur newyear be class...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13300</th>\n",
       "      <td>18145</td>\n",
       "      <td>@user #koreans &amp;amp; joseon people in japan, w...</td>\n",
       "      <td>#koreans amp joseon people in japan will abuse...</td>\n",
       "      <td>koreans amp joseon people in japan will abuse ...</td>\n",
       "      <td>[koreans, amp, joseon, people, in, japan, will...</td>\n",
       "      <td>[korean, amp, joseon, peopl, in, japan, will, ...</td>\n",
       "      <td>[korean, amp, joseon, people, in, japan, will,...</td>\n",
       "      <td>koreans amp joseon people in japan will abuse ...</td>\n",
       "      <td>koreans amp joseon people in japan will abuse ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18925</th>\n",
       "      <td>8506</td>\n",
       "      <td>@user @user @user @user classic ! yet you jewi...</td>\n",
       "      <td>classic yet you jewish bastards wonder why you...</td>\n",
       "      <td>classic yet you jewish bastards wonder why you...</td>\n",
       "      <td>[classic, yet, you, jewish, bastards, wonder, ...</td>\n",
       "      <td>[classic, yet, you, jewish, bastard, wonder, w...</td>\n",
       "      <td>[classic, yet, you, jewish, bastard, wonder, w...</td>\n",
       "      <td>classic yet you jewish bastards wonder why you...</td>\n",
       "      <td>classic yet you jewish bastards wonder why you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12619</th>\n",
       "      <td>15464</td>\n",
       "      <td>@user did someone say #antisemetic ? gee (((@u...</td>\n",
       "      <td>did someone say #antisemetic gee you re a bit ...</td>\n",
       "      <td>did someone say antisemetic gee you re a bit t...</td>\n",
       "      <td>[did, someone, say, antisemetic, gee, you, re,...</td>\n",
       "      <td>[did, someon, say, antisemet, gee, you, re, a,...</td>\n",
       "      <td>[did, someone, say, antisemetic, gee, you, re,...</td>\n",
       "      <td>did someone say antisemetic gee you re a bit t...</td>\n",
       "      <td>did someone say antisemetic gee you re a bit t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26964</th>\n",
       "      <td>28937</td>\n",
       "      <td>couldn't have said this any better nor truthfu...</td>\n",
       "      <td>couldn t have said this any better nor truthfu...</td>\n",
       "      <td>couldn t have said this any better nor truthfu...</td>\n",
       "      <td>[couldn, t, have, said, this, any, better, nor...</td>\n",
       "      <td>[couldn, t, have, said, this, ani, better, nor...</td>\n",
       "      <td>[couldn, t, have, said, this, any, better, nor...</td>\n",
       "      <td>couldn t have said this any better nor truthfu...</td>\n",
       "      <td>couldn t have said this any better nor truthfu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17273</th>\n",
       "      <td>25291</td>\n",
       "      <td>@user racism stuffed into skinny jeans with a ...</td>\n",
       "      <td>racism stuffed into skinny jeans with a hipste...</td>\n",
       "      <td>racism stuffed into skinny jeans with a hipste...</td>\n",
       "      <td>[racism, stuffed, into, skinny, jeans, with, a...</td>\n",
       "      <td>[racism, stuf, into, skinni, jean, with, a, hi...</td>\n",
       "      <td>[racism, stuffed, into, skinny, jean, with, a,...</td>\n",
       "      <td>racism stuffed into skinny jeans with a hipste...</td>\n",
       "      <td>racism stuffed into skinny jeans with a hipste...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>12717</td>\n",
       "      <td>the end of   #me #selfie # #love #messi #cr7 #...</td>\n",
       "      <td>the end of #me #selfie # #love #messi #cr #rel...</td>\n",
       "      <td>the end of me selfie  love messi cr religion c...</td>\n",
       "      <td>[the, end, of, me, selfie, love, messi, cr, re...</td>\n",
       "      <td>[the, end, of, me, selfi, love, messi, cr, rel...</td>\n",
       "      <td>[the, end, of, me, selfie, love, messi, cr, re...</td>\n",
       "      <td>the end of me selfie  love messi cr religion c...</td>\n",
       "      <td>the end of me selfie  love messi cr religion c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17875</th>\n",
       "      <td>11612</td>\n",
       "      <td>trump ally wishes mad cow disease death for ob...</td>\n",
       "      <td>trump ally wishes mad cow disease death for ob...</td>\n",
       "      <td>trump ally wishes mad cow disease death for ob...</td>\n",
       "      <td>[trump, ally, wishes, mad, cow, disease, death...</td>\n",
       "      <td>[trump, alli, wish, mad, cow, diseas, death, f...</td>\n",
       "      <td>[trump, ally, wish, mad, cow, disease, death, ...</td>\n",
       "      <td>trump ally wishes mad cow disease death for ob...</td>\n",
       "      <td>trump ally wishes mad cow disease death for ob...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17184</th>\n",
       "      <td>20554</td>\n",
       "      <td>opinion:  is rife in the #lgbt community. #gay...</td>\n",
       "      <td>opinion is rife in the #lgbt community #gay pe...</td>\n",
       "      <td>opinion is rife in the lgbt community gay peop...</td>\n",
       "      <td>[opinion, is, rife, in, the, lgbt, community, ...</td>\n",
       "      <td>[opinion, is, rife, in, the, lgbt, communiti, ...</td>\n",
       "      <td>[opinion, is, rife, in, the, lgbt, community, ...</td>\n",
       "      <td>opinion is rife in the lgbt community gay peop...</td>\n",
       "      <td>opinion is rife in the lgbt community gay peop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8573</th>\n",
       "      <td>25151</td>\n",
       "      <td>@user #allahsoil the cold war was fought over ...</td>\n",
       "      <td>#allahsoil the cold war was fought over oil #t...</td>\n",
       "      <td>allahsoil the cold war was fought over oil tea...</td>\n",
       "      <td>[allahsoil, the, cold, war, was, fought, over,...</td>\n",
       "      <td>[allahsoil, the, cold, war, was, fought, over,...</td>\n",
       "      <td>[allahsoil, the, cold, war, wa, fought, over, ...</td>\n",
       "      <td>allahsoil the cold war was fought over oil tea...</td>\n",
       "      <td>allahsoil the cold war was fought over oil tea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26779</th>\n",
       "      <td>13585</td>\n",
       "      <td>omg, these trump suppoers are deplorable! #dum...</td>\n",
       "      <td>omg these trump suppoers are deplorable #dumpt...</td>\n",
       "      <td>omg these trump suppoers are deplorable dumptr...</td>\n",
       "      <td>[omg, these, trump, suppoers, are, deplorable,...</td>\n",
       "      <td>[omg, these, trump, suppoer, are, deplor, dump...</td>\n",
       "      <td>[omg, these, trump, suppoers, are, deplorable,...</td>\n",
       "      <td>omg these trump suppoers are deplorable dumptr...</td>\n",
       "      <td>omg these trump suppoers are deplorable dumptr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15332</th>\n",
       "      <td>2581</td>\n",
       "      <td>sea shepherd suppoers are racist!   #antiracis...</td>\n",
       "      <td>sea shepherd suppoers are racist #antiracism #...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>[sea, shepherd, suppoers, are, racist, antirac...</td>\n",
       "      <td>[sea, shepherd, suppoer, are, racist, antirac,...</td>\n",
       "      <td>[sea, shepherd, suppoers, are, racist, antirac...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12290</th>\n",
       "      <td>6519</td>\n",
       "      <td>.@user @user while @user can use phrases like ...</td>\n",
       "      <td>while can use phrases like #sandniggers is acc...</td>\n",
       "      <td>while can use phrases like sandniggers is acce...</td>\n",
       "      <td>[while, can, use, phrases, like, sandniggers, ...</td>\n",
       "      <td>[while, can, use, phrase, like, sandnigg, is, ...</td>\n",
       "      <td>[while, can, use, phrase, like, sandniggers, i...</td>\n",
       "      <td>while can use phrases like sandniggers is acce...</td>\n",
       "      <td>while can use phrases like sandniggers is acce...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24159</th>\n",
       "      <td>9563</td>\n",
       "      <td>@user #allahsoil enlightenment is wasted on th...</td>\n",
       "      <td>#allahsoil enlightenment is wasted on the wilf...</td>\n",
       "      <td>allahsoil enlightenment is wasted on the wilfu...</td>\n",
       "      <td>[allahsoil, enlightenment, is, wasted, on, the...</td>\n",
       "      <td>[allahsoil, enlighten, is, wast, on, the, wil,...</td>\n",
       "      <td>[allahsoil, enlightenment, is, wasted, on, the...</td>\n",
       "      <td>allahsoil enlightenment is wasted on the wilfu...</td>\n",
       "      <td>allahsoil enlightenment is wasted on the wilfu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25895</th>\n",
       "      <td>8452</td>\n",
       "      <td>@user here comes a  #supermistict douchebag wh...</td>\n",
       "      <td>here comes a #supermistict douchebag who can o...</td>\n",
       "      <td>here comes a supermistict douchebag who can on...</td>\n",
       "      <td>[here, comes, a, supermistict, douchebag, who,...</td>\n",
       "      <td>[here, come, a, supermistict, douchebag, who, ...</td>\n",
       "      <td>[here, come, a, supermistict, douchebag, who, ...</td>\n",
       "      <td>here comes a supermistict douchebag who can on...</td>\n",
       "      <td>here comes a supermistict douchebag who can on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23934</th>\n",
       "      <td>17750</td>\n",
       "      <td>@user hidden  in #america is as rampant as bla...</td>\n",
       "      <td>hidden in #america is as rampant as blatant ra...</td>\n",
       "      <td>hidden in america is as rampant as blatant racism</td>\n",
       "      <td>[hidden, in, america, is, as, rampant, as, bla...</td>\n",
       "      <td>[hidden, in, america, is, as, rampant, as, bla...</td>\n",
       "      <td>[hidden, in, america, is, a, rampant, a, blata...</td>\n",
       "      <td>hidden in america is as rampant as blatant racism</td>\n",
       "      <td>hidden in america is as rampant as blatant rac</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29174</th>\n",
       "      <td>21389</td>\n",
       "      <td>will the alt-right promote a new kind of  gene...</td>\n",
       "      <td>will the alt right promote a new kind of genet...</td>\n",
       "      <td>will the alt right promote a new kind of genet...</td>\n",
       "      <td>[will, the, alt, right, promote, a, new, kind,...</td>\n",
       "      <td>[will, the, alt, right, promot, a, new, kind, ...</td>\n",
       "      <td>[will, the, alt, right, promote, a, new, kind,...</td>\n",
       "      <td>will the alt right promote a new kind of genet...</td>\n",
       "      <td>will the alt right promote a new kind of genet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10474</th>\n",
       "      <td>23431</td>\n",
       "      <td>#us why weneed #empathy #ageoftrump #grassroot...</td>\n",
       "      <td>#us why weneed #empathy #ageoftrump #grassroot...</td>\n",
       "      <td>us why weneed empathy ageoftrump grassrootsact...</td>\n",
       "      <td>[us, why, weneed, empathy, ageoftrump, grassro...</td>\n",
       "      <td>[us, whi, wene, empathi, ageoftrump, grassroot...</td>\n",
       "      <td>[u, why, weneed, empathy, ageoftrump, grassroo...</td>\n",
       "      <td>us why weneed empathy ageoftrump grassrootsact...</td>\n",
       "      <td>us why weneed empathy ageoftrump grassrootsact...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30743</th>\n",
       "      <td>12301</td>\n",
       "      <td>black trump suppoer smacks down cnn repoer for...</td>\n",
       "      <td>black trump suppoer smacks down cnn repoer for...</td>\n",
       "      <td>black trump suppoer smacks down cnn repoer for...</td>\n",
       "      <td>[black, trump, suppoer, smacks, down, cnn, rep...</td>\n",
       "      <td>[black, trump, suppoer, smack, down, cnn, repo...</td>\n",
       "      <td>[black, trump, suppoer, smack, down, cnn, repo...</td>\n",
       "      <td>black trump suppoer smacks down cnn repoer for...</td>\n",
       "      <td>black trump suppoer smacks down cnn repoer for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6190</th>\n",
       "      <td>19407</td>\n",
       "      <td>so sick of all the pre-programmed #hillbots rh...</td>\n",
       "      <td>so sick of all the pre programmed #hillbots rh...</td>\n",
       "      <td>so sick of all the pre programmed hillbots rhe...</td>\n",
       "      <td>[so, sick, of, all, the, pre, programmed, hill...</td>\n",
       "      <td>[so, sick, of, all, the, pre, program, hillbot...</td>\n",
       "      <td>[so, sick, of, all, the, pre, programmed, hill...</td>\n",
       "      <td>so sick of all the pre programmed hillbots rhe...</td>\n",
       "      <td>so sick of all the pre programmed hillbots rhe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19746</th>\n",
       "      <td>406</td>\n",
       "      <td>@user when you're blocked by a  troll because ...</td>\n",
       "      <td>when you re blocked by a troll because you pro...</td>\n",
       "      <td>when you re blocked by a troll because you pro...</td>\n",
       "      <td>[when, you, re, blocked, by, a, troll, because...</td>\n",
       "      <td>[when, you, re, block, by, a, troll, becaus, y...</td>\n",
       "      <td>[when, you, re, blocked, by, a, troll, because...</td>\n",
       "      <td>when you re blocked by a troll because you pro...</td>\n",
       "      <td>when you re blocked by a troll because you pro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19581</th>\n",
       "      <td>11720</td>\n",
       "      <td>@user @user you forgot #democrate, because vet...</td>\n",
       "      <td>you forgot #democrate because vetting people f...</td>\n",
       "      <td>you forgot democrate because vetting people fr...</td>\n",
       "      <td>[you, forgot, democrate, because, vetting, peo...</td>\n",
       "      <td>[you, forgot, democr, becaus, vet, peopl, from...</td>\n",
       "      <td>[you, forgot, democrate, because, vetting, peo...</td>\n",
       "      <td>you forgot democrate because vetting people fr...</td>\n",
       "      <td>you forgot democrate because vetting people fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31767</th>\n",
       "      <td>28476</td>\n",
       "      <td>sea shepherd suppoers are racist!   #antiracis...</td>\n",
       "      <td>sea shepherd suppoers are racist #antiracism #...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>[sea, shepherd, suppoers, are, racist, antirac...</td>\n",
       "      <td>[sea, shepherd, suppoer, are, racist, antirac,...</td>\n",
       "      <td>[sea, shepherd, suppoers, are, racist, antirac...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25210</th>\n",
       "      <td>24768</td>\n",
       "      <td>porn vids web free sex</td>\n",
       "      <td>porn vids web free sex</td>\n",
       "      <td>porn vids web free sex</td>\n",
       "      <td>[porn, vids, web, free, sex]</td>\n",
       "      <td>[porn, vid, web, free, sex]</td>\n",
       "      <td>[porn, vids, web, free, sex]</td>\n",
       "      <td>porn vids web free sex</td>\n",
       "      <td>porn vids web free sex</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19800</th>\n",
       "      <td>19745</td>\n",
       "      <td>@user &amp;amp; the  #democraticpay keeps telling ...</td>\n",
       "      <td>amp the #democraticpay keeps telling me that #...</td>\n",
       "      <td>amp the democraticpay keeps telling me that bl...</td>\n",
       "      <td>[amp, the, democraticpay, keeps, telling, me, ...</td>\n",
       "      <td>[amp, the, democraticpay, keep, tell, me, that...</td>\n",
       "      <td>[amp, the, democraticpay, keep, telling, me, t...</td>\n",
       "      <td>amp the democraticpay keeps telling me that bl...</td>\n",
       "      <td>amp the democraticpay keeps telling me that bl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7812</th>\n",
       "      <td>1242</td>\n",
       "      <td>@user although, i am not a , a #bigot or a #mi...</td>\n",
       "      <td>although i am not a a #bigot or a #misogynist ...</td>\n",
       "      <td>although i am not a a bigot or a misogynist so...</td>\n",
       "      <td>[although, i, am, not, a, a, bigot, or, a, mis...</td>\n",
       "      <td>[although, i, am, not, a, a, bigot, or, a, mis...</td>\n",
       "      <td>[although, i, am, not, a, a, bigot, or, a, mis...</td>\n",
       "      <td>although i am not a a bigot or a misogynist so...</td>\n",
       "      <td>although i am not a a bigot or a misogynist so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5074</th>\n",
       "      <td>23280</td>\n",
       "      <td>this is sooooo  or may be just funny</td>\n",
       "      <td>this is sooooo or may be just funny</td>\n",
       "      <td>this is sooooo or may be just funny</td>\n",
       "      <td>[this, is, sooooo, or, may, be, just, funny]</td>\n",
       "      <td>[this, is, sooooo, or, may, be, just, funni]</td>\n",
       "      <td>[this, is, sooooo, or, may, be, just, funny]</td>\n",
       "      <td>this is sooooo or may be just funny</td>\n",
       "      <td>this is sooooo or may be just funni</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6317</th>\n",
       "      <td>15050</td>\n",
       "      <td>@user âwe the peopleâ originally meant â...</td>\n",
       "      <td>we the people originally meant we the white la...</td>\n",
       "      <td>we the people originally meant we the white la...</td>\n",
       "      <td>[we, the, people, originally, meant, we, the, ...</td>\n",
       "      <td>[we, the, peopl, origin, meant, we, the, white...</td>\n",
       "      <td>[we, the, people, originally, meant, we, the, ...</td>\n",
       "      <td>we the people originally meant we the white la...</td>\n",
       "      <td>we the people originally meant we the white la...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5988</th>\n",
       "      <td>24766</td>\n",
       "      <td>i've no problem with #universities that teach ...</td>\n",
       "      <td>i ve no problem with #universities that teach ...</td>\n",
       "      <td>i ve no problem with universities that teach a...</td>\n",
       "      <td>[i, ve, no, problem, with, universities, that,...</td>\n",
       "      <td>[i, ve, no, problem, with, univers, that, teac...</td>\n",
       "      <td>[i, ve, no, problem, with, university, that, t...</td>\n",
       "      <td>i ve no problem with universities that teach a...</td>\n",
       "      <td>i ve no problem with universities that teach a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3721</th>\n",
       "      <td>17405</td>\n",
       "      <td>check out this new trending #funny #gif !  , p...</td>\n",
       "      <td>check out this new trending #funny #gif pixel ...</td>\n",
       "      <td>check out this new trending funny gif pixel ce...</td>\n",
       "      <td>[check, out, this, new, trending, funny, gif, ...</td>\n",
       "      <td>[check, out, this, new, trend, funni, gif, pix...</td>\n",
       "      <td>[check, out, this, new, trending, funny, gif, ...</td>\n",
       "      <td>check out this new trending funny gif pixel ce...</td>\n",
       "      <td>check out this new trending funny gif pixel ce...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28057</th>\n",
       "      <td>31774</td>\n",
       "      <td>this reminds me of this. i am   love these two...</td>\n",
       "      <td>this reminds me of this i am love these two th...</td>\n",
       "      <td>this reminds me of this i am love these two th...</td>\n",
       "      <td>[this, reminds, me, of, this, i, am, love, the...</td>\n",
       "      <td>[this, remind, me, of, this, i, am, love, thes...</td>\n",
       "      <td>[this, reminds, me, of, this, i, am, love, the...</td>\n",
       "      <td>this reminds me of this i am love these two th...</td>\n",
       "      <td>this reminds me of this i am love these two th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15044</th>\n",
       "      <td>5789</td>\n",
       "      <td>livelypics: just when you think you know peop...</td>\n",
       "      <td>livelypics just when you think you know people...</td>\n",
       "      <td>livelypics just when you think you know people...</td>\n",
       "      <td>[livelypics, just, when, you, think, you, know...</td>\n",
       "      <td>[livelyp, just, when, you, think, you, know, p...</td>\n",
       "      <td>[livelypics, just, when, you, think, you, know...</td>\n",
       "      <td>livelypics just when you think you know people...</td>\n",
       "      <td>livelypics just when you think you know people...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23222</th>\n",
       "      <td>29934</td>\n",
       "      <td>trying not to shut down but maybe #pokemon wil...</td>\n",
       "      <td>trying not to shut down but maybe #pokemon wil...</td>\n",
       "      <td>trying not to shut down but maybe pokemon will...</td>\n",
       "      <td>[trying, not, to, shut, down, but, maybe, poke...</td>\n",
       "      <td>[tri, not, to, shut, down, but, mayb, pokemon,...</td>\n",
       "      <td>[trying, not, to, shut, down, but, maybe, poke...</td>\n",
       "      <td>trying not to shut down but maybe pokemon will...</td>\n",
       "      <td>trying not to shut down but maybe pokemon will...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27273</th>\n",
       "      <td>395</td>\n",
       "      <td>#first #bihday to our #puppy #eloise #sweetbab...</td>\n",
       "      <td>#first #bihday to our #puppy #eloise #sweetbab...</td>\n",
       "      <td>first bihday to our puppy eloise sweetbabins d...</td>\n",
       "      <td>[first, bihday, to, our, puppy, eloise, sweetb...</td>\n",
       "      <td>[first, bihday, to, our, puppi, elois, sweetba...</td>\n",
       "      <td>[first, bihday, to, our, puppy, eloise, sweetb...</td>\n",
       "      <td>first bihday to our puppy eloise sweetbabins d...</td>\n",
       "      <td>first bihday to our puppy eloise sweetbabins d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30304</th>\n",
       "      <td>10458</td>\n",
       "      <td>@user it was in ceain areas yeah, you not seen...</td>\n",
       "      <td>it was in ceain areas yeah you not seen the vi...</td>\n",
       "      <td>it was in ceain areas yeah you not seen the vi...</td>\n",
       "      <td>[it, was, in, ceain, areas, yeah, you, not, se...</td>\n",
       "      <td>[it, was, in, ceain, area, yeah, you, not, see...</td>\n",
       "      <td>[it, wa, in, ceain, area, yeah, you, not, seen...</td>\n",
       "      <td>it was in ceain areas yeah you not seen the vi...</td>\n",
       "      <td>it was in ceain areas yeah you not seen the vi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3928</th>\n",
       "      <td>19081</td>\n",
       "      <td>so many shit talkers/bullies in the youtube co...</td>\n",
       "      <td>so many shit talkers bullies in the youtube co...</td>\n",
       "      <td>so many shit talkers bullies in the youtube co...</td>\n",
       "      <td>[so, many, shit, talkers, bullies, in, the, yo...</td>\n",
       "      <td>[so, mani, shit, talker, bulli, in, the, youtu...</td>\n",
       "      <td>[so, many, shit, talker, bully, in, the, youtu...</td>\n",
       "      <td>so many shit talkers bullies in the youtube co...</td>\n",
       "      <td>so many shit talkers bullies in the youtube co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>13429</td>\n",
       "      <td>all ready to pay xx #saturday #daughter #love ...</td>\n",
       "      <td>all ready to pay xx #saturday #daughter #love ...</td>\n",
       "      <td>all ready to pay xx saturday daughter love pay...</td>\n",
       "      <td>[all, ready, to, pay, xx, saturday, daughter, ...</td>\n",
       "      <td>[all, readi, to, pay, xx, saturday, daughter, ...</td>\n",
       "      <td>[all, ready, to, pay, xx, saturday, daughter, ...</td>\n",
       "      <td>all ready to pay xx saturday daughter love pay...</td>\n",
       "      <td>all ready to pay xx saturday daughter love pay...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9826</th>\n",
       "      <td>6122</td>\n",
       "      <td>feeling a little #mole tonight! ó¾« #food #fo...</td>\n",
       "      <td>feeling a little #mole tonight #food #foodblog...</td>\n",
       "      <td>feeling a little mole tonight food foodblogger...</td>\n",
       "      <td>[feeling, a, little, mole, tonight, food, food...</td>\n",
       "      <td>[feel, a, littl, mole, tonight, food, foodblog...</td>\n",
       "      <td>[feeling, a, little, mole, tonight, food, food...</td>\n",
       "      <td>feeling a little mole tonight food foodblogger...</td>\n",
       "      <td>feeling a little mole tonight food foodblogger...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11548</th>\n",
       "      <td>14426</td>\n",
       "      <td>while we're still trying to get over the shock...</td>\n",
       "      <td>while we re still trying to get over the shock...</td>\n",
       "      <td>while we re still trying to get over the shock...</td>\n",
       "      <td>[while, we, re, still, trying, to, get, over, ...</td>\n",
       "      <td>[while, we, re, still, tri, to, get, over, the...</td>\n",
       "      <td>[while, we, re, still, trying, to, get, over, ...</td>\n",
       "      <td>while we re still trying to get over the shock...</td>\n",
       "      <td>while we re still trying to get over the shock...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19491</th>\n",
       "      <td>19469</td>\n",
       "      <td>fantasy aisle ð #i #fantasy #aisle #summer ...</td>\n",
       "      <td>fantasy aisle #i #fantasy #aisle #summer #gran...</td>\n",
       "      <td>fantasy aisle i fantasy aisle summer grand vac...</td>\n",
       "      <td>[fantasy, aisle, i, fantasy, aisle, summer, gr...</td>\n",
       "      <td>[fantasi, aisl, i, fantasi, aisl, summer, gran...</td>\n",
       "      <td>[fantasy, aisle, i, fantasy, aisle, summer, gr...</td>\n",
       "      <td>fantasy aisle i fantasy aisle summer grand vac...</td>\n",
       "      <td>fantasy aisle i fantasy aisle summer grand vac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31522</th>\n",
       "      <td>17794</td>\n",
       "      <td>have a wonderful f r i d a y ð¸  #love #emik...</td>\n",
       "      <td>have a wonderful f r i d a y #love #emikagifts...</td>\n",
       "      <td>have a wonderful f r i d a y love emikagifts j...</td>\n",
       "      <td>[have, a, wonderful, f, r, i, d, a, y, love, e...</td>\n",
       "      <td>[have, a, wonder, f, r, i, d, a, y, love, emik...</td>\n",
       "      <td>[have, a, wonderful, f, r, i, d, a, y, love, e...</td>\n",
       "      <td>have a wonderful f r i d a y love emikagifts j...</td>\n",
       "      <td>have a wonderful f r i d a y love emikagifts j...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>11550</td>\n",
       "      <td>who wants him to be the next commander in chie...</td>\n",
       "      <td>who wants him to be the next commander in chie...</td>\n",
       "      <td>who wants him to be the next commander in chie...</td>\n",
       "      <td>[who, wants, him, to, be, the, next, commander...</td>\n",
       "      <td>[who, want, him, to, be, the, next, command, i...</td>\n",
       "      <td>[who, want, him, to, be, the, next, commander,...</td>\n",
       "      <td>who wants him to be the next commander in chie...</td>\n",
       "      <td>who wants him to be the next commander in chie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21399</th>\n",
       "      <td>18137</td>\n",
       "      <td>we beat that cock.   #beatit #penis</td>\n",
       "      <td>we beat that cock #beatit #penis</td>\n",
       "      <td>we beat that cock beatit penis</td>\n",
       "      <td>[we, beat, that, cock, beatit, penis]</td>\n",
       "      <td>[we, beat, that, cock, beatit, peni]</td>\n",
       "      <td>[we, beat, that, cock, beatit, penis]</td>\n",
       "      <td>we beat that cock beatit penis</td>\n",
       "      <td>we beat that cock beatit peni</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16625</th>\n",
       "      <td>14918</td>\n",
       "      <td>lik if u cri everi tim</td>\n",
       "      <td>lik if u cri everi tim</td>\n",
       "      <td>lik if u cri everi tim</td>\n",
       "      <td>[lik, if, u, cri, everi, tim]</td>\n",
       "      <td>[lik, if, u, cri, everi, tim]</td>\n",
       "      <td>[lik, if, u, cri, everi, tim]</td>\n",
       "      <td>lik if u cri everi tim</td>\n",
       "      <td>lik if u cri everi tim</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7084</th>\n",
       "      <td>12925</td>\n",
       "      <td>speakers dinner on eve of #ricsrural conferenc...</td>\n",
       "      <td>speakers dinner on eve of #ricsrural conferenc...</td>\n",
       "      <td>speakers dinner on eve of ricsrural conference...</td>\n",
       "      <td>[speakers, dinner, on, eve, of, ricsrural, con...</td>\n",
       "      <td>[speaker, dinner, on, eve, of, ricsrur, confer...</td>\n",
       "      <td>[speaker, dinner, on, eve, of, ricsrural, conf...</td>\n",
       "      <td>speakers dinner on eve of ricsrural conference...</td>\n",
       "      <td>speakers dinner on eve of ricsrural conference...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21332</th>\n",
       "      <td>1640</td>\n",
       "      <td>best facetime today; i can't wait to see my bo...</td>\n",
       "      <td>best facetime today i can t wait to see my boy...</td>\n",
       "      <td>best facetime today i can t wait to see my boy...</td>\n",
       "      <td>[best, facetime, today, i, can, t, wait, to, s...</td>\n",
       "      <td>[best, facetim, today, i, can, t, wait, to, se...</td>\n",
       "      <td>[best, facetime, today, i, can, t, wait, to, s...</td>\n",
       "      <td>best facetime today i can t wait to see my boy...</td>\n",
       "      <td>best facetime today i can t wait to see my boy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28564</th>\n",
       "      <td>18855</td>\n",
       "      <td>archery for year 5 at 1.30pm!   #archery</td>\n",
       "      <td>archery for year at pm #archery</td>\n",
       "      <td>archery for year at pm archery</td>\n",
       "      <td>[archery, for, year, at, pm, archery]</td>\n",
       "      <td>[archeri, for, year, at, pm, archeri]</td>\n",
       "      <td>[archery, for, year, at, pm, archery]</td>\n",
       "      <td>archery for year at pm archery</td>\n",
       "      <td>archery for year at pm archeri</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12504</th>\n",
       "      <td>23278</td>\n",
       "      <td>#whoolo in film you can have sad endings.  #an...</td>\n",
       "      <td>#whoolo in film you can have sad endings #anna...</td>\n",
       "      <td>whoolo in film you can have sad endings anna torv</td>\n",
       "      <td>[whoolo, in, film, you, can, have, sad, ending...</td>\n",
       "      <td>[whoolo, in, film, you, can, have, sad, end, a...</td>\n",
       "      <td>[whoolo, in, film, you, can, have, sad, ending...</td>\n",
       "      <td>whoolo in film you can have sad endings anna torv</td>\n",
       "      <td>whoolo in film you can have sad endings anna torv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22295</th>\n",
       "      <td>17561</td>\n",
       "      <td>@user @user at what time will the gates open?</td>\n",
       "      <td>at what time will the gates open</td>\n",
       "      <td>at what time will the gates open</td>\n",
       "      <td>[at, what, time, will, the, gates, open]</td>\n",
       "      <td>[at, what, time, will, the, gate, open]</td>\n",
       "      <td>[at, what, time, will, the, gate, open]</td>\n",
       "      <td>at what time will the gates open</td>\n",
       "      <td>at what time will the gates open</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13326</th>\n",
       "      <td>6135</td>\n",
       "      <td>thank you @user for a new #chargehr after mine...</td>\n",
       "      <td>thank you for a new #chargehr after mine broke...</td>\n",
       "      <td>thank you for a new chargehr after mine broke ...</td>\n",
       "      <td>[thank, you, for, a, new, chargehr, after, min...</td>\n",
       "      <td>[thank, you, for, a, new, chargehr, after, min...</td>\n",
       "      <td>[thank, you, for, a, new, chargehr, after, min...</td>\n",
       "      <td>thank you for a new chargehr after mine broke ...</td>\n",
       "      <td>thank you for a new chargehr after mine broke ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>21952</td>\n",
       "      <td>our thoughts and prayers goes out to everyone ...</td>\n",
       "      <td>our thoughts and prayers goes out to everyone ...</td>\n",
       "      <td>our thoughts and prayers goes out to everyone ...</td>\n",
       "      <td>[our, thoughts, and, prayers, goes, out, to, e...</td>\n",
       "      <td>[our, thought, and, prayer, goe, out, to, ever...</td>\n",
       "      <td>[our, thought, and, prayer, go, out, to, every...</td>\n",
       "      <td>our thoughts and prayers goes out to everyone ...</td>\n",
       "      <td>our thoughts and prayers goes out to everyone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19044</th>\n",
       "      <td>25937</td>\n",
       "      <td>#film   bull up: you will dominate your bull a...</td>\n",
       "      <td>#film bull up you will dominate your bull and ...</td>\n",
       "      <td>film bull up you will dominate your bull and y...</td>\n",
       "      <td>[film, bull, up, you, will, dominate, your, bu...</td>\n",
       "      <td>[film, bull, up, you, will, domin, your, bull,...</td>\n",
       "      <td>[film, bull, up, you, will, dominate, your, bu...</td>\n",
       "      <td>film bull up you will dominate your bull and y...</td>\n",
       "      <td>film bull up you will dominate your bull and y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31112</th>\n",
       "      <td>17229</td>\n",
       "      <td>this is horribly sad news. a fine actor with g...</td>\n",
       "      <td>this is horribly sad news a fine actor with gr...</td>\n",
       "      <td>this is horribly sad news a fine actor with gr...</td>\n",
       "      <td>[this, is, horribly, sad, news, a, fine, actor...</td>\n",
       "      <td>[this, is, horribl, sad, news, a, fine, actor,...</td>\n",
       "      <td>[this, is, horribly, sad, news, a, fine, actor...</td>\n",
       "      <td>this is horribly sad news a fine actor with gr...</td>\n",
       "      <td>this is horribly sad news a fine actor with gr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14188</th>\n",
       "      <td>7012</td>\n",
       "      <td>follow your #hea and be   â¤</td>\n",
       "      <td>follow your #hea and be</td>\n",
       "      <td>follow your hea and be</td>\n",
       "      <td>[follow, your, hea, and, be]</td>\n",
       "      <td>[follow, your, hea, and, be]</td>\n",
       "      <td>[follow, your, hea, and, be]</td>\n",
       "      <td>follow your hea and be</td>\n",
       "      <td>follow your hea and b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>31177</td>\n",
       "      <td>cops giving tickets when they didn't even see ...</td>\n",
       "      <td>cops giving tickets when they didn t even see ...</td>\n",
       "      <td>cops giving tickets when they didn t even see ...</td>\n",
       "      <td>[cops, giving, tickets, when, they, didn, t, e...</td>\n",
       "      <td>[cop, give, ticket, when, they, didn, t, even,...</td>\n",
       "      <td>[cop, giving, ticket, when, they, didn, t, eve...</td>\n",
       "      <td>cops giving tickets when they didn t even see ...</td>\n",
       "      <td>cops giving tickets when they didn t even see ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>23672</td>\n",
       "      <td>singapore city gallery   #riclswtravelbook #be...</td>\n",
       "      <td>singapore city gallery #riclswtravelbook #bear...</td>\n",
       "      <td>singapore city gallery riclswtravelbook bearlo...</td>\n",
       "      <td>[singapore, city, gallery, riclswtravelbook, b...</td>\n",
       "      <td>[singapor, citi, galleri, riclswtravelbook, be...</td>\n",
       "      <td>[singapore, city, gallery, riclswtravelbook, b...</td>\n",
       "      <td>singapore city gallery riclswtravelbook bearlo...</td>\n",
       "      <td>singapore city gallery riclswtravelbook bearlo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8230</th>\n",
       "      <td>19584</td>\n",
       "      <td>oh man, been waiting for #wehappyfew for so lo...</td>\n",
       "      <td>oh man been waiting for #wehappyfew for so lon...</td>\n",
       "      <td>oh man been waiting for wehappyfew for so long...</td>\n",
       "      <td>[oh, man, been, waiting, for, wehappyfew, for,...</td>\n",
       "      <td>[oh, man, been, wait, for, wehappyfew, for, so...</td>\n",
       "      <td>[oh, man, been, waiting, for, wehappyfew, for,...</td>\n",
       "      <td>oh man been waiting for wehappyfew for so long...</td>\n",
       "      <td>oh man been waiting for wehappyfew for so long...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15277</th>\n",
       "      <td>9967</td>\n",
       "      <td>schools almost over.</td>\n",
       "      <td>schools almost over</td>\n",
       "      <td>schools almost over</td>\n",
       "      <td>[schools, almost, over]</td>\n",
       "      <td>[school, almost, over]</td>\n",
       "      <td>[school, almost, over]</td>\n",
       "      <td>schools almost over</td>\n",
       "      <td>schools almost ov</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30396</th>\n",
       "      <td>2987</td>\n",
       "      <td>â #usd/cad bounces-off 0.2900, despite high...</td>\n",
       "      <td>#usd cad bounces off despite higher oil #blog ...</td>\n",
       "      <td>usd cad bounces off despite higher oil blog si...</td>\n",
       "      <td>[usd, cad, bounces, off, despite, higher, oil,...</td>\n",
       "      <td>[usd, cad, bounc, off, despit, higher, oil, bl...</td>\n",
       "      <td>[usd, cad, bounce, off, despite, higher, oil, ...</td>\n",
       "      <td>usd cad bounces off despite higher oil blog si...</td>\n",
       "      <td>usd cad bounces off despite higher oil blog si...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37982 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  \\\n",
       "565    20676  @user f*** this  ð¦ðº government that deli...   \n",
       "21531  24025  despite a demoralizing 2016: may ur #newyear20...   \n",
       "13300  18145  @user #koreans &amp; joseon people in japan, w...   \n",
       "18925   8506  @user @user @user @user classic ! yet you jewi...   \n",
       "12619  15464  @user did someone say #antisemetic ? gee (((@u...   \n",
       "26964  28937  couldn't have said this any better nor truthfu...   \n",
       "17273  25291  @user racism stuffed into skinny jeans with a ...   \n",
       "1561   12717  the end of   #me #selfie # #love #messi #cr7 #...   \n",
       "17875  11612  trump ally wishes mad cow disease death for ob...   \n",
       "17184  20554  opinion:  is rife in the #lgbt community. #gay...   \n",
       "8573   25151  @user #allahsoil the cold war was fought over ...   \n",
       "26779  13585  omg, these trump suppoers are deplorable! #dum...   \n",
       "15332   2581  sea shepherd suppoers are racist!   #antiracis...   \n",
       "12290   6519  .@user @user while @user can use phrases like ...   \n",
       "24159   9563  @user #allahsoil enlightenment is wasted on th...   \n",
       "25895   8452  @user here comes a  #supermistict douchebag wh...   \n",
       "23934  17750  @user hidden  in #america is as rampant as bla...   \n",
       "29174  21389  will the alt-right promote a new kind of  gene...   \n",
       "10474  23431  #us why weneed #empathy #ageoftrump #grassroot...   \n",
       "30743  12301  black trump suppoer smacks down cnn repoer for...   \n",
       "6190   19407  so sick of all the pre-programmed #hillbots rh...   \n",
       "19746    406  @user when you're blocked by a  troll because ...   \n",
       "19581  11720  @user @user you forgot #democrate, because vet...   \n",
       "31767  28476  sea shepherd suppoers are racist!   #antiracis...   \n",
       "25210  24768                             porn vids web free sex   \n",
       "19800  19745  @user &amp; the  #democraticpay keeps telling ...   \n",
       "7812    1242  @user although, i am not a , a #bigot or a #mi...   \n",
       "5074   23280               this is sooooo  or may be just funny   \n",
       "6317   15050  @user âwe the peopleâ originally meant â...   \n",
       "5988   24766  i've no problem with #universities that teach ...   \n",
       "...      ...                                                ...   \n",
       "3721   17405  check out this new trending #funny #gif !  , p...   \n",
       "28057  31774  this reminds me of this. i am   love these two...   \n",
       "15044   5789   livelypics: just when you think you know peop...   \n",
       "23222  29934  trying not to shut down but maybe #pokemon wil...   \n",
       "27273    395  #first #bihday to our #puppy #eloise #sweetbab...   \n",
       "30304  10458  @user it was in ceain areas yeah, you not seen...   \n",
       "3928   19081  so many shit talkers/bullies in the youtube co...   \n",
       "269    13429  all ready to pay xx #saturday #daughter #love ...   \n",
       "9826    6122  feeling a little #mole tonight! ó¾« #food #fo...   \n",
       "11548  14426  while we're still trying to get over the shock...   \n",
       "19491  19469  fantasy aisle ð #i #fantasy #aisle #summer ...   \n",
       "31522  17794  have a wonderful f r i d a y ð¸  #love #emik...   \n",
       "750    11550  who wants him to be the next commander in chie...   \n",
       "21399  18137                we beat that cock.   #beatit #penis   \n",
       "16625  14918                             lik if u cri everi tim   \n",
       "7084   12925  speakers dinner on eve of #ricsrural conferenc...   \n",
       "21332   1640  best facetime today; i can't wait to see my bo...   \n",
       "28564  18855           archery for year 5 at 1.30pm!   #archery   \n",
       "12504  23278  #whoolo in film you can have sad endings.  #an...   \n",
       "22295  17561      @user @user at what time will the gates open?   \n",
       "13326   6135  thank you @user for a new #chargehr after mine...   \n",
       "1147   21952  our thoughts and prayers goes out to everyone ...   \n",
       "19044  25937  #film   bull up: you will dominate your bull a...   \n",
       "31112  17229  this is horribly sad news. a fine actor with g...   \n",
       "14188   7012                      follow your #hea and be   â¤   \n",
       "1034   31177  cops giving tickets when they didn't even see ...   \n",
       "2874   23672  singapore city gallery   #riclswtravelbook #be...   \n",
       "8230   19584  oh man, been waiting for #wehappyfew for so lo...   \n",
       "15277   9967                               schools almost over.   \n",
       "30396   2987   â #usd/cad bounces-off 0.2900, despite high...   \n",
       "\n",
       "                                              tidy_tweet  \\\n",
       "565    f this government that deliberately toures #re...   \n",
       "21531  despite a demoralizing may ur #newyear be #cla...   \n",
       "13300  #koreans amp joseon people in japan will abuse...   \n",
       "18925  classic yet you jewish bastards wonder why you...   \n",
       "12619  did someone say #antisemetic gee you re a bit ...   \n",
       "26964  couldn t have said this any better nor truthfu...   \n",
       "17273  racism stuffed into skinny jeans with a hipste...   \n",
       "1561   the end of #me #selfie # #love #messi #cr #rel...   \n",
       "17875  trump ally wishes mad cow disease death for ob...   \n",
       "17184  opinion is rife in the #lgbt community #gay pe...   \n",
       "8573   #allahsoil the cold war was fought over oil #t...   \n",
       "26779  omg these trump suppoers are deplorable #dumpt...   \n",
       "15332  sea shepherd suppoers are racist #antiracism #...   \n",
       "12290  while can use phrases like #sandniggers is acc...   \n",
       "24159  #allahsoil enlightenment is wasted on the wilf...   \n",
       "25895  here comes a #supermistict douchebag who can o...   \n",
       "23934  hidden in #america is as rampant as blatant ra...   \n",
       "29174  will the alt right promote a new kind of genet...   \n",
       "10474  #us why weneed #empathy #ageoftrump #grassroot...   \n",
       "30743  black trump suppoer smacks down cnn repoer for...   \n",
       "6190   so sick of all the pre programmed #hillbots rh...   \n",
       "19746  when you re blocked by a troll because you pro...   \n",
       "19581  you forgot #democrate because vetting people f...   \n",
       "31767  sea shepherd suppoers are racist #antiracism #...   \n",
       "25210                             porn vids web free sex   \n",
       "19800  amp the #democraticpay keeps telling me that #...   \n",
       "7812   although i am not a a #bigot or a #misogynist ...   \n",
       "5074                 this is sooooo or may be just funny   \n",
       "6317   we the people originally meant we the white la...   \n",
       "5988   i ve no problem with #universities that teach ...   \n",
       "...                                                  ...   \n",
       "3721   check out this new trending #funny #gif pixel ...   \n",
       "28057  this reminds me of this i am love these two th...   \n",
       "15044  livelypics just when you think you know people...   \n",
       "23222  trying not to shut down but maybe #pokemon wil...   \n",
       "27273  #first #bihday to our #puppy #eloise #sweetbab...   \n",
       "30304  it was in ceain areas yeah you not seen the vi...   \n",
       "3928   so many shit talkers bullies in the youtube co...   \n",
       "269    all ready to pay xx #saturday #daughter #love ...   \n",
       "9826   feeling a little #mole tonight #food #foodblog...   \n",
       "11548  while we re still trying to get over the shock...   \n",
       "19491  fantasy aisle #i #fantasy #aisle #summer #gran...   \n",
       "31522  have a wonderful f r i d a y #love #emikagifts...   \n",
       "750    who wants him to be the next commander in chie...   \n",
       "21399                   we beat that cock #beatit #penis   \n",
       "16625                             lik if u cri everi tim   \n",
       "7084   speakers dinner on eve of #ricsrural conferenc...   \n",
       "21332  best facetime today i can t wait to see my boy...   \n",
       "28564                    archery for year at pm #archery   \n",
       "12504  #whoolo in film you can have sad endings #anna...   \n",
       "22295                   at what time will the gates open   \n",
       "13326  thank you for a new #chargehr after mine broke...   \n",
       "1147   our thoughts and prayers goes out to everyone ...   \n",
       "19044  #film bull up you will dominate your bull and ...   \n",
       "31112  this is horribly sad news a fine actor with gr...   \n",
       "14188                            follow your #hea and be   \n",
       "1034   cops giving tickets when they didn t even see ...   \n",
       "2874   singapore city gallery #riclswtravelbook #bear...   \n",
       "8230   oh man been waiting for #wehappyfew for so lon...   \n",
       "15277                                schools almost over   \n",
       "30396  #usd cad bounces off despite higher oil #blog ...   \n",
       "\n",
       "                                           no_hash_tweet  \\\n",
       "565    f this government that deliberately toures ref...   \n",
       "21531  despite a demoralizing may ur newyear be class...   \n",
       "13300  koreans amp joseon people in japan will abuse ...   \n",
       "18925  classic yet you jewish bastards wonder why you...   \n",
       "12619  did someone say antisemetic gee you re a bit t...   \n",
       "26964  couldn t have said this any better nor truthfu...   \n",
       "17273  racism stuffed into skinny jeans with a hipste...   \n",
       "1561   the end of me selfie  love messi cr religion c...   \n",
       "17875  trump ally wishes mad cow disease death for ob...   \n",
       "17184  opinion is rife in the lgbt community gay peop...   \n",
       "8573   allahsoil the cold war was fought over oil tea...   \n",
       "26779  omg these trump suppoers are deplorable dumptr...   \n",
       "15332  sea shepherd suppoers are racist antiracism se...   \n",
       "12290  while can use phrases like sandniggers is acce...   \n",
       "24159  allahsoil enlightenment is wasted on the wilfu...   \n",
       "25895  here comes a supermistict douchebag who can on...   \n",
       "23934  hidden in america is as rampant as blatant racism   \n",
       "29174  will the alt right promote a new kind of genet...   \n",
       "10474  us why weneed empathy ageoftrump grassrootsact...   \n",
       "30743  black trump suppoer smacks down cnn repoer for...   \n",
       "6190   so sick of all the pre programmed hillbots rhe...   \n",
       "19746  when you re blocked by a troll because you pro...   \n",
       "19581  you forgot democrate because vetting people fr...   \n",
       "31767  sea shepherd suppoers are racist antiracism se...   \n",
       "25210                             porn vids web free sex   \n",
       "19800  amp the democraticpay keeps telling me that bl...   \n",
       "7812   although i am not a a bigot or a misogynist so...   \n",
       "5074                 this is sooooo or may be just funny   \n",
       "6317   we the people originally meant we the white la...   \n",
       "5988   i ve no problem with universities that teach a...   \n",
       "...                                                  ...   \n",
       "3721   check out this new trending funny gif pixel ce...   \n",
       "28057  this reminds me of this i am love these two th...   \n",
       "15044  livelypics just when you think you know people...   \n",
       "23222  trying not to shut down but maybe pokemon will...   \n",
       "27273  first bihday to our puppy eloise sweetbabins d...   \n",
       "30304  it was in ceain areas yeah you not seen the vi...   \n",
       "3928   so many shit talkers bullies in the youtube co...   \n",
       "269    all ready to pay xx saturday daughter love pay...   \n",
       "9826   feeling a little mole tonight food foodblogger...   \n",
       "11548  while we re still trying to get over the shock...   \n",
       "19491  fantasy aisle i fantasy aisle summer grand vac...   \n",
       "31522  have a wonderful f r i d a y love emikagifts j...   \n",
       "750    who wants him to be the next commander in chie...   \n",
       "21399                     we beat that cock beatit penis   \n",
       "16625                             lik if u cri everi tim   \n",
       "7084   speakers dinner on eve of ricsrural conference...   \n",
       "21332  best facetime today i can t wait to see my boy...   \n",
       "28564                     archery for year at pm archery   \n",
       "12504  whoolo in film you can have sad endings anna torv   \n",
       "22295                   at what time will the gates open   \n",
       "13326  thank you for a new chargehr after mine broke ...   \n",
       "1147   our thoughts and prayers goes out to everyone ...   \n",
       "19044  film bull up you will dominate your bull and y...   \n",
       "31112  this is horribly sad news a fine actor with gr...   \n",
       "14188                             follow your hea and be   \n",
       "1034   cops giving tickets when they didn t even see ...   \n",
       "2874   singapore city gallery riclswtravelbook bearlo...   \n",
       "8230   oh man been waiting for wehappyfew for so long...   \n",
       "15277                                schools almost over   \n",
       "30396  usd cad bounces off despite higher oil blog si...   \n",
       "\n",
       "                                         tokenized_tweet  \\\n",
       "565    [f, this, government, that, deliberately, tour...   \n",
       "21531  [despite, a, demoralizing, may, ur, newyear, b...   \n",
       "13300  [koreans, amp, joseon, people, in, japan, will...   \n",
       "18925  [classic, yet, you, jewish, bastards, wonder, ...   \n",
       "12619  [did, someone, say, antisemetic, gee, you, re,...   \n",
       "26964  [couldn, t, have, said, this, any, better, nor...   \n",
       "17273  [racism, stuffed, into, skinny, jeans, with, a...   \n",
       "1561   [the, end, of, me, selfie, love, messi, cr, re...   \n",
       "17875  [trump, ally, wishes, mad, cow, disease, death...   \n",
       "17184  [opinion, is, rife, in, the, lgbt, community, ...   \n",
       "8573   [allahsoil, the, cold, war, was, fought, over,...   \n",
       "26779  [omg, these, trump, suppoers, are, deplorable,...   \n",
       "15332  [sea, shepherd, suppoers, are, racist, antirac...   \n",
       "12290  [while, can, use, phrases, like, sandniggers, ...   \n",
       "24159  [allahsoil, enlightenment, is, wasted, on, the...   \n",
       "25895  [here, comes, a, supermistict, douchebag, who,...   \n",
       "23934  [hidden, in, america, is, as, rampant, as, bla...   \n",
       "29174  [will, the, alt, right, promote, a, new, kind,...   \n",
       "10474  [us, why, weneed, empathy, ageoftrump, grassro...   \n",
       "30743  [black, trump, suppoer, smacks, down, cnn, rep...   \n",
       "6190   [so, sick, of, all, the, pre, programmed, hill...   \n",
       "19746  [when, you, re, blocked, by, a, troll, because...   \n",
       "19581  [you, forgot, democrate, because, vetting, peo...   \n",
       "31767  [sea, shepherd, suppoers, are, racist, antirac...   \n",
       "25210                       [porn, vids, web, free, sex]   \n",
       "19800  [amp, the, democraticpay, keeps, telling, me, ...   \n",
       "7812   [although, i, am, not, a, a, bigot, or, a, mis...   \n",
       "5074        [this, is, sooooo, or, may, be, just, funny]   \n",
       "6317   [we, the, people, originally, meant, we, the, ...   \n",
       "5988   [i, ve, no, problem, with, universities, that,...   \n",
       "...                                                  ...   \n",
       "3721   [check, out, this, new, trending, funny, gif, ...   \n",
       "28057  [this, reminds, me, of, this, i, am, love, the...   \n",
       "15044  [livelypics, just, when, you, think, you, know...   \n",
       "23222  [trying, not, to, shut, down, but, maybe, poke...   \n",
       "27273  [first, bihday, to, our, puppy, eloise, sweetb...   \n",
       "30304  [it, was, in, ceain, areas, yeah, you, not, se...   \n",
       "3928   [so, many, shit, talkers, bullies, in, the, yo...   \n",
       "269    [all, ready, to, pay, xx, saturday, daughter, ...   \n",
       "9826   [feeling, a, little, mole, tonight, food, food...   \n",
       "11548  [while, we, re, still, trying, to, get, over, ...   \n",
       "19491  [fantasy, aisle, i, fantasy, aisle, summer, gr...   \n",
       "31522  [have, a, wonderful, f, r, i, d, a, y, love, e...   \n",
       "750    [who, wants, him, to, be, the, next, commander...   \n",
       "21399              [we, beat, that, cock, beatit, penis]   \n",
       "16625                      [lik, if, u, cri, everi, tim]   \n",
       "7084   [speakers, dinner, on, eve, of, ricsrural, con...   \n",
       "21332  [best, facetime, today, i, can, t, wait, to, s...   \n",
       "28564              [archery, for, year, at, pm, archery]   \n",
       "12504  [whoolo, in, film, you, can, have, sad, ending...   \n",
       "22295           [at, what, time, will, the, gates, open]   \n",
       "13326  [thank, you, for, a, new, chargehr, after, min...   \n",
       "1147   [our, thoughts, and, prayers, goes, out, to, e...   \n",
       "19044  [film, bull, up, you, will, dominate, your, bu...   \n",
       "31112  [this, is, horribly, sad, news, a, fine, actor...   \n",
       "14188                       [follow, your, hea, and, be]   \n",
       "1034   [cops, giving, tickets, when, they, didn, t, e...   \n",
       "2874   [singapore, city, gallery, riclswtravelbook, b...   \n",
       "8230   [oh, man, been, waiting, for, wehappyfew, for,...   \n",
       "15277                            [schools, almost, over]   \n",
       "30396  [usd, cad, bounces, off, despite, higher, oil,...   \n",
       "\n",
       "                                          stemmed_tokens  \\\n",
       "565    [f, this, govern, that, deliber, tour, refuge,...   \n",
       "21531  [despit, a, demor, may, ur, newyear, be, class...   \n",
       "13300  [korean, amp, joseon, peopl, in, japan, will, ...   \n",
       "18925  [classic, yet, you, jewish, bastard, wonder, w...   \n",
       "12619  [did, someon, say, antisemet, gee, you, re, a,...   \n",
       "26964  [couldn, t, have, said, this, ani, better, nor...   \n",
       "17273  [racism, stuf, into, skinni, jean, with, a, hi...   \n",
       "1561   [the, end, of, me, selfi, love, messi, cr, rel...   \n",
       "17875  [trump, alli, wish, mad, cow, diseas, death, f...   \n",
       "17184  [opinion, is, rife, in, the, lgbt, communiti, ...   \n",
       "8573   [allahsoil, the, cold, war, was, fought, over,...   \n",
       "26779  [omg, these, trump, suppoer, are, deplor, dump...   \n",
       "15332  [sea, shepherd, suppoer, are, racist, antirac,...   \n",
       "12290  [while, can, use, phrase, like, sandnigg, is, ...   \n",
       "24159  [allahsoil, enlighten, is, wast, on, the, wil,...   \n",
       "25895  [here, come, a, supermistict, douchebag, who, ...   \n",
       "23934  [hidden, in, america, is, as, rampant, as, bla...   \n",
       "29174  [will, the, alt, right, promot, a, new, kind, ...   \n",
       "10474  [us, whi, wene, empathi, ageoftrump, grassroot...   \n",
       "30743  [black, trump, suppoer, smack, down, cnn, repo...   \n",
       "6190   [so, sick, of, all, the, pre, program, hillbot...   \n",
       "19746  [when, you, re, block, by, a, troll, becaus, y...   \n",
       "19581  [you, forgot, democr, becaus, vet, peopl, from...   \n",
       "31767  [sea, shepherd, suppoer, are, racist, antirac,...   \n",
       "25210                        [porn, vid, web, free, sex]   \n",
       "19800  [amp, the, democraticpay, keep, tell, me, that...   \n",
       "7812   [although, i, am, not, a, a, bigot, or, a, mis...   \n",
       "5074        [this, is, sooooo, or, may, be, just, funni]   \n",
       "6317   [we, the, peopl, origin, meant, we, the, white...   \n",
       "5988   [i, ve, no, problem, with, univers, that, teac...   \n",
       "...                                                  ...   \n",
       "3721   [check, out, this, new, trend, funni, gif, pix...   \n",
       "28057  [this, remind, me, of, this, i, am, love, thes...   \n",
       "15044  [livelyp, just, when, you, think, you, know, p...   \n",
       "23222  [tri, not, to, shut, down, but, mayb, pokemon,...   \n",
       "27273  [first, bihday, to, our, puppi, elois, sweetba...   \n",
       "30304  [it, was, in, ceain, area, yeah, you, not, see...   \n",
       "3928   [so, mani, shit, talker, bulli, in, the, youtu...   \n",
       "269    [all, readi, to, pay, xx, saturday, daughter, ...   \n",
       "9826   [feel, a, littl, mole, tonight, food, foodblog...   \n",
       "11548  [while, we, re, still, tri, to, get, over, the...   \n",
       "19491  [fantasi, aisl, i, fantasi, aisl, summer, gran...   \n",
       "31522  [have, a, wonder, f, r, i, d, a, y, love, emik...   \n",
       "750    [who, want, him, to, be, the, next, command, i...   \n",
       "21399               [we, beat, that, cock, beatit, peni]   \n",
       "16625                      [lik, if, u, cri, everi, tim]   \n",
       "7084   [speaker, dinner, on, eve, of, ricsrur, confer...   \n",
       "21332  [best, facetim, today, i, can, t, wait, to, se...   \n",
       "28564              [archeri, for, year, at, pm, archeri]   \n",
       "12504  [whoolo, in, film, you, can, have, sad, end, a...   \n",
       "22295            [at, what, time, will, the, gate, open]   \n",
       "13326  [thank, you, for, a, new, chargehr, after, min...   \n",
       "1147   [our, thought, and, prayer, goe, out, to, ever...   \n",
       "19044  [film, bull, up, you, will, domin, your, bull,...   \n",
       "31112  [this, is, horribl, sad, news, a, fine, actor,...   \n",
       "14188                       [follow, your, hea, and, be]   \n",
       "1034   [cop, give, ticket, when, they, didn, t, even,...   \n",
       "2874   [singapor, citi, galleri, riclswtravelbook, be...   \n",
       "8230   [oh, man, been, wait, for, wehappyfew, for, so...   \n",
       "15277                             [school, almost, over]   \n",
       "30396  [usd, cad, bounc, off, despit, higher, oil, bl...   \n",
       "\n",
       "                                       lemmatized_tokens  \\\n",
       "565    [f, this, government, that, deliberately, tour...   \n",
       "21531  [despite, a, demoralizing, may, ur, newyear, b...   \n",
       "13300  [korean, amp, joseon, people, in, japan, will,...   \n",
       "18925  [classic, yet, you, jewish, bastard, wonder, w...   \n",
       "12619  [did, someone, say, antisemetic, gee, you, re,...   \n",
       "26964  [couldn, t, have, said, this, any, better, nor...   \n",
       "17273  [racism, stuffed, into, skinny, jean, with, a,...   \n",
       "1561   [the, end, of, me, selfie, love, messi, cr, re...   \n",
       "17875  [trump, ally, wish, mad, cow, disease, death, ...   \n",
       "17184  [opinion, is, rife, in, the, lgbt, community, ...   \n",
       "8573   [allahsoil, the, cold, war, wa, fought, over, ...   \n",
       "26779  [omg, these, trump, suppoers, are, deplorable,...   \n",
       "15332  [sea, shepherd, suppoers, are, racist, antirac...   \n",
       "12290  [while, can, use, phrase, like, sandniggers, i...   \n",
       "24159  [allahsoil, enlightenment, is, wasted, on, the...   \n",
       "25895  [here, come, a, supermistict, douchebag, who, ...   \n",
       "23934  [hidden, in, america, is, a, rampant, a, blata...   \n",
       "29174  [will, the, alt, right, promote, a, new, kind,...   \n",
       "10474  [u, why, weneed, empathy, ageoftrump, grassroo...   \n",
       "30743  [black, trump, suppoer, smack, down, cnn, repo...   \n",
       "6190   [so, sick, of, all, the, pre, programmed, hill...   \n",
       "19746  [when, you, re, blocked, by, a, troll, because...   \n",
       "19581  [you, forgot, democrate, because, vetting, peo...   \n",
       "31767  [sea, shepherd, suppoers, are, racist, antirac...   \n",
       "25210                       [porn, vids, web, free, sex]   \n",
       "19800  [amp, the, democraticpay, keep, telling, me, t...   \n",
       "7812   [although, i, am, not, a, a, bigot, or, a, mis...   \n",
       "5074        [this, is, sooooo, or, may, be, just, funny]   \n",
       "6317   [we, the, people, originally, meant, we, the, ...   \n",
       "5988   [i, ve, no, problem, with, university, that, t...   \n",
       "...                                                  ...   \n",
       "3721   [check, out, this, new, trending, funny, gif, ...   \n",
       "28057  [this, reminds, me, of, this, i, am, love, the...   \n",
       "15044  [livelypics, just, when, you, think, you, know...   \n",
       "23222  [trying, not, to, shut, down, but, maybe, poke...   \n",
       "27273  [first, bihday, to, our, puppy, eloise, sweetb...   \n",
       "30304  [it, wa, in, ceain, area, yeah, you, not, seen...   \n",
       "3928   [so, many, shit, talker, bully, in, the, youtu...   \n",
       "269    [all, ready, to, pay, xx, saturday, daughter, ...   \n",
       "9826   [feeling, a, little, mole, tonight, food, food...   \n",
       "11548  [while, we, re, still, trying, to, get, over, ...   \n",
       "19491  [fantasy, aisle, i, fantasy, aisle, summer, gr...   \n",
       "31522  [have, a, wonderful, f, r, i, d, a, y, love, e...   \n",
       "750    [who, want, him, to, be, the, next, commander,...   \n",
       "21399              [we, beat, that, cock, beatit, penis]   \n",
       "16625                      [lik, if, u, cri, everi, tim]   \n",
       "7084   [speaker, dinner, on, eve, of, ricsrural, conf...   \n",
       "21332  [best, facetime, today, i, can, t, wait, to, s...   \n",
       "28564              [archery, for, year, at, pm, archery]   \n",
       "12504  [whoolo, in, film, you, can, have, sad, ending...   \n",
       "22295            [at, what, time, will, the, gate, open]   \n",
       "13326  [thank, you, for, a, new, chargehr, after, min...   \n",
       "1147   [our, thought, and, prayer, go, out, to, every...   \n",
       "19044  [film, bull, up, you, will, dominate, your, bu...   \n",
       "31112  [this, is, horribly, sad, news, a, fine, actor...   \n",
       "14188                       [follow, your, hea, and, be]   \n",
       "1034   [cop, giving, ticket, when, they, didn, t, eve...   \n",
       "2874   [singapore, city, gallery, riclswtravelbook, b...   \n",
       "8230   [oh, man, been, waiting, for, wehappyfew, for,...   \n",
       "15277                             [school, almost, over]   \n",
       "30396  [usd, cad, bounce, off, despite, higher, oil, ...   \n",
       "\n",
       "                                               lem_tweet  \\\n",
       "565    f this government that deliberately toures ref...   \n",
       "21531  despite a demoralizing may ur newyear be class...   \n",
       "13300  koreans amp joseon people in japan will abuse ...   \n",
       "18925  classic yet you jewish bastards wonder why you...   \n",
       "12619  did someone say antisemetic gee you re a bit t...   \n",
       "26964  couldn t have said this any better nor truthfu...   \n",
       "17273  racism stuffed into skinny jeans with a hipste...   \n",
       "1561   the end of me selfie  love messi cr religion c...   \n",
       "17875  trump ally wishes mad cow disease death for ob...   \n",
       "17184  opinion is rife in the lgbt community gay peop...   \n",
       "8573   allahsoil the cold war was fought over oil tea...   \n",
       "26779  omg these trump suppoers are deplorable dumptr...   \n",
       "15332  sea shepherd suppoers are racist antiracism se...   \n",
       "12290  while can use phrases like sandniggers is acce...   \n",
       "24159  allahsoil enlightenment is wasted on the wilfu...   \n",
       "25895  here comes a supermistict douchebag who can on...   \n",
       "23934  hidden in america is as rampant as blatant racism   \n",
       "29174  will the alt right promote a new kind of genet...   \n",
       "10474  us why weneed empathy ageoftrump grassrootsact...   \n",
       "30743  black trump suppoer smacks down cnn repoer for...   \n",
       "6190   so sick of all the pre programmed hillbots rhe...   \n",
       "19746  when you re blocked by a troll because you pro...   \n",
       "19581  you forgot democrate because vetting people fr...   \n",
       "31767  sea shepherd suppoers are racist antiracism se...   \n",
       "25210                             porn vids web free sex   \n",
       "19800  amp the democraticpay keeps telling me that bl...   \n",
       "7812   although i am not a a bigot or a misogynist so...   \n",
       "5074                 this is sooooo or may be just funny   \n",
       "6317   we the people originally meant we the white la...   \n",
       "5988   i ve no problem with universities that teach a...   \n",
       "...                                                  ...   \n",
       "3721   check out this new trending funny gif pixel ce...   \n",
       "28057  this reminds me of this i am love these two th...   \n",
       "15044  livelypics just when you think you know people...   \n",
       "23222  trying not to shut down but maybe pokemon will...   \n",
       "27273  first bihday to our puppy eloise sweetbabins d...   \n",
       "30304  it was in ceain areas yeah you not seen the vi...   \n",
       "3928   so many shit talkers bullies in the youtube co...   \n",
       "269    all ready to pay xx saturday daughter love pay...   \n",
       "9826   feeling a little mole tonight food foodblogger...   \n",
       "11548  while we re still trying to get over the shock...   \n",
       "19491  fantasy aisle i fantasy aisle summer grand vac...   \n",
       "31522  have a wonderful f r i d a y love emikagifts j...   \n",
       "750    who wants him to be the next commander in chie...   \n",
       "21399                     we beat that cock beatit penis   \n",
       "16625                             lik if u cri everi tim   \n",
       "7084   speakers dinner on eve of ricsrural conference...   \n",
       "21332  best facetime today i can t wait to see my boy...   \n",
       "28564                     archery for year at pm archery   \n",
       "12504  whoolo in film you can have sad endings anna torv   \n",
       "22295                   at what time will the gates open   \n",
       "13326  thank you for a new chargehr after mine broke ...   \n",
       "1147   our thoughts and prayers goes out to everyone ...   \n",
       "19044  film bull up you will dominate your bull and y...   \n",
       "31112  this is horribly sad news a fine actor with gr...   \n",
       "14188                             follow your hea and be   \n",
       "1034   cops giving tickets when they didn t even see ...   \n",
       "2874   singapore city gallery riclswtravelbook bearlo...   \n",
       "8230   oh man been waiting for wehappyfew for so long...   \n",
       "15277                                schools almost over   \n",
       "30396  usd cad bounces off despite higher oil blog si...   \n",
       "\n",
       "                                              stem_tweet  label  \n",
       "565    f this government that deliberately toures ref...      1  \n",
       "21531  despite a demoralizing may ur newyear be class...      1  \n",
       "13300  koreans amp joseon people in japan will abuse ...      1  \n",
       "18925  classic yet you jewish bastards wonder why you...      1  \n",
       "12619  did someone say antisemetic gee you re a bit t...      1  \n",
       "26964  couldn t have said this any better nor truthfu...      1  \n",
       "17273  racism stuffed into skinny jeans with a hipste...      1  \n",
       "1561   the end of me selfie  love messi cr religion c...      1  \n",
       "17875  trump ally wishes mad cow disease death for ob...      1  \n",
       "17184  opinion is rife in the lgbt community gay peop...      1  \n",
       "8573   allahsoil the cold war was fought over oil tea...      1  \n",
       "26779  omg these trump suppoers are deplorable dumptr...      1  \n",
       "15332  sea shepherd suppoers are racist antiracism se...      1  \n",
       "12290  while can use phrases like sandniggers is acce...      1  \n",
       "24159  allahsoil enlightenment is wasted on the wilfu...      1  \n",
       "25895  here comes a supermistict douchebag who can on...      1  \n",
       "23934     hidden in america is as rampant as blatant rac      1  \n",
       "29174  will the alt right promote a new kind of genet...      1  \n",
       "10474  us why weneed empathy ageoftrump grassrootsact...      1  \n",
       "30743  black trump suppoer smacks down cnn repoer for...      1  \n",
       "6190   so sick of all the pre programmed hillbots rhe...      1  \n",
       "19746  when you re blocked by a troll because you pro...      1  \n",
       "19581  you forgot democrate because vetting people fr...      1  \n",
       "31767  sea shepherd suppoers are racist antiracism se...      1  \n",
       "25210                             porn vids web free sex      1  \n",
       "19800  amp the democraticpay keeps telling me that bl...      1  \n",
       "7812   although i am not a a bigot or a misogynist so...      1  \n",
       "5074                 this is sooooo or may be just funni      1  \n",
       "6317   we the people originally meant we the white la...      1  \n",
       "5988   i ve no problem with universities that teach a...      1  \n",
       "...                                                  ...    ...  \n",
       "3721   check out this new trending funny gif pixel ce...      0  \n",
       "28057  this reminds me of this i am love these two th...      0  \n",
       "15044  livelypics just when you think you know people...      0  \n",
       "23222  trying not to shut down but maybe pokemon will...      0  \n",
       "27273  first bihday to our puppy eloise sweetbabins d...      0  \n",
       "30304  it was in ceain areas yeah you not seen the vi...      0  \n",
       "3928   so many shit talkers bullies in the youtube co...      0  \n",
       "269    all ready to pay xx saturday daughter love pay...      0  \n",
       "9826   feeling a little mole tonight food foodblogger...      0  \n",
       "11548  while we re still trying to get over the shock...      0  \n",
       "19491  fantasy aisle i fantasy aisle summer grand vac...      0  \n",
       "31522  have a wonderful f r i d a y love emikagifts j...      0  \n",
       "750    who wants him to be the next commander in chie...      0  \n",
       "21399                      we beat that cock beatit peni      0  \n",
       "16625                             lik if u cri everi tim      0  \n",
       "7084   speakers dinner on eve of ricsrural conference...      0  \n",
       "21332  best facetime today i can t wait to see my boy...      0  \n",
       "28564                     archery for year at pm archeri      0  \n",
       "12504  whoolo in film you can have sad endings anna torv      0  \n",
       "22295                   at what time will the gates open      0  \n",
       "13326  thank you for a new chargehr after mine broke ...      0  \n",
       "1147   our thoughts and prayers goes out to everyone ...      0  \n",
       "19044  film bull up you will dominate your bull and y...      0  \n",
       "31112  this is horribly sad news a fine actor with gr...      0  \n",
       "14188                              follow your hea and b      0  \n",
       "1034   cops giving tickets when they didn t even see ...      0  \n",
       "2874   singapore city gallery riclswtravelbook bearlo...      0  \n",
       "8230   oh man been waiting for wehappyfew for so long...      0  \n",
       "15277                                  schools almost ov      0  \n",
       "30396  usd cad bounces off despite higher oil blog si...      0  \n",
       "\n",
       "[37982 rows x 10 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsample_training_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_upsampled = upsample_training_data(X_train, y_train)\n",
    "\n",
    "X_train_up = train_upsampled.drop(['label'], axis = 1)\n",
    "y_train_up = pd.DataFrame(train_upsampled.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    18991\n",
       "0    18991\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_upsampled['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_downsampled = downsample_training_data(X_train, y_train)\n",
    "\n",
    "X_train_down = train_downsampled.drop(['label'], axis = 1)\n",
    "y_train_down = pd.DataFrame(train_downsampled.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_downsampled['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Vectorization and Method Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=.001)\n",
    "tfidf_ngram = TfidfVectorizer(ngram_range=(1,2), min_df=.001)\n",
    "tfidf_ngram2 = TfidfVectorizer(ngram_range=(2,3),min_df=.001)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "rfc = RandomForestClassifier(random_state=10)\n",
    "nb = GaussianNB()\n",
    "svc = SVC(random_state=10)\n",
    "\n",
    "vectorization_list = [('COUNT_VECTORIZER', count_vect),\n",
    "                      ('TFIDF_VECTORIZER', tfidf_vectorizer),\n",
    "                      ('TFIDF_NGRAM_1_2', tfidf_ngram),\n",
    "                      ('TFIDF_NGRAM_2_3', tfidf_ngram2)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train.stem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.DataFrame(X_train.lem_tweet)\n",
    "training_data['label']= y_train\n",
    "\n",
    "train_0 = training_data[training_data.label==0]\n",
    "train_1 = training_data[training_data.label==1]\n",
    "\n",
    "train_1_up = resample(train_1, replace=True, n_samples=len(train_0),random_state=10)\n",
    "\n",
    "train_upsampled = pd.concat([train_1_up, train_0])\n",
    "\n",
    "train_upsampled.head()\n",
    "# X_train_col_up = train_upsampled.drop(['label'], axis = 1)\n",
    "# y_train_up = train_upsampled.label\n",
    "\n",
    "# # perform vectorization\n",
    "# X_train_up_transformed = vectorizer.fit_transform(X_train_col_up)\n",
    "# X_val_transformed = vectorizer.transform(X_val_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.lem_tweet.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample_training_data(X_train.lem_tweet, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_up2, y_train_up2, X_train_transf2, X_val_transf2 = \\\n",
    "upsample_training_data3(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, log, count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.lem_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_up2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transf2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transf2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_up.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.lem_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_up, y_up, X_train_transformed, X_val_transformed = \\\n",
    "upsample_training_data3(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, LogisticRegression(), tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_up.lem_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_up.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(X_train_up.lem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = count_vect.fit_transform(X_up)\n",
    "vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = count_vect.fit_transform(test)\n",
    "vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed, X_val_transformed, y_train_pred, y_val_pred, y_val_pred_prob, matrix, \\\n",
    "compare_metrics, compare_predictions = \\\n",
    "single_vector_model3(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, LogisticRegression(), count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed, X_val_transformed, y_train_pred, y_val_pred, y_val_pred_prob, matrix, \\\n",
    "compare_metrics, compare_predictions = \\\n",
    "upsample_vector_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, LogisticRegression(), count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed, X_val_transformed, y_train_pred, y_val_pred2, y_val_pred_prob, \\\n",
    "compare_predictions, metrics_dict2 = \\\n",
    "smote_vector_model2(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, LogisticRegression(), count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20455, 28610)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed, X_val_transformed, y_train_pred, y_val_pred2, y_val_pred_prob, \\\n",
    "compare_predictions, metrics_dict2 = \\\n",
    "upsample_vector_model2(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, LogisticRegression(), count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed, X_val_transformed, y_train_pred, y_val_pred2, y_val_pred_prob, \\\n",
    "compare_predictions, metrics_dict2 = \\\n",
    "downsample_vector_model2(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, LogisticRegression(), count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix (y_val, y_val_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_down_transformed, X_val_transformed, y_train_down, y_val_pred2, yval_pp, \\\n",
    " compare_predictions, metrics_dict = \\\n",
    "upsample_vector_model2(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, LogisticRegression(), count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_down_transformed, X_val_transformed, y_train_down, y_val_pred2, yval_pp, \\\n",
    " compare_predictions, metrics_dict = \\\n",
    "single_vector_model2(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, LogisticRegression(), count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t, X_val_t, y_train_pred, y_val_pred3, y_val_prob, metrics2, pred_df = \\\n",
    "wrapper_single_vectorization(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val,\n",
    "                              LogisticRegression(), count_vect, sampling='smote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t, X_val_t, y_train_pred, y_val_pred3, y_val_prob, metrics3, pred_df = \\\n",
    "wrapper_single_vectorization(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val,\n",
    "                              LogisticRegression(), count_vect, sampling='upsample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t, X_val_t, y_train_pred, y_val_pred3, y_val_prob, metrics3, pred_df = \\\n",
    "wrapper_single_vectorization(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val,\n",
    "                              LogisticRegression(), count_vect, sampling='downsample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t, X_val_t, y_train_pred, y_val_pred3, y_val_prob, metrics3, pred_df = \\\n",
    "wrapper_single_vectorization(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, log, count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_results = downsample_compare_vectorization_model(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                                         LogisticRegression(penalty = 'l1', random_state=1),\n",
    "                                                         vectorization_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT_VECTORIZER</th>\n",
       "      <th>TFIDF_VECTORIZER</th>\n",
       "      <th>TFIDF_NGRAM_1_2</th>\n",
       "      <th>TFIDF_NGRAM_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation F1</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Precision</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Recall</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      COUNT_VECTORIZER  TFIDF_VECTORIZER  TFIDF_NGRAM_1_2  \\\n",
       "Train Accuracy                    0.93              0.85             0.84   \n",
       "Train F1                          0.93              0.85             0.84   \n",
       "Train Precision                   0.94              0.87             0.87   \n",
       "Train Recall                      0.91              0.82             0.81   \n",
       "Validation Accuracy               0.85              0.83             0.82   \n",
       "Validation F1                     0.40              0.35             0.35   \n",
       "Validation Precision              0.27              0.23             0.23   \n",
       "Validation Recall                 0.76              0.73             0.74   \n",
       "\n",
       "                      TFIDF_NGRAM_2_3  \n",
       "Train Accuracy                   0.72  \n",
       "Train F1                         0.64  \n",
       "Train Precision                  0.90  \n",
       "Train Recall                     0.50  \n",
       "Validation Accuracy              0.86  \n",
       "Validation F1                    0.27  \n",
       "Validation Precision             0.21  \n",
       "Validation Recall                0.38  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_results = \\\n",
    "wrapper_compare_vectorizations2(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val,\n",
    "                             LogisticRegression(penalty='l1', random_state =1), \n",
    "                                vectorization_list, sampling = 'upsample')\n",
    "pd.DataFrame(metrics_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT_VECTORIZER</th>\n",
       "      <th>TFIDF_VECTORIZER</th>\n",
       "      <th>TFIDF_NGRAM_1_2</th>\n",
       "      <th>TFIDF_NGRAM_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation F1</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Precision</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Recall</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      COUNT_VECTORIZER  TFIDF_VECTORIZER  TFIDF_NGRAM_1_2  \\\n",
       "Train Accuracy                    0.93              0.85             0.84   \n",
       "Train F1                          0.93              0.85             0.84   \n",
       "Train Precision                   0.94              0.87             0.87   \n",
       "Train Recall                      0.91              0.82             0.81   \n",
       "Validation Accuracy               0.85              0.83             0.82   \n",
       "Validation F1                     0.40              0.35             0.35   \n",
       "Validation Precision              0.27              0.23             0.23   \n",
       "Validation Recall                 0.76              0.73             0.74   \n",
       "\n",
       "                      TFIDF_NGRAM_2_3  \n",
       "Train Accuracy                   0.72  \n",
       "Train F1                         0.64  \n",
       "Train Precision                  0.90  \n",
       "Train Recall                     0.50  \n",
       "Validation Accuracy              0.86  \n",
       "Validation F1                    0.27  \n",
       "Validation Precision             0.21  \n",
       "Validation Recall                0.38  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_results = \\\n",
    "wrapper_compare_vectorizations2(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val,\n",
    "                             LogisticRegression(penalty='l1', random_state =1), \n",
    "                                vectorization_list, sampling = 'downsample')\n",
    "pd.DataFrame(metrics_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT_VECTORIZER</th>\n",
       "      <th>TFIDF_VECTORIZER</th>\n",
       "      <th>TFIDF_NGRAM_1_2</th>\n",
       "      <th>TFIDF_NGRAM_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation F1</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Precision</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Recall</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      COUNT_VECTORIZER  TFIDF_VECTORIZER  TFIDF_NGRAM_1_2  \\\n",
       "Train Accuracy                    0.97              0.95             0.95   \n",
       "Train F1                          0.79              0.53             0.52   \n",
       "Train Precision                   0.96              0.84             0.84   \n",
       "Train Recall                      0.67              0.39             0.38   \n",
       "Validation Accuracy               0.96              0.95             0.95   \n",
       "Validation F1                     0.62              0.50             0.48   \n",
       "Validation Precision              0.81              0.78             0.79   \n",
       "Validation Recall                 0.50              0.37             0.35   \n",
       "\n",
       "                      TFIDF_NGRAM_2_3  \n",
       "Train Accuracy                   0.93  \n",
       "Train F1                         0.14  \n",
       "Train Precision                  0.91  \n",
       "Train Recall                     0.08  \n",
       "Validation Accuracy              0.94  \n",
       "Validation F1                    0.13  \n",
       "Validation Precision             0.96  \n",
       "Validation Recall                0.07  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_results = \\\n",
    "wrapper_compare_vectorizations2(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val,\n",
    "                             LogisticRegression(penalty='l1', random_state =1), \n",
    "                                vectorization_list)\n",
    "pd.DataFrame(metrics_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling method does not exist. Indicate: smote, upsample or downsample\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_results = \\\n",
    "wrapper_compare_vectorizations2(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val,\n",
    "                             LogisticRegression(penalty='l1', random_state =1), \n",
    "                                vectorization_list, sampling = 'gibberish')\n",
    "pd.DataFrame(metrics_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_compare_vectorization_model(X_train.lem_tweet, y_train, \n",
    "                                   X_val.lem_tweet, y_val, GaussianNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression: compare vectorizers using lemmitizing + class balances\n",
    "LR_cw_lemm = wrapper_compare_vectorizations2(X_train.lem_tweet, \n",
    "                            y_train, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', solver = 'lbfgs'), \n",
    "                                            vectorization_list, sampling = 'upsample')\n",
    "\n",
    "LR_cw_lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression: compare vectorizers using lemmitizing + upsampling\n",
    "LR_cw_lemm = wrapper_compare_vectorizations(X_train_up.lem_tweet, \n",
    "                            y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', solver = 'lbfgs'), \n",
    "                                            vectorization_list, apply_smote = False)\n",
    "\n",
    "pd.DataFrame(LR_cw_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mycsvfile.csv','a') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(LR_cw_lemm.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame.from_dict(data= LR_cw_lemm).to_csv('dict_file.csv', header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression: compare vectorizers using stemming + class balances\n",
    "pd.DataFrame(wrapper_compare_vectorizations(X_train.stem_tweet, \n",
    "                            y_train, X_val.stem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', solver = 'lbfgs'),\n",
    "                            vectorization_list, apply_smote= True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "single_vector_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val,\n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1'), count_vect)\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "smote_vector_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                    LogisticRegression(class_weight='balanced', penalty = 'l1'), count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "single_vector_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                    LogisticRegression(class_weight='balanced', penalty = 'l1', random_state=1), count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "wrapper_single_vectorization(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1', random_state=1),\n",
    "                            count_vect, apply_smote = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Comparison Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = wrapper_compare_vectorizations(X_train_up.lem_tweet, \n",
    "                            y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(random_state =10, solver = 'lbfgs'),\n",
    "                            vectorization_list, apply_smote = False)\n",
    "\n",
    "pd.DataFrame(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = wrapper_compare_vectorizations(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1'), \n",
    "                            vectorization_list, apply_smote = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = wrapper_compare_vectorizations(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1'), \n",
    "                            vectorization_list, apply_smote = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = wrapper_compare_vectorizations(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            RandomForestClassifier(max_depth=10, random_state=1), \n",
    "                            vectorization_list, sampling= 'upsampling')\n",
    "pd.DataFrame(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "wrapper_single_vectorization2(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                            RandomForestClassifier(random_state=1), count_vect, sampling='upsample')\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed, X_val_transformed, y_train_pred, y_val_pred, y_val_predprob, confusion_matrix, metrics_dict, pred_df = \\\n",
    "\\\n",
    "wrapper_single_vectorization2(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(), count_vect, sampling='downsample')\n",
    "metrics_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions_test.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed, X_val_transformed, y_train_pred, y_val_pred, y_val_predprob, confusion_matrix, metrics_dict, pred_df = \\\n",
    "\\\n",
    "upsample_vector_model2(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(), count_vect)\n",
    "\n",
    "X_train_transformed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed, X_val_transformed, y_train_pred, y_val_pred, y_val_predprob, confusion_matrix, metrics_dict, pred_df = \\\n",
    "\\\n",
    "wrapper_single_vectorization2(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(), count_vect, sampling = 'upsample')\n",
    "\n",
    "X_train_transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Final Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = wrapper_compare_vectorizations(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1'), \n",
    "                            vectorization_list, sampling='help')\n",
    "pd.DataFrame(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "wrapper_single_vectorization(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            RandomForestClassifier(random_state=1), count_vect, apply_smote=False)\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "wrapper_single_vectorization(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            RandomForestClassifier(random_state=1), count_vect, apply_smote=True)\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "wrapper_single_vectorization(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            logreg, count_vect, apply_smote = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = logreg.decision_function(X_val_transformed)\n",
    "   \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_score)\n",
    "y_val_score = logreg.decision_function(X_val_transformed)\n",
    "val_fpr, val_tpr, thresholds = roc_curve(y_val, y_val_score)\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "plt.figure(figsize=(10,8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve for Validation Set')\n",
    "plt.legend(loc=\"lower right\")\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "plt.show()\n",
    "\n",
    "average_precision = average_precision_score(y_val, y_val_pred)\n",
    "\n",
    "print('Average precision-recall score RF: {}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_predictions_val = []\n",
    "for item in logreg.predict_proba(X_val_transformed):\n",
    "    if item[0] <= .85:\n",
    "        weighted_predictions_val.append(1)\n",
    "    else:\n",
    "        weighted_predictions_val.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with threshhold adjustment\n",
    "pd.DataFrame(confusion_matrix(y_val, weighted_predictions_val), index = ['actual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[pred_df['actual_class'] != pred_df['predicted_class']]\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['weighted_predictions'] = weighted_predictions_val\n",
    "pred_df[pred_df['actual_class'] != pred_df['weighted_predictions']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tweet[11418]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed, metrics_dict, train_confusion_matrix, y_test_pred, y_test_prob, test_df = \\\n",
    "\\\n",
    "wrapper_single_vectorization(X_train_up.lem_tweet, y_train_up, X_test.lem_tweet, y_test, \n",
    "                            logreg, count_vect, apply_smote = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First calculate the probability scores of each of the datapoints:\n",
    "y_val_score = model_log.decision_function(X_val)\n",
    "   \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_val_score)\n",
    "y_train_score = model_log.decision_function(X_train)\n",
    "train_fpr, train_tpr, thresholds = roc_curve(y_train, y_train_score)\n",
    "\n",
    "#plot curve\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "plt.figure(figsize=(10,8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve for Test Set')\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "plt.show()\n",
    "\n",
    "average_precision = average_precision_score(y_test, y_test_pred)\n",
    "\n",
    "print('Average precision-recall score RF: {}'.format(average_precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = logreg.decision_function(X_train)\n",
    "   \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "y_test_score = logreg.decision_function(X_train_transformed)\n",
    "test_fpr, test_tpr, thresholds = roc_curve(y_test, y_test_score)\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "plt.figure(figsize=(10,8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve for Validation Set')\n",
    "plt.legend(loc=\"lower right\")\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "plt.show()\n",
    "\n",
    "average_precision = average_precision_score(y_test, y_test_pred)\n",
    "\n",
    "print('Average precision-recall score RF: {}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_predictions_test = []\n",
    "for item in logreg.predict_proba(X_train_transformed):\n",
    "    if item[0] <= .85:\n",
    "        weighted_predictions_test.append(1)\n",
    "    else:\n",
    "        weighted_predictions_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(y_test, weighted_predictions_test), index = ['actual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class weight = balanced + lemmatized\n",
    "svm_metrics_balance, svm_X_train_transformed, svm_X_val_transformed = \\\n",
    "wrapper_compare_vectorizations(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                   SVC(class_weight ='balanced', gamma='auto', random_state = 10), vectorization_list, apply_smote=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(svm_metrics_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE + lemmatized \n",
    "svm_metrics_smote, svm_X_train_smote, svm_X_val_smote   = \\\n",
    "wrapper_compare_vectorizations(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                   SVC(class_weight ='balanced', gamma='auto', random_state = 10), vectorization_list, apply_smote=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(svm_metrics_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upsampling + lemmatized\n",
    "svm_metrics_up, svm_X_train_up, svm_X_val_up = \\\n",
    "wrapper_compare_vectorizations(X_train_up.lem_tweet, y_train, X_val_up.lem_tweet, y_val, \n",
    "                                   SVC(gamma='auto', random_state = 10), vectorization_list, apply_smote=False)\n",
    "pd.DataFrame(svm_metrics_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Searching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_compare_vectorizations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfid2 =  tfidf_ngram2.fit_transform(X_train_up.lemmatized_tweet)\n",
    "X_val_tfid2 =  tfidf_ngram2.transform(X_val.lemmatized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = SVC(kernel='linear', C=1, gamma=1, class_weight ='balanced')\n",
    "\n",
    "params = {\n",
    "'C': [0.1,.2, .3, 0.8,1,1.2,1.4],\n",
    "'kernel':['linear', 'rbf'],\n",
    "'gamma' :[0.1,0.8,1,1.2,1.4]\n",
    "}\n",
    "\n",
    "svm_gs= GridSearchCV(svc, param_grid = params, cv = 3)\n",
    "\n",
    "scores = ['f1','accuracy','recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_gs.fit(X_train_tfid2, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_vector_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vectorization_model(X_train_up.lemmatized_tweet, y_train_up, X_val.lemmatized_tweet, y_val, \n",
    "                                   SVC(C=1.2, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma=1.4, kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Multiple Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with class weight balances + lemmatizing \n",
    "rfc_metrics_bal = \\\n",
    "wrapper_compare_vectorizations(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 10, \n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10), \n",
    "                                   vectorization_list, apply_smote=False)\n",
    "pd.DataFrame(rfc_metrics_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with upsampling + lemmatizing \n",
    "rfc_metrics_up = \\\n",
    "wrapper_compare_vectorizations(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 20, \n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10), \n",
    "                                   vectorization_list, apply_smote=False)\n",
    "pd.DataFrame(rfc_metrics_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with SMOTE + lemmatizing  \n",
    "rfc_metrics_smote  = \\\n",
    "wrapper_compare_vectorizations(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 20, \n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10), \n",
    "                                   vectorization_list, apply_smote=False)\n",
    "pd.DataFrame(rfc_metrics_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "single_vector_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                    RandomForestClassifier(max_depth= 20, \n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10), count_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-Searching For Best Fit for Count Vectorizer + Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# rfc = RandomForestClassifier(n_estimators=60, max_depth=6, random_state=10, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countvect =  count_vect.fit_transform(X_train.lem_tweet)\n",
    "X_val_countvect =  count_vect.transform(X_val.lem_tweet)\n",
    "X_test_countvect = count_vect.transform(X_test.lem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=10)\n",
    "\n",
    "parameters = {'n_estimators' : [40, 60, 80, 100],\n",
    "'max_leaf_nodes' : [200, 400, 600],\n",
    "'random_state' : [10],\n",
    "'max_depth': [5, 7, 10, 20],\n",
    " 'verbose' : [0],\n",
    "'class_weight': ['balanced', 'balanced_subsample']}\n",
    "          \n",
    "rfc_gs = GridSearchCV(rfc, param_grid=parameters, cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.fit(X_train_countvect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc2 = RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
    "                       criterion='gini', max_depth=20, max_features='auto',\n",
    "                       max_leaf_nodes=200, min_impurity_decrease=0.0,\n",
    "                       min_impurity_split=None, min_samples_leaf=1,\n",
    "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                       n_estimators=100, n_jobs=None, oob_score=False,\n",
    "                       random_state=10, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc2.fit(X_train_countvect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict = rfc2.predict(X_train_countvect)\n",
    "metrics.f1_score(y_train, y_train_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_predict = rfc2.predict(X_val_countvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_val, y_val_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_feature_importances(rfc2):\n",
    "    n_features = X_val_countvect.shape[1]\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.barh(range(n_features), rfc2.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), countvect.values) \n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "\n",
    "plot_feature_importances(rfc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.tokenized_tweet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train.tokenized_tweet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X-train pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.tokenized_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train['tokenized_tweet']= X_train['tokenized_tweet'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_list = list(X_train.tokenized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_token_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_sumlist = sum(X_train_token_list,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unique_tokens = set(X_train_token_sumlist)\n",
    "print('The unique number of words in the training dataset is: {}'.format(len(X_train_unique_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X-val pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_token_list = list(X_val['tokenized_tweet'])\n",
    "X_val_token_sumlist = sum(X_val_token_list,[])\n",
    "X_val_unique_tokens = set(X_val_token_sumlist)\n",
    "\n",
    "print('The unique number of words in the validation dataset is: {}'.format(len(X_val_unique_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X-test pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_token_list = list(X_test['tokenized_tweet'])\n",
    "X_test_token_sumlist = sum(X_test_token_list,[])\n",
    "\n",
    "X_test_unique_tokens = set(X_test_token_sumlist)\n",
    "print('The unique number of words in the training dataset is: {}'.format(len(X_test_unique_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "t = time()\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(X_train_token_list, sg=1, min_count=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.train(X_train_token_list, total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save('data/w2v.model')\n",
    "\n",
    "w2v = gensim.models.Word2Vec.load('data/w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w2v.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocab= w2v.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv['trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(['trump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(['racist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(positive=['lazy','black'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.get_keras_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_X = w2v.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = X_train_token_list[1]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = np.empty((20455, 100))\n",
    "for sentence in X_train_token_list:\n",
    "    np.append(X_train_w2v, np.mean([w2v[w] for w in sentence if w in w2v]\n",
    "                   or [np.zeros(100)], axis=0))\n",
    "\n",
    "X_val_w2v = np.empty((5114, 100))\n",
    "for sentence in X_val_token_list:\n",
    "    np.append(X_val_w2v, np.mean([w2v[w] for w in sentence if w in w2v]\n",
    "                   or [np.zeros(100)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create average vector for train and test from model\n",
    "#returned list of numpy arrays are then stacked \n",
    "\n",
    "X_train_w2v_2 = np.concatenate([avg_word_vectors(word, w2v) for word in X_train_token_list])\n",
    "\n",
    "X_val_w2v_2 = np.concatenate([avg_word_vectors(word, w2v) for word in X_val_token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v_2[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model(X_train_w2v, y_train, X_val_w2v, y_val, LogisticRegression(solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model(X_train_w2v_2, y_train, X_val_w2v_2, y_val, LogisticRegression(solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model(X_train_w2v_2, y_train, X_val_w2v_2, y_val, RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# glove_input_file = 'data/glove.twitter.27B.100d.txt'\n",
    "# glove_output_file = 'data/glove.txt.word2vec'\n",
    "# glove2word2vec(glove_input_file, glove_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = KeyedVectors.load_word2vec_format('data/glove.txt.word2vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar('black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar('black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove2 = np.empty((20455, 100))\n",
    "for sentence in X_train_token_list:\n",
    "    np.append(X_train_glove2, np.mean([glove_model[w] for w in sentence if w in glove_model]\n",
    "                   or [np.zeros(100)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove = np.concatenate([avg_word_vectors(w, glove_model) for w in X_train_token_list])\n",
    "X_val_glove = np.concatenate([avg_word_vectors(w, glove_model) for w in X_val_token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v_2[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove[255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, LogisticRegression (class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Testing Scraped Trump Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trump_df= pd.read_csv('data/cleaned-trump-tweet.csv')\n",
    "trump_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countvect =  count_vect.fit_transform(X_train_up.lem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train_countvect, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trump = count_vect.transform(trump_df.lem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trump = X_trump.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trump.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trump_predict = logreg.predict(X_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trump_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df['predictions'] = y_trump_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trump_predict_prob = logreg.predict_proba(X_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trump_predict_prob = pd.DataFrame(y_trump_predict_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df['predict_probability'] = y_trump_predict_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df = trump_df[['tweet','predictions', 'predict_probability']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump0 = trump_df[trump_df.predictions == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump0.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df[trump_df.predictions == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump1 = trump_df[trump_df.predictions == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump1.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
