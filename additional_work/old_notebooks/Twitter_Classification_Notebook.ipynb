{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import sklearn\n",
    "\n",
    "# NLTK/NLP\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import string, re\n",
    "import urllib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from nltk.collocations import *\n",
    "import gensim\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Classifiers \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Sampling\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import sklearn.decomposition as decomposition\n",
    "\n",
    "#Visualization\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import csv\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.metrics import average_precision_score, auc, roc_curve, precision_recall_curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/cleaned-reshuffled.csv')\n",
    "# df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "# df.lem_tweet= df.lem_tweet.apply(str)\n",
    "# df.stem_tweet= df.stem_tweet.apply(str)\n",
    "# df.tokenized_tweet.apply(eval)\n",
    "# df.stemmed_tokens.apply(eval)\n",
    "# df.lemmatized_tokens.apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cleaned-reshuffled.pkl', 'rb') as f:\n",
    "\tdf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>no_hash_tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lem_tweet</th>\n",
       "      <th>stem_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29727</td>\n",
       "      <td>0</td>\n",
       "      <td>sad to see the scenes of hooligans pre #engrus...</td>\n",
       "      <td>sad to see the scenes of hooligans pre #engrus...</td>\n",
       "      <td>sad to see the scenes of hooligans pre engrus ...</td>\n",
       "      <td>[sad, to, see, the, scenes, of, hooligans, pre...</td>\n",
       "      <td>[sad, to, see, the, scene, of, hooligan, pre, ...</td>\n",
       "      <td>[sad, to, see, the, scene, of, hooligan, pre, ...</td>\n",
       "      <td>sad to see the scenes of hooligans pre engrus ...</td>\n",
       "      <td>sad to see the scenes of hooligans pre engrus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14466</td>\n",
       "      <td>0</td>\n",
       "      <td>#gooddyeyoung #yoyoyo  !! super happy to be ap...</td>\n",
       "      <td>#gooddyeyoung #yoyoyo super happy to be apa of...</td>\n",
       "      <td>gooddyeyoung yoyoyo super happy to be apa of t...</td>\n",
       "      <td>[gooddyeyoung, yoyoyo, super, happy, to, be, a...</td>\n",
       "      <td>[gooddyeyoung, yoyoyo, super, happi, to, be, a...</td>\n",
       "      <td>[gooddyeyoung, yoyoyo, super, happy, to, be, a...</td>\n",
       "      <td>gooddyeyoung yoyoyo super happy to be apa of t...</td>\n",
       "      <td>gooddyeyoung yoyoyo super happy to be apa of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18194</td>\n",
       "      <td>0</td>\n",
       "      <td>queen evil's bihdayð#lnic #lnicjustanevilbd...</td>\n",
       "      <td>queen evil s bihday #lnic #lnicjustanevilbday ...</td>\n",
       "      <td>queen evil s bihday lnic lnicjustanevilbday bi...</td>\n",
       "      <td>[queen, evil, s, bihday, lnic, lnicjustanevilb...</td>\n",
       "      <td>[queen, evil, s, bihday, lnic, lnicjustanevilb...</td>\n",
       "      <td>[queen, evil, s, bihday, lnic, lnicjustanevilb...</td>\n",
       "      <td>queen evil s bihday lnic lnicjustanevilbday bi...</td>\n",
       "      <td>queen evil s bihday lnic lnicjustanevilbday bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18283</td>\n",
       "      <td>1</td>\n",
       "      <td>@user you might be a libtard if... #libtard  #...</td>\n",
       "      <td>you might be a libtard if #libtard #sjw #liber...</td>\n",
       "      <td>you might be a libtard if libtard sjw liberal ...</td>\n",
       "      <td>[you, might, be, a, libtard, if, libtard, sjw,...</td>\n",
       "      <td>[you, might, be, a, libtard, if, libtard, sjw,...</td>\n",
       "      <td>[you, might, be, a, libtard, if, libtard, sjw,...</td>\n",
       "      <td>you might be a libtard if libtard sjw liberal ...</td>\n",
       "      <td>you might be a libtard if libtard sjw liberal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25845</td>\n",
       "      <td>0</td>\n",
       "      <td>what are your goals? find out here...   #smile...</td>\n",
       "      <td>what are your goals find out here #smile</td>\n",
       "      <td>what are your goals find out here smile</td>\n",
       "      <td>[what, are, your, goals, find, out, here, smile]</td>\n",
       "      <td>[what, are, your, goal, find, out, here, smile]</td>\n",
       "      <td>[what, are, your, goal, find, out, here, smile]</td>\n",
       "      <td>what are your goals find out here smile</td>\n",
       "      <td>what are your goals find out here smil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label                                              tweet  \\\n",
       "0  29727      0  sad to see the scenes of hooligans pre #engrus...   \n",
       "1  14466      0  #gooddyeyoung #yoyoyo  !! super happy to be ap...   \n",
       "2  18194      0  queen evil's bihdayð#lnic #lnicjustanevilbd...   \n",
       "3  18283      1  @user you might be a libtard if... #libtard  #...   \n",
       "4  25845      0  what are your goals? find out here...   #smile...   \n",
       "\n",
       "                                          tidy_tweet  \\\n",
       "0  sad to see the scenes of hooligans pre #engrus...   \n",
       "1  #gooddyeyoung #yoyoyo super happy to be apa of...   \n",
       "2  queen evil s bihday #lnic #lnicjustanevilbday ...   \n",
       "3  you might be a libtard if #libtard #sjw #liber...   \n",
       "4           what are your goals find out here #smile   \n",
       "\n",
       "                                       no_hash_tweet  \\\n",
       "0  sad to see the scenes of hooligans pre engrus ...   \n",
       "1  gooddyeyoung yoyoyo super happy to be apa of t...   \n",
       "2  queen evil s bihday lnic lnicjustanevilbday bi...   \n",
       "3  you might be a libtard if libtard sjw liberal ...   \n",
       "4            what are your goals find out here smile   \n",
       "\n",
       "                                     tokenized_tweet  \\\n",
       "0  [sad, to, see, the, scenes, of, hooligans, pre...   \n",
       "1  [gooddyeyoung, yoyoyo, super, happy, to, be, a...   \n",
       "2  [queen, evil, s, bihday, lnic, lnicjustanevilb...   \n",
       "3  [you, might, be, a, libtard, if, libtard, sjw,...   \n",
       "4   [what, are, your, goals, find, out, here, smile]   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  [sad, to, see, the, scene, of, hooligan, pre, ...   \n",
       "1  [gooddyeyoung, yoyoyo, super, happi, to, be, a...   \n",
       "2  [queen, evil, s, bihday, lnic, lnicjustanevilb...   \n",
       "3  [you, might, be, a, libtard, if, libtard, sjw,...   \n",
       "4    [what, are, your, goal, find, out, here, smile]   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  [sad, to, see, the, scene, of, hooligan, pre, ...   \n",
       "1  [gooddyeyoung, yoyoyo, super, happy, to, be, a...   \n",
       "2  [queen, evil, s, bihday, lnic, lnicjustanevilb...   \n",
       "3  [you, might, be, a, libtard, if, libtard, sjw,...   \n",
       "4    [what, are, your, goal, find, out, here, smile]   \n",
       "\n",
       "                                           lem_tweet  \\\n",
       "0  sad to see the scenes of hooligans pre engrus ...   \n",
       "1  gooddyeyoung yoyoyo super happy to be apa of t...   \n",
       "2  queen evil s bihday lnic lnicjustanevilbday bi...   \n",
       "3  you might be a libtard if libtard sjw liberal ...   \n",
       "4            what are your goals find out here smile   \n",
       "\n",
       "                                          stem_tweet  \n",
       "0  sad to see the scenes of hooligans pre engrus ...  \n",
       "1  gooddyeyoung yoyoyo super happy to be apa of t...  \n",
       "2  queen evil s bihday lnic lnicjustanevilbday bi...  \n",
       "3  you might be a libtard if libtard sjw liberal ...  \n",
       "4             what are your goals find out here smil  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Val / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['label'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into train and test \n",
    "X_model, X_test, y_model, y_test = train_test_split(X, y, stratify = y,  test_size=0.20, random_state=123)\n",
    "\n",
    "#splitting \"model\" into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_model, y_model, test_size=0.20, random_state=123)\n",
    "\n",
    "# df_train_full = X_train.copy()\n",
    "# df_train_full['label']= y_train\n",
    "# train_full_df.to_csv('train_full_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.929854\n",
       "1    0.070146\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling and Downsampling Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>no_hash_tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lem_tweet</th>\n",
       "      <th>stem_tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20676</td>\n",
       "      <td>@user f*** this  ð¦ðº government that deli...</td>\n",
       "      <td>f this government that deliberately toures #re...</td>\n",
       "      <td>f this government that deliberately toures ref...</td>\n",
       "      <td>[f, this, government, that, deliberately, tour...</td>\n",
       "      <td>[f, this, govern, that, deliber, tour, refuge,...</td>\n",
       "      <td>[f, this, government, that, deliberately, tour...</td>\n",
       "      <td>f this government that deliberately toures ref...</td>\n",
       "      <td>f this government that deliberately toures ref...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21531</th>\n",
       "      <td>24025</td>\n",
       "      <td>despite a demoralizing 2016: may ur #newyear20...</td>\n",
       "      <td>despite a demoralizing may ur #newyear be #cla...</td>\n",
       "      <td>despite a demoralizing may ur newyear be class...</td>\n",
       "      <td>[despite, a, demoralizing, may, ur, newyear, b...</td>\n",
       "      <td>[despit, a, demor, may, ur, newyear, be, class...</td>\n",
       "      <td>[despite, a, demoralizing, may, ur, newyear, b...</td>\n",
       "      <td>despite a demoralizing may ur newyear be class...</td>\n",
       "      <td>despite a demoralizing may ur newyear be class...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13300</th>\n",
       "      <td>18145</td>\n",
       "      <td>@user #koreans &amp;amp; joseon people in japan, w...</td>\n",
       "      <td>#koreans amp joseon people in japan will abuse...</td>\n",
       "      <td>koreans amp joseon people in japan will abuse ...</td>\n",
       "      <td>[koreans, amp, joseon, people, in, japan, will...</td>\n",
       "      <td>[korean, amp, joseon, peopl, in, japan, will, ...</td>\n",
       "      <td>[korean, amp, joseon, people, in, japan, will,...</td>\n",
       "      <td>koreans amp joseon people in japan will abuse ...</td>\n",
       "      <td>koreans amp joseon people in japan will abuse ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18925</th>\n",
       "      <td>8506</td>\n",
       "      <td>@user @user @user @user classic ! yet you jewi...</td>\n",
       "      <td>classic yet you jewish bastards wonder why you...</td>\n",
       "      <td>classic yet you jewish bastards wonder why you...</td>\n",
       "      <td>[classic, yet, you, jewish, bastards, wonder, ...</td>\n",
       "      <td>[classic, yet, you, jewish, bastard, wonder, w...</td>\n",
       "      <td>[classic, yet, you, jewish, bastard, wonder, w...</td>\n",
       "      <td>classic yet you jewish bastards wonder why you...</td>\n",
       "      <td>classic yet you jewish bastards wonder why you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12619</th>\n",
       "      <td>15464</td>\n",
       "      <td>@user did someone say #antisemetic ? gee (((@u...</td>\n",
       "      <td>did someone say #antisemetic gee you re a bit ...</td>\n",
       "      <td>did someone say antisemetic gee you re a bit t...</td>\n",
       "      <td>[did, someone, say, antisemetic, gee, you, re,...</td>\n",
       "      <td>[did, someon, say, antisemet, gee, you, re, a,...</td>\n",
       "      <td>[did, someone, say, antisemetic, gee, you, re,...</td>\n",
       "      <td>did someone say antisemetic gee you re a bit t...</td>\n",
       "      <td>did someone say antisemetic gee you re a bit t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26964</th>\n",
       "      <td>28937</td>\n",
       "      <td>couldn't have said this any better nor truthfu...</td>\n",
       "      <td>couldn t have said this any better nor truthfu...</td>\n",
       "      <td>couldn t have said this any better nor truthfu...</td>\n",
       "      <td>[couldn, t, have, said, this, any, better, nor...</td>\n",
       "      <td>[couldn, t, have, said, this, ani, better, nor...</td>\n",
       "      <td>[couldn, t, have, said, this, any, better, nor...</td>\n",
       "      <td>couldn t have said this any better nor truthfu...</td>\n",
       "      <td>couldn t have said this any better nor truthfu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17273</th>\n",
       "      <td>25291</td>\n",
       "      <td>@user racism stuffed into skinny jeans with a ...</td>\n",
       "      <td>racism stuffed into skinny jeans with a hipste...</td>\n",
       "      <td>racism stuffed into skinny jeans with a hipste...</td>\n",
       "      <td>[racism, stuffed, into, skinny, jeans, with, a...</td>\n",
       "      <td>[racism, stuf, into, skinni, jean, with, a, hi...</td>\n",
       "      <td>[racism, stuffed, into, skinny, jean, with, a,...</td>\n",
       "      <td>racism stuffed into skinny jeans with a hipste...</td>\n",
       "      <td>racism stuffed into skinny jeans with a hipste...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>12717</td>\n",
       "      <td>the end of   #me #selfie # #love #messi #cr7 #...</td>\n",
       "      <td>the end of #me #selfie # #love #messi #cr #rel...</td>\n",
       "      <td>the end of me selfie  love messi cr religion c...</td>\n",
       "      <td>[the, end, of, me, selfie, love, messi, cr, re...</td>\n",
       "      <td>[the, end, of, me, selfi, love, messi, cr, rel...</td>\n",
       "      <td>[the, end, of, me, selfie, love, messi, cr, re...</td>\n",
       "      <td>the end of me selfie  love messi cr religion c...</td>\n",
       "      <td>the end of me selfie  love messi cr religion c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17875</th>\n",
       "      <td>11612</td>\n",
       "      <td>trump ally wishes mad cow disease death for ob...</td>\n",
       "      <td>trump ally wishes mad cow disease death for ob...</td>\n",
       "      <td>trump ally wishes mad cow disease death for ob...</td>\n",
       "      <td>[trump, ally, wishes, mad, cow, disease, death...</td>\n",
       "      <td>[trump, alli, wish, mad, cow, diseas, death, f...</td>\n",
       "      <td>[trump, ally, wish, mad, cow, disease, death, ...</td>\n",
       "      <td>trump ally wishes mad cow disease death for ob...</td>\n",
       "      <td>trump ally wishes mad cow disease death for ob...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17184</th>\n",
       "      <td>20554</td>\n",
       "      <td>opinion:  is rife in the #lgbt community. #gay...</td>\n",
       "      <td>opinion is rife in the #lgbt community #gay pe...</td>\n",
       "      <td>opinion is rife in the lgbt community gay peop...</td>\n",
       "      <td>[opinion, is, rife, in, the, lgbt, community, ...</td>\n",
       "      <td>[opinion, is, rife, in, the, lgbt, communiti, ...</td>\n",
       "      <td>[opinion, is, rife, in, the, lgbt, community, ...</td>\n",
       "      <td>opinion is rife in the lgbt community gay peop...</td>\n",
       "      <td>opinion is rife in the lgbt community gay peop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8573</th>\n",
       "      <td>25151</td>\n",
       "      <td>@user #allahsoil the cold war was fought over ...</td>\n",
       "      <td>#allahsoil the cold war was fought over oil #t...</td>\n",
       "      <td>allahsoil the cold war was fought over oil tea...</td>\n",
       "      <td>[allahsoil, the, cold, war, was, fought, over,...</td>\n",
       "      <td>[allahsoil, the, cold, war, was, fought, over,...</td>\n",
       "      <td>[allahsoil, the, cold, war, wa, fought, over, ...</td>\n",
       "      <td>allahsoil the cold war was fought over oil tea...</td>\n",
       "      <td>allahsoil the cold war was fought over oil tea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26779</th>\n",
       "      <td>13585</td>\n",
       "      <td>omg, these trump suppoers are deplorable! #dum...</td>\n",
       "      <td>omg these trump suppoers are deplorable #dumpt...</td>\n",
       "      <td>omg these trump suppoers are deplorable dumptr...</td>\n",
       "      <td>[omg, these, trump, suppoers, are, deplorable,...</td>\n",
       "      <td>[omg, these, trump, suppoer, are, deplor, dump...</td>\n",
       "      <td>[omg, these, trump, suppoers, are, deplorable,...</td>\n",
       "      <td>omg these trump suppoers are deplorable dumptr...</td>\n",
       "      <td>omg these trump suppoers are deplorable dumptr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15332</th>\n",
       "      <td>2581</td>\n",
       "      <td>sea shepherd suppoers are racist!   #antiracis...</td>\n",
       "      <td>sea shepherd suppoers are racist #antiracism #...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>[sea, shepherd, suppoers, are, racist, antirac...</td>\n",
       "      <td>[sea, shepherd, suppoer, are, racist, antirac,...</td>\n",
       "      <td>[sea, shepherd, suppoers, are, racist, antirac...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12290</th>\n",
       "      <td>6519</td>\n",
       "      <td>.@user @user while @user can use phrases like ...</td>\n",
       "      <td>while can use phrases like #sandniggers is acc...</td>\n",
       "      <td>while can use phrases like sandniggers is acce...</td>\n",
       "      <td>[while, can, use, phrases, like, sandniggers, ...</td>\n",
       "      <td>[while, can, use, phrase, like, sandnigg, is, ...</td>\n",
       "      <td>[while, can, use, phrase, like, sandniggers, i...</td>\n",
       "      <td>while can use phrases like sandniggers is acce...</td>\n",
       "      <td>while can use phrases like sandniggers is acce...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24159</th>\n",
       "      <td>9563</td>\n",
       "      <td>@user #allahsoil enlightenment is wasted on th...</td>\n",
       "      <td>#allahsoil enlightenment is wasted on the wilf...</td>\n",
       "      <td>allahsoil enlightenment is wasted on the wilfu...</td>\n",
       "      <td>[allahsoil, enlightenment, is, wasted, on, the...</td>\n",
       "      <td>[allahsoil, enlighten, is, wast, on, the, wil,...</td>\n",
       "      <td>[allahsoil, enlightenment, is, wasted, on, the...</td>\n",
       "      <td>allahsoil enlightenment is wasted on the wilfu...</td>\n",
       "      <td>allahsoil enlightenment is wasted on the wilfu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25895</th>\n",
       "      <td>8452</td>\n",
       "      <td>@user here comes a  #supermistict douchebag wh...</td>\n",
       "      <td>here comes a #supermistict douchebag who can o...</td>\n",
       "      <td>here comes a supermistict douchebag who can on...</td>\n",
       "      <td>[here, comes, a, supermistict, douchebag, who,...</td>\n",
       "      <td>[here, come, a, supermistict, douchebag, who, ...</td>\n",
       "      <td>[here, come, a, supermistict, douchebag, who, ...</td>\n",
       "      <td>here comes a supermistict douchebag who can on...</td>\n",
       "      <td>here comes a supermistict douchebag who can on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23934</th>\n",
       "      <td>17750</td>\n",
       "      <td>@user hidden  in #america is as rampant as bla...</td>\n",
       "      <td>hidden in #america is as rampant as blatant ra...</td>\n",
       "      <td>hidden in america is as rampant as blatant racism</td>\n",
       "      <td>[hidden, in, america, is, as, rampant, as, bla...</td>\n",
       "      <td>[hidden, in, america, is, as, rampant, as, bla...</td>\n",
       "      <td>[hidden, in, america, is, a, rampant, a, blata...</td>\n",
       "      <td>hidden in america is as rampant as blatant racism</td>\n",
       "      <td>hidden in america is as rampant as blatant rac</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29174</th>\n",
       "      <td>21389</td>\n",
       "      <td>will the alt-right promote a new kind of  gene...</td>\n",
       "      <td>will the alt right promote a new kind of genet...</td>\n",
       "      <td>will the alt right promote a new kind of genet...</td>\n",
       "      <td>[will, the, alt, right, promote, a, new, kind,...</td>\n",
       "      <td>[will, the, alt, right, promot, a, new, kind, ...</td>\n",
       "      <td>[will, the, alt, right, promote, a, new, kind,...</td>\n",
       "      <td>will the alt right promote a new kind of genet...</td>\n",
       "      <td>will the alt right promote a new kind of genet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10474</th>\n",
       "      <td>23431</td>\n",
       "      <td>#us why weneed #empathy #ageoftrump #grassroot...</td>\n",
       "      <td>#us why weneed #empathy #ageoftrump #grassroot...</td>\n",
       "      <td>us why weneed empathy ageoftrump grassrootsact...</td>\n",
       "      <td>[us, why, weneed, empathy, ageoftrump, grassro...</td>\n",
       "      <td>[us, whi, wene, empathi, ageoftrump, grassroot...</td>\n",
       "      <td>[u, why, weneed, empathy, ageoftrump, grassroo...</td>\n",
       "      <td>us why weneed empathy ageoftrump grassrootsact...</td>\n",
       "      <td>us why weneed empathy ageoftrump grassrootsact...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30743</th>\n",
       "      <td>12301</td>\n",
       "      <td>black trump suppoer smacks down cnn repoer for...</td>\n",
       "      <td>black trump suppoer smacks down cnn repoer for...</td>\n",
       "      <td>black trump suppoer smacks down cnn repoer for...</td>\n",
       "      <td>[black, trump, suppoer, smacks, down, cnn, rep...</td>\n",
       "      <td>[black, trump, suppoer, smack, down, cnn, repo...</td>\n",
       "      <td>[black, trump, suppoer, smack, down, cnn, repo...</td>\n",
       "      <td>black trump suppoer smacks down cnn repoer for...</td>\n",
       "      <td>black trump suppoer smacks down cnn repoer for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6190</th>\n",
       "      <td>19407</td>\n",
       "      <td>so sick of all the pre-programmed #hillbots rh...</td>\n",
       "      <td>so sick of all the pre programmed #hillbots rh...</td>\n",
       "      <td>so sick of all the pre programmed hillbots rhe...</td>\n",
       "      <td>[so, sick, of, all, the, pre, programmed, hill...</td>\n",
       "      <td>[so, sick, of, all, the, pre, program, hillbot...</td>\n",
       "      <td>[so, sick, of, all, the, pre, programmed, hill...</td>\n",
       "      <td>so sick of all the pre programmed hillbots rhe...</td>\n",
       "      <td>so sick of all the pre programmed hillbots rhe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19746</th>\n",
       "      <td>406</td>\n",
       "      <td>@user when you're blocked by a  troll because ...</td>\n",
       "      <td>when you re blocked by a troll because you pro...</td>\n",
       "      <td>when you re blocked by a troll because you pro...</td>\n",
       "      <td>[when, you, re, blocked, by, a, troll, because...</td>\n",
       "      <td>[when, you, re, block, by, a, troll, becaus, y...</td>\n",
       "      <td>[when, you, re, blocked, by, a, troll, because...</td>\n",
       "      <td>when you re blocked by a troll because you pro...</td>\n",
       "      <td>when you re blocked by a troll because you pro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19581</th>\n",
       "      <td>11720</td>\n",
       "      <td>@user @user you forgot #democrate, because vet...</td>\n",
       "      <td>you forgot #democrate because vetting people f...</td>\n",
       "      <td>you forgot democrate because vetting people fr...</td>\n",
       "      <td>[you, forgot, democrate, because, vetting, peo...</td>\n",
       "      <td>[you, forgot, democr, becaus, vet, peopl, from...</td>\n",
       "      <td>[you, forgot, democrate, because, vetting, peo...</td>\n",
       "      <td>you forgot democrate because vetting people fr...</td>\n",
       "      <td>you forgot democrate because vetting people fr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31767</th>\n",
       "      <td>28476</td>\n",
       "      <td>sea shepherd suppoers are racist!   #antiracis...</td>\n",
       "      <td>sea shepherd suppoers are racist #antiracism #...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>[sea, shepherd, suppoers, are, racist, antirac...</td>\n",
       "      <td>[sea, shepherd, suppoer, are, racist, antirac,...</td>\n",
       "      <td>[sea, shepherd, suppoers, are, racist, antirac...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>sea shepherd suppoers are racist antiracism se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25210</th>\n",
       "      <td>24768</td>\n",
       "      <td>porn vids web free sex</td>\n",
       "      <td>porn vids web free sex</td>\n",
       "      <td>porn vids web free sex</td>\n",
       "      <td>[porn, vids, web, free, sex]</td>\n",
       "      <td>[porn, vid, web, free, sex]</td>\n",
       "      <td>[porn, vids, web, free, sex]</td>\n",
       "      <td>porn vids web free sex</td>\n",
       "      <td>porn vids web free sex</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19800</th>\n",
       "      <td>19745</td>\n",
       "      <td>@user &amp;amp; the  #democraticpay keeps telling ...</td>\n",
       "      <td>amp the #democraticpay keeps telling me that #...</td>\n",
       "      <td>amp the democraticpay keeps telling me that bl...</td>\n",
       "      <td>[amp, the, democraticpay, keeps, telling, me, ...</td>\n",
       "      <td>[amp, the, democraticpay, keep, tell, me, that...</td>\n",
       "      <td>[amp, the, democraticpay, keep, telling, me, t...</td>\n",
       "      <td>amp the democraticpay keeps telling me that bl...</td>\n",
       "      <td>amp the democraticpay keeps telling me that bl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7812</th>\n",
       "      <td>1242</td>\n",
       "      <td>@user although, i am not a , a #bigot or a #mi...</td>\n",
       "      <td>although i am not a a #bigot or a #misogynist ...</td>\n",
       "      <td>although i am not a a bigot or a misogynist so...</td>\n",
       "      <td>[although, i, am, not, a, a, bigot, or, a, mis...</td>\n",
       "      <td>[although, i, am, not, a, a, bigot, or, a, mis...</td>\n",
       "      <td>[although, i, am, not, a, a, bigot, or, a, mis...</td>\n",
       "      <td>although i am not a a bigot or a misogynist so...</td>\n",
       "      <td>although i am not a a bigot or a misogynist so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5074</th>\n",
       "      <td>23280</td>\n",
       "      <td>this is sooooo  or may be just funny</td>\n",
       "      <td>this is sooooo or may be just funny</td>\n",
       "      <td>this is sooooo or may be just funny</td>\n",
       "      <td>[this, is, sooooo, or, may, be, just, funny]</td>\n",
       "      <td>[this, is, sooooo, or, may, be, just, funni]</td>\n",
       "      <td>[this, is, sooooo, or, may, be, just, funny]</td>\n",
       "      <td>this is sooooo or may be just funny</td>\n",
       "      <td>this is sooooo or may be just funni</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6317</th>\n",
       "      <td>15050</td>\n",
       "      <td>@user âwe the peopleâ originally meant â...</td>\n",
       "      <td>we the people originally meant we the white la...</td>\n",
       "      <td>we the people originally meant we the white la...</td>\n",
       "      <td>[we, the, people, originally, meant, we, the, ...</td>\n",
       "      <td>[we, the, peopl, origin, meant, we, the, white...</td>\n",
       "      <td>[we, the, people, originally, meant, we, the, ...</td>\n",
       "      <td>we the people originally meant we the white la...</td>\n",
       "      <td>we the people originally meant we the white la...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5988</th>\n",
       "      <td>24766</td>\n",
       "      <td>i've no problem with #universities that teach ...</td>\n",
       "      <td>i ve no problem with #universities that teach ...</td>\n",
       "      <td>i ve no problem with universities that teach a...</td>\n",
       "      <td>[i, ve, no, problem, with, universities, that,...</td>\n",
       "      <td>[i, ve, no, problem, with, univers, that, teac...</td>\n",
       "      <td>[i, ve, no, problem, with, university, that, t...</td>\n",
       "      <td>i ve no problem with universities that teach a...</td>\n",
       "      <td>i ve no problem with universities that teach a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3721</th>\n",
       "      <td>17405</td>\n",
       "      <td>check out this new trending #funny #gif !  , p...</td>\n",
       "      <td>check out this new trending #funny #gif pixel ...</td>\n",
       "      <td>check out this new trending funny gif pixel ce...</td>\n",
       "      <td>[check, out, this, new, trending, funny, gif, ...</td>\n",
       "      <td>[check, out, this, new, trend, funni, gif, pix...</td>\n",
       "      <td>[check, out, this, new, trending, funny, gif, ...</td>\n",
       "      <td>check out this new trending funny gif pixel ce...</td>\n",
       "      <td>check out this new trending funny gif pixel ce...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28057</th>\n",
       "      <td>31774</td>\n",
       "      <td>this reminds me of this. i am   love these two...</td>\n",
       "      <td>this reminds me of this i am love these two th...</td>\n",
       "      <td>this reminds me of this i am love these two th...</td>\n",
       "      <td>[this, reminds, me, of, this, i, am, love, the...</td>\n",
       "      <td>[this, remind, me, of, this, i, am, love, thes...</td>\n",
       "      <td>[this, reminds, me, of, this, i, am, love, the...</td>\n",
       "      <td>this reminds me of this i am love these two th...</td>\n",
       "      <td>this reminds me of this i am love these two th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15044</th>\n",
       "      <td>5789</td>\n",
       "      <td>livelypics: just when you think you know peop...</td>\n",
       "      <td>livelypics just when you think you know people...</td>\n",
       "      <td>livelypics just when you think you know people...</td>\n",
       "      <td>[livelypics, just, when, you, think, you, know...</td>\n",
       "      <td>[livelyp, just, when, you, think, you, know, p...</td>\n",
       "      <td>[livelypics, just, when, you, think, you, know...</td>\n",
       "      <td>livelypics just when you think you know people...</td>\n",
       "      <td>livelypics just when you think you know people...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23222</th>\n",
       "      <td>29934</td>\n",
       "      <td>trying not to shut down but maybe #pokemon wil...</td>\n",
       "      <td>trying not to shut down but maybe #pokemon wil...</td>\n",
       "      <td>trying not to shut down but maybe pokemon will...</td>\n",
       "      <td>[trying, not, to, shut, down, but, maybe, poke...</td>\n",
       "      <td>[tri, not, to, shut, down, but, mayb, pokemon,...</td>\n",
       "      <td>[trying, not, to, shut, down, but, maybe, poke...</td>\n",
       "      <td>trying not to shut down but maybe pokemon will...</td>\n",
       "      <td>trying not to shut down but maybe pokemon will...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27273</th>\n",
       "      <td>395</td>\n",
       "      <td>#first #bihday to our #puppy #eloise #sweetbab...</td>\n",
       "      <td>#first #bihday to our #puppy #eloise #sweetbab...</td>\n",
       "      <td>first bihday to our puppy eloise sweetbabins d...</td>\n",
       "      <td>[first, bihday, to, our, puppy, eloise, sweetb...</td>\n",
       "      <td>[first, bihday, to, our, puppi, elois, sweetba...</td>\n",
       "      <td>[first, bihday, to, our, puppy, eloise, sweetb...</td>\n",
       "      <td>first bihday to our puppy eloise sweetbabins d...</td>\n",
       "      <td>first bihday to our puppy eloise sweetbabins d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30304</th>\n",
       "      <td>10458</td>\n",
       "      <td>@user it was in ceain areas yeah, you not seen...</td>\n",
       "      <td>it was in ceain areas yeah you not seen the vi...</td>\n",
       "      <td>it was in ceain areas yeah you not seen the vi...</td>\n",
       "      <td>[it, was, in, ceain, areas, yeah, you, not, se...</td>\n",
       "      <td>[it, was, in, ceain, area, yeah, you, not, see...</td>\n",
       "      <td>[it, wa, in, ceain, area, yeah, you, not, seen...</td>\n",
       "      <td>it was in ceain areas yeah you not seen the vi...</td>\n",
       "      <td>it was in ceain areas yeah you not seen the vi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3928</th>\n",
       "      <td>19081</td>\n",
       "      <td>so many shit talkers/bullies in the youtube co...</td>\n",
       "      <td>so many shit talkers bullies in the youtube co...</td>\n",
       "      <td>so many shit talkers bullies in the youtube co...</td>\n",
       "      <td>[so, many, shit, talkers, bullies, in, the, yo...</td>\n",
       "      <td>[so, mani, shit, talker, bulli, in, the, youtu...</td>\n",
       "      <td>[so, many, shit, talker, bully, in, the, youtu...</td>\n",
       "      <td>so many shit talkers bullies in the youtube co...</td>\n",
       "      <td>so many shit talkers bullies in the youtube co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>13429</td>\n",
       "      <td>all ready to pay xx #saturday #daughter #love ...</td>\n",
       "      <td>all ready to pay xx #saturday #daughter #love ...</td>\n",
       "      <td>all ready to pay xx saturday daughter love pay...</td>\n",
       "      <td>[all, ready, to, pay, xx, saturday, daughter, ...</td>\n",
       "      <td>[all, readi, to, pay, xx, saturday, daughter, ...</td>\n",
       "      <td>[all, ready, to, pay, xx, saturday, daughter, ...</td>\n",
       "      <td>all ready to pay xx saturday daughter love pay...</td>\n",
       "      <td>all ready to pay xx saturday daughter love pay...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9826</th>\n",
       "      <td>6122</td>\n",
       "      <td>feeling a little #mole tonight! ó¾« #food #fo...</td>\n",
       "      <td>feeling a little #mole tonight #food #foodblog...</td>\n",
       "      <td>feeling a little mole tonight food foodblogger...</td>\n",
       "      <td>[feeling, a, little, mole, tonight, food, food...</td>\n",
       "      <td>[feel, a, littl, mole, tonight, food, foodblog...</td>\n",
       "      <td>[feeling, a, little, mole, tonight, food, food...</td>\n",
       "      <td>feeling a little mole tonight food foodblogger...</td>\n",
       "      <td>feeling a little mole tonight food foodblogger...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11548</th>\n",
       "      <td>14426</td>\n",
       "      <td>while we're still trying to get over the shock...</td>\n",
       "      <td>while we re still trying to get over the shock...</td>\n",
       "      <td>while we re still trying to get over the shock...</td>\n",
       "      <td>[while, we, re, still, trying, to, get, over, ...</td>\n",
       "      <td>[while, we, re, still, tri, to, get, over, the...</td>\n",
       "      <td>[while, we, re, still, trying, to, get, over, ...</td>\n",
       "      <td>while we re still trying to get over the shock...</td>\n",
       "      <td>while we re still trying to get over the shock...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19491</th>\n",
       "      <td>19469</td>\n",
       "      <td>fantasy aisle ð #i #fantasy #aisle #summer ...</td>\n",
       "      <td>fantasy aisle #i #fantasy #aisle #summer #gran...</td>\n",
       "      <td>fantasy aisle i fantasy aisle summer grand vac...</td>\n",
       "      <td>[fantasy, aisle, i, fantasy, aisle, summer, gr...</td>\n",
       "      <td>[fantasi, aisl, i, fantasi, aisl, summer, gran...</td>\n",
       "      <td>[fantasy, aisle, i, fantasy, aisle, summer, gr...</td>\n",
       "      <td>fantasy aisle i fantasy aisle summer grand vac...</td>\n",
       "      <td>fantasy aisle i fantasy aisle summer grand vac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31522</th>\n",
       "      <td>17794</td>\n",
       "      <td>have a wonderful f r i d a y ð¸  #love #emik...</td>\n",
       "      <td>have a wonderful f r i d a y #love #emikagifts...</td>\n",
       "      <td>have a wonderful f r i d a y love emikagifts j...</td>\n",
       "      <td>[have, a, wonderful, f, r, i, d, a, y, love, e...</td>\n",
       "      <td>[have, a, wonder, f, r, i, d, a, y, love, emik...</td>\n",
       "      <td>[have, a, wonderful, f, r, i, d, a, y, love, e...</td>\n",
       "      <td>have a wonderful f r i d a y love emikagifts j...</td>\n",
       "      <td>have a wonderful f r i d a y love emikagifts j...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>11550</td>\n",
       "      <td>who wants him to be the next commander in chie...</td>\n",
       "      <td>who wants him to be the next commander in chie...</td>\n",
       "      <td>who wants him to be the next commander in chie...</td>\n",
       "      <td>[who, wants, him, to, be, the, next, commander...</td>\n",
       "      <td>[who, want, him, to, be, the, next, command, i...</td>\n",
       "      <td>[who, want, him, to, be, the, next, commander,...</td>\n",
       "      <td>who wants him to be the next commander in chie...</td>\n",
       "      <td>who wants him to be the next commander in chie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21399</th>\n",
       "      <td>18137</td>\n",
       "      <td>we beat that cock.   #beatit #penis</td>\n",
       "      <td>we beat that cock #beatit #penis</td>\n",
       "      <td>we beat that cock beatit penis</td>\n",
       "      <td>[we, beat, that, cock, beatit, penis]</td>\n",
       "      <td>[we, beat, that, cock, beatit, peni]</td>\n",
       "      <td>[we, beat, that, cock, beatit, penis]</td>\n",
       "      <td>we beat that cock beatit penis</td>\n",
       "      <td>we beat that cock beatit peni</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16625</th>\n",
       "      <td>14918</td>\n",
       "      <td>lik if u cri everi tim</td>\n",
       "      <td>lik if u cri everi tim</td>\n",
       "      <td>lik if u cri everi tim</td>\n",
       "      <td>[lik, if, u, cri, everi, tim]</td>\n",
       "      <td>[lik, if, u, cri, everi, tim]</td>\n",
       "      <td>[lik, if, u, cri, everi, tim]</td>\n",
       "      <td>lik if u cri everi tim</td>\n",
       "      <td>lik if u cri everi tim</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7084</th>\n",
       "      <td>12925</td>\n",
       "      <td>speakers dinner on eve of #ricsrural conferenc...</td>\n",
       "      <td>speakers dinner on eve of #ricsrural conferenc...</td>\n",
       "      <td>speakers dinner on eve of ricsrural conference...</td>\n",
       "      <td>[speakers, dinner, on, eve, of, ricsrural, con...</td>\n",
       "      <td>[speaker, dinner, on, eve, of, ricsrur, confer...</td>\n",
       "      <td>[speaker, dinner, on, eve, of, ricsrural, conf...</td>\n",
       "      <td>speakers dinner on eve of ricsrural conference...</td>\n",
       "      <td>speakers dinner on eve of ricsrural conference...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21332</th>\n",
       "      <td>1640</td>\n",
       "      <td>best facetime today; i can't wait to see my bo...</td>\n",
       "      <td>best facetime today i can t wait to see my boy...</td>\n",
       "      <td>best facetime today i can t wait to see my boy...</td>\n",
       "      <td>[best, facetime, today, i, can, t, wait, to, s...</td>\n",
       "      <td>[best, facetim, today, i, can, t, wait, to, se...</td>\n",
       "      <td>[best, facetime, today, i, can, t, wait, to, s...</td>\n",
       "      <td>best facetime today i can t wait to see my boy...</td>\n",
       "      <td>best facetime today i can t wait to see my boy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28564</th>\n",
       "      <td>18855</td>\n",
       "      <td>archery for year 5 at 1.30pm!   #archery</td>\n",
       "      <td>archery for year at pm #archery</td>\n",
       "      <td>archery for year at pm archery</td>\n",
       "      <td>[archery, for, year, at, pm, archery]</td>\n",
       "      <td>[archeri, for, year, at, pm, archeri]</td>\n",
       "      <td>[archery, for, year, at, pm, archery]</td>\n",
       "      <td>archery for year at pm archery</td>\n",
       "      <td>archery for year at pm archeri</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12504</th>\n",
       "      <td>23278</td>\n",
       "      <td>#whoolo in film you can have sad endings.  #an...</td>\n",
       "      <td>#whoolo in film you can have sad endings #anna...</td>\n",
       "      <td>whoolo in film you can have sad endings anna torv</td>\n",
       "      <td>[whoolo, in, film, you, can, have, sad, ending...</td>\n",
       "      <td>[whoolo, in, film, you, can, have, sad, end, a...</td>\n",
       "      <td>[whoolo, in, film, you, can, have, sad, ending...</td>\n",
       "      <td>whoolo in film you can have sad endings anna torv</td>\n",
       "      <td>whoolo in film you can have sad endings anna torv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22295</th>\n",
       "      <td>17561</td>\n",
       "      <td>@user @user at what time will the gates open?</td>\n",
       "      <td>at what time will the gates open</td>\n",
       "      <td>at what time will the gates open</td>\n",
       "      <td>[at, what, time, will, the, gates, open]</td>\n",
       "      <td>[at, what, time, will, the, gate, open]</td>\n",
       "      <td>[at, what, time, will, the, gate, open]</td>\n",
       "      <td>at what time will the gates open</td>\n",
       "      <td>at what time will the gates open</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13326</th>\n",
       "      <td>6135</td>\n",
       "      <td>thank you @user for a new #chargehr after mine...</td>\n",
       "      <td>thank you for a new #chargehr after mine broke...</td>\n",
       "      <td>thank you for a new chargehr after mine broke ...</td>\n",
       "      <td>[thank, you, for, a, new, chargehr, after, min...</td>\n",
       "      <td>[thank, you, for, a, new, chargehr, after, min...</td>\n",
       "      <td>[thank, you, for, a, new, chargehr, after, min...</td>\n",
       "      <td>thank you for a new chargehr after mine broke ...</td>\n",
       "      <td>thank you for a new chargehr after mine broke ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>21952</td>\n",
       "      <td>our thoughts and prayers goes out to everyone ...</td>\n",
       "      <td>our thoughts and prayers goes out to everyone ...</td>\n",
       "      <td>our thoughts and prayers goes out to everyone ...</td>\n",
       "      <td>[our, thoughts, and, prayers, goes, out, to, e...</td>\n",
       "      <td>[our, thought, and, prayer, goe, out, to, ever...</td>\n",
       "      <td>[our, thought, and, prayer, go, out, to, every...</td>\n",
       "      <td>our thoughts and prayers goes out to everyone ...</td>\n",
       "      <td>our thoughts and prayers goes out to everyone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19044</th>\n",
       "      <td>25937</td>\n",
       "      <td>#film   bull up: you will dominate your bull a...</td>\n",
       "      <td>#film bull up you will dominate your bull and ...</td>\n",
       "      <td>film bull up you will dominate your bull and y...</td>\n",
       "      <td>[film, bull, up, you, will, dominate, your, bu...</td>\n",
       "      <td>[film, bull, up, you, will, domin, your, bull,...</td>\n",
       "      <td>[film, bull, up, you, will, dominate, your, bu...</td>\n",
       "      <td>film bull up you will dominate your bull and y...</td>\n",
       "      <td>film bull up you will dominate your bull and y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31112</th>\n",
       "      <td>17229</td>\n",
       "      <td>this is horribly sad news. a fine actor with g...</td>\n",
       "      <td>this is horribly sad news a fine actor with gr...</td>\n",
       "      <td>this is horribly sad news a fine actor with gr...</td>\n",
       "      <td>[this, is, horribly, sad, news, a, fine, actor...</td>\n",
       "      <td>[this, is, horribl, sad, news, a, fine, actor,...</td>\n",
       "      <td>[this, is, horribly, sad, news, a, fine, actor...</td>\n",
       "      <td>this is horribly sad news a fine actor with gr...</td>\n",
       "      <td>this is horribly sad news a fine actor with gr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14188</th>\n",
       "      <td>7012</td>\n",
       "      <td>follow your #hea and be   â¤</td>\n",
       "      <td>follow your #hea and be</td>\n",
       "      <td>follow your hea and be</td>\n",
       "      <td>[follow, your, hea, and, be]</td>\n",
       "      <td>[follow, your, hea, and, be]</td>\n",
       "      <td>[follow, your, hea, and, be]</td>\n",
       "      <td>follow your hea and be</td>\n",
       "      <td>follow your hea and b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>31177</td>\n",
       "      <td>cops giving tickets when they didn't even see ...</td>\n",
       "      <td>cops giving tickets when they didn t even see ...</td>\n",
       "      <td>cops giving tickets when they didn t even see ...</td>\n",
       "      <td>[cops, giving, tickets, when, they, didn, t, e...</td>\n",
       "      <td>[cop, give, ticket, when, they, didn, t, even,...</td>\n",
       "      <td>[cop, giving, ticket, when, they, didn, t, eve...</td>\n",
       "      <td>cops giving tickets when they didn t even see ...</td>\n",
       "      <td>cops giving tickets when they didn t even see ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>23672</td>\n",
       "      <td>singapore city gallery   #riclswtravelbook #be...</td>\n",
       "      <td>singapore city gallery #riclswtravelbook #bear...</td>\n",
       "      <td>singapore city gallery riclswtravelbook bearlo...</td>\n",
       "      <td>[singapore, city, gallery, riclswtravelbook, b...</td>\n",
       "      <td>[singapor, citi, galleri, riclswtravelbook, be...</td>\n",
       "      <td>[singapore, city, gallery, riclswtravelbook, b...</td>\n",
       "      <td>singapore city gallery riclswtravelbook bearlo...</td>\n",
       "      <td>singapore city gallery riclswtravelbook bearlo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8230</th>\n",
       "      <td>19584</td>\n",
       "      <td>oh man, been waiting for #wehappyfew for so lo...</td>\n",
       "      <td>oh man been waiting for #wehappyfew for so lon...</td>\n",
       "      <td>oh man been waiting for wehappyfew for so long...</td>\n",
       "      <td>[oh, man, been, waiting, for, wehappyfew, for,...</td>\n",
       "      <td>[oh, man, been, wait, for, wehappyfew, for, so...</td>\n",
       "      <td>[oh, man, been, waiting, for, wehappyfew, for,...</td>\n",
       "      <td>oh man been waiting for wehappyfew for so long...</td>\n",
       "      <td>oh man been waiting for wehappyfew for so long...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15277</th>\n",
       "      <td>9967</td>\n",
       "      <td>schools almost over.</td>\n",
       "      <td>schools almost over</td>\n",
       "      <td>schools almost over</td>\n",
       "      <td>[schools, almost, over]</td>\n",
       "      <td>[school, almost, over]</td>\n",
       "      <td>[school, almost, over]</td>\n",
       "      <td>schools almost over</td>\n",
       "      <td>schools almost ov</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30396</th>\n",
       "      <td>2987</td>\n",
       "      <td>â #usd/cad bounces-off 0.2900, despite high...</td>\n",
       "      <td>#usd cad bounces off despite higher oil #blog ...</td>\n",
       "      <td>usd cad bounces off despite higher oil blog si...</td>\n",
       "      <td>[usd, cad, bounces, off, despite, higher, oil,...</td>\n",
       "      <td>[usd, cad, bounc, off, despit, higher, oil, bl...</td>\n",
       "      <td>[usd, cad, bounce, off, despite, higher, oil, ...</td>\n",
       "      <td>usd cad bounces off despite higher oil blog si...</td>\n",
       "      <td>usd cad bounces off despite higher oil blog si...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37982 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  \\\n",
       "565    20676  @user f*** this  ð¦ðº government that deli...   \n",
       "21531  24025  despite a demoralizing 2016: may ur #newyear20...   \n",
       "13300  18145  @user #koreans &amp; joseon people in japan, w...   \n",
       "18925   8506  @user @user @user @user classic ! yet you jewi...   \n",
       "12619  15464  @user did someone say #antisemetic ? gee (((@u...   \n",
       "26964  28937  couldn't have said this any better nor truthfu...   \n",
       "17273  25291  @user racism stuffed into skinny jeans with a ...   \n",
       "1561   12717  the end of   #me #selfie # #love #messi #cr7 #...   \n",
       "17875  11612  trump ally wishes mad cow disease death for ob...   \n",
       "17184  20554  opinion:  is rife in the #lgbt community. #gay...   \n",
       "8573   25151  @user #allahsoil the cold war was fought over ...   \n",
       "26779  13585  omg, these trump suppoers are deplorable! #dum...   \n",
       "15332   2581  sea shepherd suppoers are racist!   #antiracis...   \n",
       "12290   6519  .@user @user while @user can use phrases like ...   \n",
       "24159   9563  @user #allahsoil enlightenment is wasted on th...   \n",
       "25895   8452  @user here comes a  #supermistict douchebag wh...   \n",
       "23934  17750  @user hidden  in #america is as rampant as bla...   \n",
       "29174  21389  will the alt-right promote a new kind of  gene...   \n",
       "10474  23431  #us why weneed #empathy #ageoftrump #grassroot...   \n",
       "30743  12301  black trump suppoer smacks down cnn repoer for...   \n",
       "6190   19407  so sick of all the pre-programmed #hillbots rh...   \n",
       "19746    406  @user when you're blocked by a  troll because ...   \n",
       "19581  11720  @user @user you forgot #democrate, because vet...   \n",
       "31767  28476  sea shepherd suppoers are racist!   #antiracis...   \n",
       "25210  24768                             porn vids web free sex   \n",
       "19800  19745  @user &amp; the  #democraticpay keeps telling ...   \n",
       "7812    1242  @user although, i am not a , a #bigot or a #mi...   \n",
       "5074   23280               this is sooooo  or may be just funny   \n",
       "6317   15050  @user âwe the peopleâ originally meant â...   \n",
       "5988   24766  i've no problem with #universities that teach ...   \n",
       "...      ...                                                ...   \n",
       "3721   17405  check out this new trending #funny #gif !  , p...   \n",
       "28057  31774  this reminds me of this. i am   love these two...   \n",
       "15044   5789   livelypics: just when you think you know peop...   \n",
       "23222  29934  trying not to shut down but maybe #pokemon wil...   \n",
       "27273    395  #first #bihday to our #puppy #eloise #sweetbab...   \n",
       "30304  10458  @user it was in ceain areas yeah, you not seen...   \n",
       "3928   19081  so many shit talkers/bullies in the youtube co...   \n",
       "269    13429  all ready to pay xx #saturday #daughter #love ...   \n",
       "9826    6122  feeling a little #mole tonight! ó¾« #food #fo...   \n",
       "11548  14426  while we're still trying to get over the shock...   \n",
       "19491  19469  fantasy aisle ð #i #fantasy #aisle #summer ...   \n",
       "31522  17794  have a wonderful f r i d a y ð¸  #love #emik...   \n",
       "750    11550  who wants him to be the next commander in chie...   \n",
       "21399  18137                we beat that cock.   #beatit #penis   \n",
       "16625  14918                             lik if u cri everi tim   \n",
       "7084   12925  speakers dinner on eve of #ricsrural conferenc...   \n",
       "21332   1640  best facetime today; i can't wait to see my bo...   \n",
       "28564  18855           archery for year 5 at 1.30pm!   #archery   \n",
       "12504  23278  #whoolo in film you can have sad endings.  #an...   \n",
       "22295  17561      @user @user at what time will the gates open?   \n",
       "13326   6135  thank you @user for a new #chargehr after mine...   \n",
       "1147   21952  our thoughts and prayers goes out to everyone ...   \n",
       "19044  25937  #film   bull up: you will dominate your bull a...   \n",
       "31112  17229  this is horribly sad news. a fine actor with g...   \n",
       "14188   7012                      follow your #hea and be   â¤   \n",
       "1034   31177  cops giving tickets when they didn't even see ...   \n",
       "2874   23672  singapore city gallery   #riclswtravelbook #be...   \n",
       "8230   19584  oh man, been waiting for #wehappyfew for so lo...   \n",
       "15277   9967                               schools almost over.   \n",
       "30396   2987   â #usd/cad bounces-off 0.2900, despite high...   \n",
       "\n",
       "                                              tidy_tweet  \\\n",
       "565    f this government that deliberately toures #re...   \n",
       "21531  despite a demoralizing may ur #newyear be #cla...   \n",
       "13300  #koreans amp joseon people in japan will abuse...   \n",
       "18925  classic yet you jewish bastards wonder why you...   \n",
       "12619  did someone say #antisemetic gee you re a bit ...   \n",
       "26964  couldn t have said this any better nor truthfu...   \n",
       "17273  racism stuffed into skinny jeans with a hipste...   \n",
       "1561   the end of #me #selfie # #love #messi #cr #rel...   \n",
       "17875  trump ally wishes mad cow disease death for ob...   \n",
       "17184  opinion is rife in the #lgbt community #gay pe...   \n",
       "8573   #allahsoil the cold war was fought over oil #t...   \n",
       "26779  omg these trump suppoers are deplorable #dumpt...   \n",
       "15332  sea shepherd suppoers are racist #antiracism #...   \n",
       "12290  while can use phrases like #sandniggers is acc...   \n",
       "24159  #allahsoil enlightenment is wasted on the wilf...   \n",
       "25895  here comes a #supermistict douchebag who can o...   \n",
       "23934  hidden in #america is as rampant as blatant ra...   \n",
       "29174  will the alt right promote a new kind of genet...   \n",
       "10474  #us why weneed #empathy #ageoftrump #grassroot...   \n",
       "30743  black trump suppoer smacks down cnn repoer for...   \n",
       "6190   so sick of all the pre programmed #hillbots rh...   \n",
       "19746  when you re blocked by a troll because you pro...   \n",
       "19581  you forgot #democrate because vetting people f...   \n",
       "31767  sea shepherd suppoers are racist #antiracism #...   \n",
       "25210                             porn vids web free sex   \n",
       "19800  amp the #democraticpay keeps telling me that #...   \n",
       "7812   although i am not a a #bigot or a #misogynist ...   \n",
       "5074                 this is sooooo or may be just funny   \n",
       "6317   we the people originally meant we the white la...   \n",
       "5988   i ve no problem with #universities that teach ...   \n",
       "...                                                  ...   \n",
       "3721   check out this new trending #funny #gif pixel ...   \n",
       "28057  this reminds me of this i am love these two th...   \n",
       "15044  livelypics just when you think you know people...   \n",
       "23222  trying not to shut down but maybe #pokemon wil...   \n",
       "27273  #first #bihday to our #puppy #eloise #sweetbab...   \n",
       "30304  it was in ceain areas yeah you not seen the vi...   \n",
       "3928   so many shit talkers bullies in the youtube co...   \n",
       "269    all ready to pay xx #saturday #daughter #love ...   \n",
       "9826   feeling a little #mole tonight #food #foodblog...   \n",
       "11548  while we re still trying to get over the shock...   \n",
       "19491  fantasy aisle #i #fantasy #aisle #summer #gran...   \n",
       "31522  have a wonderful f r i d a y #love #emikagifts...   \n",
       "750    who wants him to be the next commander in chie...   \n",
       "21399                   we beat that cock #beatit #penis   \n",
       "16625                             lik if u cri everi tim   \n",
       "7084   speakers dinner on eve of #ricsrural conferenc...   \n",
       "21332  best facetime today i can t wait to see my boy...   \n",
       "28564                    archery for year at pm #archery   \n",
       "12504  #whoolo in film you can have sad endings #anna...   \n",
       "22295                   at what time will the gates open   \n",
       "13326  thank you for a new #chargehr after mine broke...   \n",
       "1147   our thoughts and prayers goes out to everyone ...   \n",
       "19044  #film bull up you will dominate your bull and ...   \n",
       "31112  this is horribly sad news a fine actor with gr...   \n",
       "14188                            follow your #hea and be   \n",
       "1034   cops giving tickets when they didn t even see ...   \n",
       "2874   singapore city gallery #riclswtravelbook #bear...   \n",
       "8230   oh man been waiting for #wehappyfew for so lon...   \n",
       "15277                                schools almost over   \n",
       "30396  #usd cad bounces off despite higher oil #blog ...   \n",
       "\n",
       "                                           no_hash_tweet  \\\n",
       "565    f this government that deliberately toures ref...   \n",
       "21531  despite a demoralizing may ur newyear be class...   \n",
       "13300  koreans amp joseon people in japan will abuse ...   \n",
       "18925  classic yet you jewish bastards wonder why you...   \n",
       "12619  did someone say antisemetic gee you re a bit t...   \n",
       "26964  couldn t have said this any better nor truthfu...   \n",
       "17273  racism stuffed into skinny jeans with a hipste...   \n",
       "1561   the end of me selfie  love messi cr religion c...   \n",
       "17875  trump ally wishes mad cow disease death for ob...   \n",
       "17184  opinion is rife in the lgbt community gay peop...   \n",
       "8573   allahsoil the cold war was fought over oil tea...   \n",
       "26779  omg these trump suppoers are deplorable dumptr...   \n",
       "15332  sea shepherd suppoers are racist antiracism se...   \n",
       "12290  while can use phrases like sandniggers is acce...   \n",
       "24159  allahsoil enlightenment is wasted on the wilfu...   \n",
       "25895  here comes a supermistict douchebag who can on...   \n",
       "23934  hidden in america is as rampant as blatant racism   \n",
       "29174  will the alt right promote a new kind of genet...   \n",
       "10474  us why weneed empathy ageoftrump grassrootsact...   \n",
       "30743  black trump suppoer smacks down cnn repoer for...   \n",
       "6190   so sick of all the pre programmed hillbots rhe...   \n",
       "19746  when you re blocked by a troll because you pro...   \n",
       "19581  you forgot democrate because vetting people fr...   \n",
       "31767  sea shepherd suppoers are racist antiracism se...   \n",
       "25210                             porn vids web free sex   \n",
       "19800  amp the democraticpay keeps telling me that bl...   \n",
       "7812   although i am not a a bigot or a misogynist so...   \n",
       "5074                 this is sooooo or may be just funny   \n",
       "6317   we the people originally meant we the white la...   \n",
       "5988   i ve no problem with universities that teach a...   \n",
       "...                                                  ...   \n",
       "3721   check out this new trending funny gif pixel ce...   \n",
       "28057  this reminds me of this i am love these two th...   \n",
       "15044  livelypics just when you think you know people...   \n",
       "23222  trying not to shut down but maybe pokemon will...   \n",
       "27273  first bihday to our puppy eloise sweetbabins d...   \n",
       "30304  it was in ceain areas yeah you not seen the vi...   \n",
       "3928   so many shit talkers bullies in the youtube co...   \n",
       "269    all ready to pay xx saturday daughter love pay...   \n",
       "9826   feeling a little mole tonight food foodblogger...   \n",
       "11548  while we re still trying to get over the shock...   \n",
       "19491  fantasy aisle i fantasy aisle summer grand vac...   \n",
       "31522  have a wonderful f r i d a y love emikagifts j...   \n",
       "750    who wants him to be the next commander in chie...   \n",
       "21399                     we beat that cock beatit penis   \n",
       "16625                             lik if u cri everi tim   \n",
       "7084   speakers dinner on eve of ricsrural conference...   \n",
       "21332  best facetime today i can t wait to see my boy...   \n",
       "28564                     archery for year at pm archery   \n",
       "12504  whoolo in film you can have sad endings anna torv   \n",
       "22295                   at what time will the gates open   \n",
       "13326  thank you for a new chargehr after mine broke ...   \n",
       "1147   our thoughts and prayers goes out to everyone ...   \n",
       "19044  film bull up you will dominate your bull and y...   \n",
       "31112  this is horribly sad news a fine actor with gr...   \n",
       "14188                             follow your hea and be   \n",
       "1034   cops giving tickets when they didn t even see ...   \n",
       "2874   singapore city gallery riclswtravelbook bearlo...   \n",
       "8230   oh man been waiting for wehappyfew for so long...   \n",
       "15277                                schools almost over   \n",
       "30396  usd cad bounces off despite higher oil blog si...   \n",
       "\n",
       "                                         tokenized_tweet  \\\n",
       "565    [f, this, government, that, deliberately, tour...   \n",
       "21531  [despite, a, demoralizing, may, ur, newyear, b...   \n",
       "13300  [koreans, amp, joseon, people, in, japan, will...   \n",
       "18925  [classic, yet, you, jewish, bastards, wonder, ...   \n",
       "12619  [did, someone, say, antisemetic, gee, you, re,...   \n",
       "26964  [couldn, t, have, said, this, any, better, nor...   \n",
       "17273  [racism, stuffed, into, skinny, jeans, with, a...   \n",
       "1561   [the, end, of, me, selfie, love, messi, cr, re...   \n",
       "17875  [trump, ally, wishes, mad, cow, disease, death...   \n",
       "17184  [opinion, is, rife, in, the, lgbt, community, ...   \n",
       "8573   [allahsoil, the, cold, war, was, fought, over,...   \n",
       "26779  [omg, these, trump, suppoers, are, deplorable,...   \n",
       "15332  [sea, shepherd, suppoers, are, racist, antirac...   \n",
       "12290  [while, can, use, phrases, like, sandniggers, ...   \n",
       "24159  [allahsoil, enlightenment, is, wasted, on, the...   \n",
       "25895  [here, comes, a, supermistict, douchebag, who,...   \n",
       "23934  [hidden, in, america, is, as, rampant, as, bla...   \n",
       "29174  [will, the, alt, right, promote, a, new, kind,...   \n",
       "10474  [us, why, weneed, empathy, ageoftrump, grassro...   \n",
       "30743  [black, trump, suppoer, smacks, down, cnn, rep...   \n",
       "6190   [so, sick, of, all, the, pre, programmed, hill...   \n",
       "19746  [when, you, re, blocked, by, a, troll, because...   \n",
       "19581  [you, forgot, democrate, because, vetting, peo...   \n",
       "31767  [sea, shepherd, suppoers, are, racist, antirac...   \n",
       "25210                       [porn, vids, web, free, sex]   \n",
       "19800  [amp, the, democraticpay, keeps, telling, me, ...   \n",
       "7812   [although, i, am, not, a, a, bigot, or, a, mis...   \n",
       "5074        [this, is, sooooo, or, may, be, just, funny]   \n",
       "6317   [we, the, people, originally, meant, we, the, ...   \n",
       "5988   [i, ve, no, problem, with, universities, that,...   \n",
       "...                                                  ...   \n",
       "3721   [check, out, this, new, trending, funny, gif, ...   \n",
       "28057  [this, reminds, me, of, this, i, am, love, the...   \n",
       "15044  [livelypics, just, when, you, think, you, know...   \n",
       "23222  [trying, not, to, shut, down, but, maybe, poke...   \n",
       "27273  [first, bihday, to, our, puppy, eloise, sweetb...   \n",
       "30304  [it, was, in, ceain, areas, yeah, you, not, se...   \n",
       "3928   [so, many, shit, talkers, bullies, in, the, yo...   \n",
       "269    [all, ready, to, pay, xx, saturday, daughter, ...   \n",
       "9826   [feeling, a, little, mole, tonight, food, food...   \n",
       "11548  [while, we, re, still, trying, to, get, over, ...   \n",
       "19491  [fantasy, aisle, i, fantasy, aisle, summer, gr...   \n",
       "31522  [have, a, wonderful, f, r, i, d, a, y, love, e...   \n",
       "750    [who, wants, him, to, be, the, next, commander...   \n",
       "21399              [we, beat, that, cock, beatit, penis]   \n",
       "16625                      [lik, if, u, cri, everi, tim]   \n",
       "7084   [speakers, dinner, on, eve, of, ricsrural, con...   \n",
       "21332  [best, facetime, today, i, can, t, wait, to, s...   \n",
       "28564              [archery, for, year, at, pm, archery]   \n",
       "12504  [whoolo, in, film, you, can, have, sad, ending...   \n",
       "22295           [at, what, time, will, the, gates, open]   \n",
       "13326  [thank, you, for, a, new, chargehr, after, min...   \n",
       "1147   [our, thoughts, and, prayers, goes, out, to, e...   \n",
       "19044  [film, bull, up, you, will, dominate, your, bu...   \n",
       "31112  [this, is, horribly, sad, news, a, fine, actor...   \n",
       "14188                       [follow, your, hea, and, be]   \n",
       "1034   [cops, giving, tickets, when, they, didn, t, e...   \n",
       "2874   [singapore, city, gallery, riclswtravelbook, b...   \n",
       "8230   [oh, man, been, waiting, for, wehappyfew, for,...   \n",
       "15277                            [schools, almost, over]   \n",
       "30396  [usd, cad, bounces, off, despite, higher, oil,...   \n",
       "\n",
       "                                          stemmed_tokens  \\\n",
       "565    [f, this, govern, that, deliber, tour, refuge,...   \n",
       "21531  [despit, a, demor, may, ur, newyear, be, class...   \n",
       "13300  [korean, amp, joseon, peopl, in, japan, will, ...   \n",
       "18925  [classic, yet, you, jewish, bastard, wonder, w...   \n",
       "12619  [did, someon, say, antisemet, gee, you, re, a,...   \n",
       "26964  [couldn, t, have, said, this, ani, better, nor...   \n",
       "17273  [racism, stuf, into, skinni, jean, with, a, hi...   \n",
       "1561   [the, end, of, me, selfi, love, messi, cr, rel...   \n",
       "17875  [trump, alli, wish, mad, cow, diseas, death, f...   \n",
       "17184  [opinion, is, rife, in, the, lgbt, communiti, ...   \n",
       "8573   [allahsoil, the, cold, war, was, fought, over,...   \n",
       "26779  [omg, these, trump, suppoer, are, deplor, dump...   \n",
       "15332  [sea, shepherd, suppoer, are, racist, antirac,...   \n",
       "12290  [while, can, use, phrase, like, sandnigg, is, ...   \n",
       "24159  [allahsoil, enlighten, is, wast, on, the, wil,...   \n",
       "25895  [here, come, a, supermistict, douchebag, who, ...   \n",
       "23934  [hidden, in, america, is, as, rampant, as, bla...   \n",
       "29174  [will, the, alt, right, promot, a, new, kind, ...   \n",
       "10474  [us, whi, wene, empathi, ageoftrump, grassroot...   \n",
       "30743  [black, trump, suppoer, smack, down, cnn, repo...   \n",
       "6190   [so, sick, of, all, the, pre, program, hillbot...   \n",
       "19746  [when, you, re, block, by, a, troll, becaus, y...   \n",
       "19581  [you, forgot, democr, becaus, vet, peopl, from...   \n",
       "31767  [sea, shepherd, suppoer, are, racist, antirac,...   \n",
       "25210                        [porn, vid, web, free, sex]   \n",
       "19800  [amp, the, democraticpay, keep, tell, me, that...   \n",
       "7812   [although, i, am, not, a, a, bigot, or, a, mis...   \n",
       "5074        [this, is, sooooo, or, may, be, just, funni]   \n",
       "6317   [we, the, peopl, origin, meant, we, the, white...   \n",
       "5988   [i, ve, no, problem, with, univers, that, teac...   \n",
       "...                                                  ...   \n",
       "3721   [check, out, this, new, trend, funni, gif, pix...   \n",
       "28057  [this, remind, me, of, this, i, am, love, thes...   \n",
       "15044  [livelyp, just, when, you, think, you, know, p...   \n",
       "23222  [tri, not, to, shut, down, but, mayb, pokemon,...   \n",
       "27273  [first, bihday, to, our, puppi, elois, sweetba...   \n",
       "30304  [it, was, in, ceain, area, yeah, you, not, see...   \n",
       "3928   [so, mani, shit, talker, bulli, in, the, youtu...   \n",
       "269    [all, readi, to, pay, xx, saturday, daughter, ...   \n",
       "9826   [feel, a, littl, mole, tonight, food, foodblog...   \n",
       "11548  [while, we, re, still, tri, to, get, over, the...   \n",
       "19491  [fantasi, aisl, i, fantasi, aisl, summer, gran...   \n",
       "31522  [have, a, wonder, f, r, i, d, a, y, love, emik...   \n",
       "750    [who, want, him, to, be, the, next, command, i...   \n",
       "21399               [we, beat, that, cock, beatit, peni]   \n",
       "16625                      [lik, if, u, cri, everi, tim]   \n",
       "7084   [speaker, dinner, on, eve, of, ricsrur, confer...   \n",
       "21332  [best, facetim, today, i, can, t, wait, to, se...   \n",
       "28564              [archeri, for, year, at, pm, archeri]   \n",
       "12504  [whoolo, in, film, you, can, have, sad, end, a...   \n",
       "22295            [at, what, time, will, the, gate, open]   \n",
       "13326  [thank, you, for, a, new, chargehr, after, min...   \n",
       "1147   [our, thought, and, prayer, goe, out, to, ever...   \n",
       "19044  [film, bull, up, you, will, domin, your, bull,...   \n",
       "31112  [this, is, horribl, sad, news, a, fine, actor,...   \n",
       "14188                       [follow, your, hea, and, be]   \n",
       "1034   [cop, give, ticket, when, they, didn, t, even,...   \n",
       "2874   [singapor, citi, galleri, riclswtravelbook, be...   \n",
       "8230   [oh, man, been, wait, for, wehappyfew, for, so...   \n",
       "15277                             [school, almost, over]   \n",
       "30396  [usd, cad, bounc, off, despit, higher, oil, bl...   \n",
       "\n",
       "                                       lemmatized_tokens  \\\n",
       "565    [f, this, government, that, deliberately, tour...   \n",
       "21531  [despite, a, demoralizing, may, ur, newyear, b...   \n",
       "13300  [korean, amp, joseon, people, in, japan, will,...   \n",
       "18925  [classic, yet, you, jewish, bastard, wonder, w...   \n",
       "12619  [did, someone, say, antisemetic, gee, you, re,...   \n",
       "26964  [couldn, t, have, said, this, any, better, nor...   \n",
       "17273  [racism, stuffed, into, skinny, jean, with, a,...   \n",
       "1561   [the, end, of, me, selfie, love, messi, cr, re...   \n",
       "17875  [trump, ally, wish, mad, cow, disease, death, ...   \n",
       "17184  [opinion, is, rife, in, the, lgbt, community, ...   \n",
       "8573   [allahsoil, the, cold, war, wa, fought, over, ...   \n",
       "26779  [omg, these, trump, suppoers, are, deplorable,...   \n",
       "15332  [sea, shepherd, suppoers, are, racist, antirac...   \n",
       "12290  [while, can, use, phrase, like, sandniggers, i...   \n",
       "24159  [allahsoil, enlightenment, is, wasted, on, the...   \n",
       "25895  [here, come, a, supermistict, douchebag, who, ...   \n",
       "23934  [hidden, in, america, is, a, rampant, a, blata...   \n",
       "29174  [will, the, alt, right, promote, a, new, kind,...   \n",
       "10474  [u, why, weneed, empathy, ageoftrump, grassroo...   \n",
       "30743  [black, trump, suppoer, smack, down, cnn, repo...   \n",
       "6190   [so, sick, of, all, the, pre, programmed, hill...   \n",
       "19746  [when, you, re, blocked, by, a, troll, because...   \n",
       "19581  [you, forgot, democrate, because, vetting, peo...   \n",
       "31767  [sea, shepherd, suppoers, are, racist, antirac...   \n",
       "25210                       [porn, vids, web, free, sex]   \n",
       "19800  [amp, the, democraticpay, keep, telling, me, t...   \n",
       "7812   [although, i, am, not, a, a, bigot, or, a, mis...   \n",
       "5074        [this, is, sooooo, or, may, be, just, funny]   \n",
       "6317   [we, the, people, originally, meant, we, the, ...   \n",
       "5988   [i, ve, no, problem, with, university, that, t...   \n",
       "...                                                  ...   \n",
       "3721   [check, out, this, new, trending, funny, gif, ...   \n",
       "28057  [this, reminds, me, of, this, i, am, love, the...   \n",
       "15044  [livelypics, just, when, you, think, you, know...   \n",
       "23222  [trying, not, to, shut, down, but, maybe, poke...   \n",
       "27273  [first, bihday, to, our, puppy, eloise, sweetb...   \n",
       "30304  [it, wa, in, ceain, area, yeah, you, not, seen...   \n",
       "3928   [so, many, shit, talker, bully, in, the, youtu...   \n",
       "269    [all, ready, to, pay, xx, saturday, daughter, ...   \n",
       "9826   [feeling, a, little, mole, tonight, food, food...   \n",
       "11548  [while, we, re, still, trying, to, get, over, ...   \n",
       "19491  [fantasy, aisle, i, fantasy, aisle, summer, gr...   \n",
       "31522  [have, a, wonderful, f, r, i, d, a, y, love, e...   \n",
       "750    [who, want, him, to, be, the, next, commander,...   \n",
       "21399              [we, beat, that, cock, beatit, penis]   \n",
       "16625                      [lik, if, u, cri, everi, tim]   \n",
       "7084   [speaker, dinner, on, eve, of, ricsrural, conf...   \n",
       "21332  [best, facetime, today, i, can, t, wait, to, s...   \n",
       "28564              [archery, for, year, at, pm, archery]   \n",
       "12504  [whoolo, in, film, you, can, have, sad, ending...   \n",
       "22295            [at, what, time, will, the, gate, open]   \n",
       "13326  [thank, you, for, a, new, chargehr, after, min...   \n",
       "1147   [our, thought, and, prayer, go, out, to, every...   \n",
       "19044  [film, bull, up, you, will, dominate, your, bu...   \n",
       "31112  [this, is, horribly, sad, news, a, fine, actor...   \n",
       "14188                       [follow, your, hea, and, be]   \n",
       "1034   [cop, giving, ticket, when, they, didn, t, eve...   \n",
       "2874   [singapore, city, gallery, riclswtravelbook, b...   \n",
       "8230   [oh, man, been, waiting, for, wehappyfew, for,...   \n",
       "15277                             [school, almost, over]   \n",
       "30396  [usd, cad, bounce, off, despite, higher, oil, ...   \n",
       "\n",
       "                                               lem_tweet  \\\n",
       "565    f this government that deliberately toures ref...   \n",
       "21531  despite a demoralizing may ur newyear be class...   \n",
       "13300  koreans amp joseon people in japan will abuse ...   \n",
       "18925  classic yet you jewish bastards wonder why you...   \n",
       "12619  did someone say antisemetic gee you re a bit t...   \n",
       "26964  couldn t have said this any better nor truthfu...   \n",
       "17273  racism stuffed into skinny jeans with a hipste...   \n",
       "1561   the end of me selfie  love messi cr religion c...   \n",
       "17875  trump ally wishes mad cow disease death for ob...   \n",
       "17184  opinion is rife in the lgbt community gay peop...   \n",
       "8573   allahsoil the cold war was fought over oil tea...   \n",
       "26779  omg these trump suppoers are deplorable dumptr...   \n",
       "15332  sea shepherd suppoers are racist antiracism se...   \n",
       "12290  while can use phrases like sandniggers is acce...   \n",
       "24159  allahsoil enlightenment is wasted on the wilfu...   \n",
       "25895  here comes a supermistict douchebag who can on...   \n",
       "23934  hidden in america is as rampant as blatant racism   \n",
       "29174  will the alt right promote a new kind of genet...   \n",
       "10474  us why weneed empathy ageoftrump grassrootsact...   \n",
       "30743  black trump suppoer smacks down cnn repoer for...   \n",
       "6190   so sick of all the pre programmed hillbots rhe...   \n",
       "19746  when you re blocked by a troll because you pro...   \n",
       "19581  you forgot democrate because vetting people fr...   \n",
       "31767  sea shepherd suppoers are racist antiracism se...   \n",
       "25210                             porn vids web free sex   \n",
       "19800  amp the democraticpay keeps telling me that bl...   \n",
       "7812   although i am not a a bigot or a misogynist so...   \n",
       "5074                 this is sooooo or may be just funny   \n",
       "6317   we the people originally meant we the white la...   \n",
       "5988   i ve no problem with universities that teach a...   \n",
       "...                                                  ...   \n",
       "3721   check out this new trending funny gif pixel ce...   \n",
       "28057  this reminds me of this i am love these two th...   \n",
       "15044  livelypics just when you think you know people...   \n",
       "23222  trying not to shut down but maybe pokemon will...   \n",
       "27273  first bihday to our puppy eloise sweetbabins d...   \n",
       "30304  it was in ceain areas yeah you not seen the vi...   \n",
       "3928   so many shit talkers bullies in the youtube co...   \n",
       "269    all ready to pay xx saturday daughter love pay...   \n",
       "9826   feeling a little mole tonight food foodblogger...   \n",
       "11548  while we re still trying to get over the shock...   \n",
       "19491  fantasy aisle i fantasy aisle summer grand vac...   \n",
       "31522  have a wonderful f r i d a y love emikagifts j...   \n",
       "750    who wants him to be the next commander in chie...   \n",
       "21399                     we beat that cock beatit penis   \n",
       "16625                             lik if u cri everi tim   \n",
       "7084   speakers dinner on eve of ricsrural conference...   \n",
       "21332  best facetime today i can t wait to see my boy...   \n",
       "28564                     archery for year at pm archery   \n",
       "12504  whoolo in film you can have sad endings anna torv   \n",
       "22295                   at what time will the gates open   \n",
       "13326  thank you for a new chargehr after mine broke ...   \n",
       "1147   our thoughts and prayers goes out to everyone ...   \n",
       "19044  film bull up you will dominate your bull and y...   \n",
       "31112  this is horribly sad news a fine actor with gr...   \n",
       "14188                             follow your hea and be   \n",
       "1034   cops giving tickets when they didn t even see ...   \n",
       "2874   singapore city gallery riclswtravelbook bearlo...   \n",
       "8230   oh man been waiting for wehappyfew for so long...   \n",
       "15277                                schools almost over   \n",
       "30396  usd cad bounces off despite higher oil blog si...   \n",
       "\n",
       "                                              stem_tweet  label  \n",
       "565    f this government that deliberately toures ref...      1  \n",
       "21531  despite a demoralizing may ur newyear be class...      1  \n",
       "13300  koreans amp joseon people in japan will abuse ...      1  \n",
       "18925  classic yet you jewish bastards wonder why you...      1  \n",
       "12619  did someone say antisemetic gee you re a bit t...      1  \n",
       "26964  couldn t have said this any better nor truthfu...      1  \n",
       "17273  racism stuffed into skinny jeans with a hipste...      1  \n",
       "1561   the end of me selfie  love messi cr religion c...      1  \n",
       "17875  trump ally wishes mad cow disease death for ob...      1  \n",
       "17184  opinion is rife in the lgbt community gay peop...      1  \n",
       "8573   allahsoil the cold war was fought over oil tea...      1  \n",
       "26779  omg these trump suppoers are deplorable dumptr...      1  \n",
       "15332  sea shepherd suppoers are racist antiracism se...      1  \n",
       "12290  while can use phrases like sandniggers is acce...      1  \n",
       "24159  allahsoil enlightenment is wasted on the wilfu...      1  \n",
       "25895  here comes a supermistict douchebag who can on...      1  \n",
       "23934     hidden in america is as rampant as blatant rac      1  \n",
       "29174  will the alt right promote a new kind of genet...      1  \n",
       "10474  us why weneed empathy ageoftrump grassrootsact...      1  \n",
       "30743  black trump suppoer smacks down cnn repoer for...      1  \n",
       "6190   so sick of all the pre programmed hillbots rhe...      1  \n",
       "19746  when you re blocked by a troll because you pro...      1  \n",
       "19581  you forgot democrate because vetting people fr...      1  \n",
       "31767  sea shepherd suppoers are racist antiracism se...      1  \n",
       "25210                             porn vids web free sex      1  \n",
       "19800  amp the democraticpay keeps telling me that bl...      1  \n",
       "7812   although i am not a a bigot or a misogynist so...      1  \n",
       "5074                 this is sooooo or may be just funni      1  \n",
       "6317   we the people originally meant we the white la...      1  \n",
       "5988   i ve no problem with universities that teach a...      1  \n",
       "...                                                  ...    ...  \n",
       "3721   check out this new trending funny gif pixel ce...      0  \n",
       "28057  this reminds me of this i am love these two th...      0  \n",
       "15044  livelypics just when you think you know people...      0  \n",
       "23222  trying not to shut down but maybe pokemon will...      0  \n",
       "27273  first bihday to our puppy eloise sweetbabins d...      0  \n",
       "30304  it was in ceain areas yeah you not seen the vi...      0  \n",
       "3928   so many shit talkers bullies in the youtube co...      0  \n",
       "269    all ready to pay xx saturday daughter love pay...      0  \n",
       "9826   feeling a little mole tonight food foodblogger...      0  \n",
       "11548  while we re still trying to get over the shock...      0  \n",
       "19491  fantasy aisle i fantasy aisle summer grand vac...      0  \n",
       "31522  have a wonderful f r i d a y love emikagifts j...      0  \n",
       "750    who wants him to be the next commander in chie...      0  \n",
       "21399                      we beat that cock beatit peni      0  \n",
       "16625                             lik if u cri everi tim      0  \n",
       "7084   speakers dinner on eve of ricsrural conference...      0  \n",
       "21332  best facetime today i can t wait to see my boy...      0  \n",
       "28564                     archery for year at pm archeri      0  \n",
       "12504  whoolo in film you can have sad endings anna torv      0  \n",
       "22295                   at what time will the gates open      0  \n",
       "13326  thank you for a new chargehr after mine broke ...      0  \n",
       "1147   our thoughts and prayers goes out to everyone ...      0  \n",
       "19044  film bull up you will dominate your bull and y...      0  \n",
       "31112  this is horribly sad news a fine actor with gr...      0  \n",
       "14188                              follow your hea and b      0  \n",
       "1034   cops giving tickets when they didn t even see ...      0  \n",
       "2874   singapore city gallery riclswtravelbook bearlo...      0  \n",
       "8230   oh man been waiting for wehappyfew for so long...      0  \n",
       "15277                                  schools almost ov      0  \n",
       "30396  usd cad bounces off despite higher oil blog si...      0  \n",
       "\n",
       "[37982 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsample_training_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_upsampled = upsample_training_data(X_train, y_train)\n",
    "\n",
    "X_train_up = train_upsampled.drop(['label'], axis = 1)\n",
    "y_train_up = pd.DataFrame(train_upsampled.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    18991\n",
       "0    18991\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_upsampled['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_downsampled = downsample_training_data(X_train, y_train)\n",
    "\n",
    "X_train_down = train_downsampled.drop(['label'], axis = 1)\n",
    "y_train_down = pd.DataFrame(train_downsampled.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1464\n",
       "0    1464\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_downsampled['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Vectorization and Method Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=.001)\n",
    "tfidf_ngram = TfidfVectorizer(ngram_range=(1,2), min_df=.001)\n",
    "tfidf_ngram2 = TfidfVectorizer(ngram_range=(2,3),min_df=.001)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "rfc = RandomForestClassifier(random_state=10)\n",
    "nb = GaussianNB()\n",
    "svc = SVC(random_state=10)\n",
    "\n",
    "vectorization_list = [('COUNT_VECTORIZER', count_vect),\n",
    "                      ('TFIDF_VECTORIZER', tfidf_vectorizer),\n",
    "                      ('TFIDF_NGRAM_1_2', tfidf_ngram),\n",
    "                      ('TFIDF_NGRAM_2_3', tfidf_ngram2)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0e9feb973b7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m NB_compare_vectorization_model(X_train.lem_tweet, y_train, \n\u001b[0;32m----> 2\u001b[0;31m                                    X_val.lem_tweet, y_val, GaussianNB())\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-4166f9486ac3>\u001b[0m in \u001b[0;36mNB_compare_vectorization_model\u001b[0;34m(X_train_col, y_train, X_val_col, y_val, classifier)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mX_val_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val_transformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_transformed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtrain_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train_transformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[0;32m--> 191\u001b[0;31m                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;31m# boost the variance by epsilon, a small fraction of the standard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;31m# deviation of the largest dimension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_smoothing\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_refit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mvar\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m   3365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3366\u001b[0m     return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n\u001b[0;32m-> 3367\u001b[0;31m                          **kwargs)\n\u001b[0m\u001b[1;32m   3368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NB_compare_vectorization_model(X_train.lem_tweet, y_train, \n",
    "                                   X_val.lem_tweet, y_val, GaussianNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COUNT_VECTORIZER': {'Train Accuracy': 0.96,\n",
       "  'Train Precision': 0.68,\n",
       "  'Train Recall': 0.88,\n",
       "  'Train F1': 0.77,\n",
       "  'Validation Accuracy': 0.89,\n",
       "  'Validation Precision': 0.34,\n",
       "  'Validation Recall': 0.68,\n",
       "  'Validation F1': 0.45},\n",
       " 'TFIDF_VECTORIZER': {'Train Accuracy': 0.9,\n",
       "  'Train Precision': 0.4,\n",
       "  'Train Recall': 0.84,\n",
       "  'Train F1': 0.54,\n",
       "  'Validation Accuracy': 0.88,\n",
       "  'Validation Precision': 0.31,\n",
       "  'Validation Recall': 0.72,\n",
       "  'Validation F1': 0.44},\n",
       " 'TFIDF_NGRAM_1_2': {'Train Accuracy': 0.9,\n",
       "  'Train Precision': 0.42,\n",
       "  'Train Recall': 0.85,\n",
       "  'Train F1': 0.56,\n",
       "  'Validation Accuracy': 0.89,\n",
       "  'Validation Precision': 0.32,\n",
       "  'Validation Recall': 0.71,\n",
       "  'Validation F1': 0.44},\n",
       " 'TFIDF_NGRAM_2_3': {'Train Accuracy': 0.57,\n",
       "  'Train Precision': 0.13,\n",
       "  'Train Recall': 0.87,\n",
       "  'Train F1': 0.22,\n",
       "  'Validation Accuracy': 0.55,\n",
       "  'Validation Precision': 0.1,\n",
       "  'Validation Recall': 0.78,\n",
       "  'Validation F1': 0.18}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression: compare vectorizers using lemmitizing + class balances\n",
    "LR_cw_lemm = wrapper_compare_vectorizations(X_train.lem_tweet, \n",
    "                            y_train, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', solver = 'lbfgs'), \n",
    "                                            vectorization_list, apply_smote = True)\n",
    "\n",
    "LR_cw_lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT_VECTORIZER</th>\n",
       "      <th>TFIDF_VECTORIZER</th>\n",
       "      <th>TFIDF_NGRAM_1_2</th>\n",
       "      <th>TFIDF_NGRAM_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation F1</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Precision</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Recall</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      COUNT_VECTORIZER  TFIDF_VECTORIZER  TFIDF_NGRAM_1_2  \\\n",
       "Train Accuracy                    0.99              0.94             0.96   \n",
       "Train F1                          0.99              0.94             0.96   \n",
       "Train Precision                   0.99              0.93             0.94   \n",
       "Train Recall                      1.00              0.96             0.98   \n",
       "Validation Accuracy               0.95              0.91             0.91   \n",
       "Validation F1                     0.65              0.51             0.53   \n",
       "Validation Precision              0.62              0.38             0.41   \n",
       "Validation Recall                 0.68              0.76             0.76   \n",
       "\n",
       "                      TFIDF_NGRAM_2_3  \n",
       "Train Accuracy                   0.80  \n",
       "Train F1                         0.77  \n",
       "Train Precision                  0.88  \n",
       "Train Recall                     0.69  \n",
       "Validation Accuracy              0.87  \n",
       "Validation F1                    0.32  \n",
       "Validation Precision             0.24  \n",
       "Validation Recall                0.47  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression: compare vectorizers using lemmitizing + upsampling\n",
    "LR_cw_lemm = wrapper_compare_vectorizations(X_train_up.lem_tweet, \n",
    "                            y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', solver = 'lbfgs'), \n",
    "                                            vectorization_list, apply_smote = False)\n",
    "\n",
    "pd.DataFrame(LR_cw_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mycsvfile.csv','a') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(LR_cw_lemm.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame.from_dict(data= LR_cw_lemm).to_csv('dict_file.csv', header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT_VECTORIZER</th>\n",
       "      <th>TFIDF_VECTORIZER</th>\n",
       "      <th>TFIDF_NGRAM_1_2</th>\n",
       "      <th>TFIDF_NGRAM_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation F1</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Precision</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Recall</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      COUNT_VECTORIZER  TFIDF_VECTORIZER  TFIDF_NGRAM_1_2  \\\n",
       "Train Accuracy                    0.96              0.89             0.90   \n",
       "Train F1                          0.76              0.53             0.55   \n",
       "Train Precision                   0.67              0.39             0.41   \n",
       "Train Recall                      0.88              0.84             0.85   \n",
       "Validation Accuracy               0.89              0.88             0.88   \n",
       "Validation F1                     0.43              0.43             0.43   \n",
       "Validation Precision              0.32              0.31             0.31   \n",
       "Validation Recall                 0.66              0.71             0.70   \n",
       "\n",
       "                      TFIDF_NGRAM_2_3  \n",
       "Train Accuracy                   0.56  \n",
       "Train F1                         0.22  \n",
       "Train Precision                  0.13  \n",
       "Train Recall                     0.87  \n",
       "Validation Accuracy              0.54  \n",
       "Validation F1                    0.18  \n",
       "Validation Precision             0.10  \n",
       "Validation Recall                0.78  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression: compare vectorizers using stemming + class balances\n",
    "pd.DataFrame(wrapper_compare_vectorizations(X_train.stem_tweet, \n",
    "                            y_train, X_val.stem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', solver = 'lbfgs'),\n",
    "                            vectorization_list, apply_smote= True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train Accuracy': 0.99,\n",
       " 'Train Precision': 0.99,\n",
       " 'Train Recall': 1.0,\n",
       " 'Train F1': 0.99,\n",
       " 'Validation Accuracy': 0.95,\n",
       " 'Validation Precision': 0.59,\n",
       " 'Validation Recall': 0.66,\n",
       " 'Validation F1': 0.62}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "single_vector_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val,\n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1'), count_vect)\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "smote_vector_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                    LogisticRegression(class_weight='balanced', penalty = 'l1'), count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "single_vector_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                    LogisticRegression(class_weight='balanced', penalty = 'l1', random_state=1), count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "wrapper_single_vectorization(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1', random_state=1),\n",
    "                            count_vect, apply_smote = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Comparison Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT_VECTORIZER</th>\n",
       "      <th>TFIDF_VECTORIZER</th>\n",
       "      <th>TFIDF_NGRAM_1_2</th>\n",
       "      <th>TFIDF_NGRAM_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation F1</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Precision</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Recall</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      COUNT_VECTORIZER  TFIDF_VECTORIZER  TFIDF_NGRAM_1_2  \\\n",
       "Train Accuracy                    0.99              0.94             0.96   \n",
       "Train F1                          0.99              0.94             0.96   \n",
       "Train Precision                   0.99              0.93             0.94   \n",
       "Train Recall                      1.00              0.96             0.98   \n",
       "Validation Accuracy               0.95              0.91             0.91   \n",
       "Validation F1                     0.65              0.51             0.53   \n",
       "Validation Precision              0.62              0.38             0.41   \n",
       "Validation Recall                 0.68              0.76             0.76   \n",
       "\n",
       "                      TFIDF_NGRAM_2_3  \n",
       "Train Accuracy                   0.80  \n",
       "Train F1                         0.77  \n",
       "Train Precision                  0.88  \n",
       "Train Recall                     0.69  \n",
       "Validation Accuracy              0.87  \n",
       "Validation F1                    0.32  \n",
       "Validation Precision             0.24  \n",
       "Validation Recall                0.47  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = wrapper_compare_vectorizations(X_train_up.lem_tweet, \n",
    "                            y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(random_state =10, solver = 'lbfgs'),\n",
    "                            vectorization_list, apply_smote = False)\n",
    "\n",
    "pd.DataFrame(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = wrapper_compare_vectorizations(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1'), \n",
    "                            vectorization_list, apply_smote = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = wrapper_compare_vectorizations(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1'), \n",
    "                            vectorization_list, apply_smote = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT_VECTORIZER</th>\n",
       "      <th>TFIDF_VECTORIZER</th>\n",
       "      <th>TFIDF_NGRAM_1_2</th>\n",
       "      <th>TFIDF_NGRAM_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation F1</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Precision</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Recall</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      COUNT_VECTORIZER  TFIDF_VECTORIZER  TFIDF_NGRAM_1_2  \\\n",
       "Train Accuracy                    0.74              0.78             0.75   \n",
       "Train F1                          0.70              0.76             0.69   \n",
       "Train Precision                   0.84              0.85             0.90   \n",
       "Train Recall                      0.60              0.68             0.56   \n",
       "Validation Accuracy               0.85              0.85             0.91   \n",
       "Validation F1                     0.31              0.33             0.44   \n",
       "Validation Precision              0.22              0.23             0.37   \n",
       "Validation Recall                 0.51              0.60             0.53   \n",
       "\n",
       "                      TFIDF_NGRAM_2_3  \n",
       "Train Accuracy                   0.58  \n",
       "Train F1                         0.29  \n",
       "Train Precision                  0.96  \n",
       "Train Recall                     0.17  \n",
       "Validation Accuracy              0.94  \n",
       "Validation F1                    0.21  \n",
       "Validation Precision             0.57  \n",
       "Validation Recall                0.13  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = wrapper_compare_vectorizations(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            RandomForestClassifier(max_depth=10, random_state=1), \n",
    "                            vectorization_list, sampling= 'upsampling')\n",
    "pd.DataFrame(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train Accuracy': 1.0,\n",
       " 'Train Precision': 1.0,\n",
       " 'Train Recall': 1.0,\n",
       " 'Train F1': 1.0,\n",
       " 'Validation Accuracy': 0.96,\n",
       " 'Validation Precision': 0.88,\n",
       " 'Validation Recall': 0.44,\n",
       " 'Validation F1': 0.59}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "wrapper_single_vectorization2(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            RandomForestClassifier(random_state=1), count_vect, sampling='downsampling')\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Final Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT_VECTORIZER</th>\n",
       "      <th>TFIDF_VECTORIZER</th>\n",
       "      <th>TFIDF_NGRAM_1_2</th>\n",
       "      <th>TFIDF_NGRAM_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation F1</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Precision</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Recall</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      COUNT_VECTORIZER  TFIDF_VECTORIZER  TFIDF_NGRAM_1_2  \\\n",
       "Train Accuracy                    0.99              0.95             0.97   \n",
       "Train F1                          0.99              0.95             0.97   \n",
       "Train Precision                   0.99              0.94             0.96   \n",
       "Train Recall                      1.00              0.97             0.99   \n",
       "Validation Accuracy               0.95              0.92             0.93   \n",
       "Validation F1                     0.62              0.53             0.56   \n",
       "Validation Precision              0.59              0.41             0.46   \n",
       "Validation Recall                 0.66              0.73             0.72   \n",
       "\n",
       "                      TFIDF_NGRAM_2_3  \n",
       "Train Accuracy                   0.80  \n",
       "Train F1                         0.78  \n",
       "Train Precision                  0.88  \n",
       "Train Recall                     0.69  \n",
       "Validation Accuracy              0.88  \n",
       "Validation F1                    0.33  \n",
       "Validation Precision             0.25  \n",
       "Validation Recall                0.47  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict = wrapper_compare_vectorizations(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', penalty = 'l1'), \n",
    "                            vectorization_list, sampling='help')\n",
    "pd.DataFrame(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train Accuracy': 1.0,\n",
       " 'Train Precision': 1.0,\n",
       " 'Train Recall': 1.0,\n",
       " 'Train F1': 1.0,\n",
       " 'Validation Accuracy': 0.96,\n",
       " 'Validation Precision': 0.88,\n",
       " 'Validation Recall': 0.44,\n",
       " 'Validation F1': 0.59}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "wrapper_single_vectorization(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            RandomForestClassifier(random_state=1), count_vect, apply_smote=False)\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train Accuracy': 1.0,\n",
       " 'Train Precision': 1.0,\n",
       " 'Train Recall': 1.0,\n",
       " 'Train F1': 1.0,\n",
       " 'Validation Accuracy': 0.96,\n",
       " 'Validation Precision': 0.88,\n",
       " 'Validation Recall': 0.44,\n",
       " 'Validation F1': 0.59}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "wrapper_single_vectorization(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            RandomForestClassifier(random_state=1), count_vect, apply_smote=True)\n",
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "wrapper_single_vectorization(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                            logreg, count_vect, apply_smote = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Train Accuracy': 0.9,\n",
       " 'Train Precision': 0.94,\n",
       " 'Train Recall': 0.85,\n",
       " 'Train F1': 0.89,\n",
       " 'Validation Accuracy': 0.91,\n",
       " 'Validation Precision': 0.38,\n",
       " 'Validation Recall': 0.68,\n",
       " 'Validation F1': 0.49}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.924572438431134\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score RF: 0.44314644879614806\n"
     ]
    }
   ],
   "source": [
    "y_score = logreg.decision_function(X_val_transformed)\n",
    "   \n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_score)\n",
    "y_val_score = logreg.decision_function(X_val_transformed)\n",
    "val_fpr, val_tpr, thresholds = roc_curve(y_val, y_val_score)\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "plt.figure(figsize=(10,8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve for Validation Set')\n",
    "plt.legend(loc=\"lower right\")\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "plt.show()\n",
    "\n",
    "average_precision = average_precision_score(y_val, y_val_pred)\n",
    "\n",
    "print('Average precision-recall score RF: {}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_predictions_val = []\n",
    "for item in logreg.predict_proba(X_val_transformed):\n",
    "    if item[0] <= .85:\n",
    "        weighted_predictions_val.append(1)\n",
    "    else:\n",
    "        weighted_predictions_val.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted 0</th>\n",
       "      <th>predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual 0</th>\n",
       "      <td>4646</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual 1</th>\n",
       "      <td>105</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          predicted 0  predicted 1\n",
       "actual 0         4646          138\n",
       "actual 1          105          225"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted 0</th>\n",
       "      <th>predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual 0</th>\n",
       "      <td>4202</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual 1</th>\n",
       "      <td>60</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          predicted 0  predicted 1\n",
       "actual 0         4202          582\n",
       "actual 1           60          270"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with threshhold adjustment\n",
    "pd.DataFrame(confusion_matrix(y_val, weighted_predictions_val), index = ['actual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>predicted_class</th>\n",
       "      <th>actual_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17631</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20465</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29819</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15058</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1  predicted_class  actual_class\n",
       "17631  0.96  0.04                0             0\n",
       "20997  0.95  0.05                0             0\n",
       "20465  0.98  0.02                0             0\n",
       "29819  0.93  0.07                0             0\n",
       "15058  0.99  0.01                0             0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df[pred_df['actual_class'] != pred_df['predicted_class']]\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>predicted_class</th>\n",
       "      <th>actual_class</th>\n",
       "      <th>weighted_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11418</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7166</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13993</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26922</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1  predicted_class  actual_class  weighted_predictions\n",
       "11418  0.86  0.14                0             1                     0\n",
       "3651   0.81  0.19                0             0                     1\n",
       "7166   0.84  0.16                0             0                     1\n",
       "13993  0.79  0.21                0             0                     1\n",
       "26922  0.61  0.39                0             0                     1"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df['weighted_predictions'] = weighted_predictions_val\n",
    "pred_df[pred_df['actual_class'] != pred_df['weighted_predictions']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@user @user lock the gate! #pamgelleheracist   #mmiw #mmiwg'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tweet[11418]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed, metrics_dict, train_confusion_matrix, y_test_pred, y_test_prob, test_df = \\\n",
    "\\\n",
    "wrapper_single_vectorization(X_train_up.lem_tweet, y_train_up, X_test.lem_tweet, y_test, \n",
    "                            logreg, count_vect, apply_smote = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = logreg.decision_function(X_train_transformed)\n",
    "   \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "y_test_score = logreg.decision_function(X_train_transformed)\n",
    "test_fpr, test_tpr, thresholds = roc_curve(y_test, y_test_score)\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "plt.figure(figsize=(10,8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve for Validation Set')\n",
    "plt.legend(loc=\"lower right\")\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "plt.show()\n",
    "\n",
    "average_precision = average_precision_score(y_test, y_test_pred)\n",
    "\n",
    "print('Average precision-recall score RF: {}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_predictions_test = []\n",
    "for item in logreg.predict_proba(X_train_transformed):\n",
    "    if item[0] <= .85:\n",
    "        weighted_predictions_test.append(1)\n",
    "    else:\n",
    "        weighted_predictions_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(y_test, weighted_predictions_test), index = ['actual 0', 'actual 1'], columns = ['predicted 0', 'predicted 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm_metrics_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-319-e2dd7d93fd9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m wrapper_compare_vectorizations(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n\u001b[1;32m      4\u001b[0m                                    SVC(class_weight ='balanced', gamma='auto', random_state = 10), vectorization_list, apply_smote=False)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_metrics_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'svm_metrics_dict' is not defined"
     ]
    }
   ],
   "source": [
    "#class weight = balanced + lemmatized\n",
    "svm_metrics_balance, svm_X_train_transformed, svm_X_val_transformed = \\\n",
    "wrapper_compare_vectorizations(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                   SVC(class_weight ='balanced', gamma='auto', random_state = 10), vectorization_list, apply_smote=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT_VECTORIZER</th>\n",
       "      <th>TFIDF_VECTORIZER</th>\n",
       "      <th>TFIDF_NGRAM_1_2</th>\n",
       "      <th>TFIDF_NGRAM_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation F1</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Precision</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Recall</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      COUNT_VECTORIZER  TFIDF_VECTORIZER  TFIDF_NGRAM_1_2  \\\n",
       "Train Accuracy                    0.07              0.08             0.07   \n",
       "Train F1                          0.13              0.13             0.13   \n",
       "Train Precision                   0.07              0.07             0.07   \n",
       "Train Recall                      1.00              1.00             1.00   \n",
       "Validation Accuracy               0.06              0.07             0.06   \n",
       "Validation F1                     0.12              0.12             0.12   \n",
       "Validation Precision              0.06              0.06             0.06   \n",
       "Validation Recall                 1.00              1.00             1.00   \n",
       "\n",
       "                      TFIDF_NGRAM_2_3  \n",
       "Train Accuracy                   0.93  \n",
       "Train F1                         0.18  \n",
       "Train Precision                  0.54  \n",
       "Train Recall                     0.10  \n",
       "Validation Accuracy              0.94  \n",
       "Validation F1                    0.13  \n",
       "Validation Precision             0.50  \n",
       "Validation Recall                0.08  "
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(svm_metrics_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE + lemmatized \n",
    "svm_metrics_smote, svm_X_train_smote, svm_X_val_smote   = \\\n",
    "wrapper_compare_vectorizations(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                   SVC(class_weight ='balanced', gamma='auto', random_state = 10), vectorization_list, apply_smote=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(svm_metrics_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upsampling + lemmatized\n",
    "svm_metrics_up, svm_X_train_up, svm_X_val_up = \\\n",
    "wrapper_compare_vectorizations(X_train_up.lem_tweet, y_train, X_val_up.lem_tweet, y_val, \n",
    "                                   SVC(gamma='auto', random_state = 10), vectorization_list, apply_smote=False)\n",
    "pd.DataFrame(svm_metrics_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Searching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper_compare_vectorizations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfid2 =  tfidf_ngram2.fit_transform(X_train_up.lemmatized_tweet)\n",
    "X_val_tfid2 =  tfidf_ngram2.transform(X_val.lemmatized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = SVC(kernel='linear', C=1, gamma=1, class_weight ='balanced')\n",
    "\n",
    "params = {\n",
    "'C': [0.1,.2, .3, 0.8,1,1.2,1.4],\n",
    "'kernel':['linear', 'rbf'],\n",
    "'gamma' :[0.1,0.8,1,1.2,1.4]\n",
    "}\n",
    "\n",
    "svm_gs= GridSearchCV(svc, param_grid = params, cv = 3)\n",
    "\n",
    "scores = ['f1','accuracy','recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_gs.fit(X_train_tfid2, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_vector_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vectorization_model(X_train_up.lemmatized_tweet, y_train_up, X_val.lemmatized_tweet, y_val, \n",
    "                                   SVC(C=1.2, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma=1.4, kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Multiple Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT_VECTORIZER</th>\n",
       "      <th>TFIDF_VECTORIZER</th>\n",
       "      <th>TFIDF_NGRAM_1_2</th>\n",
       "      <th>TFIDF_NGRAM_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation F1</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Precision</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Recall</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      COUNT_VECTORIZER  TFIDF_VECTORIZER  TFIDF_NGRAM_1_2  \\\n",
       "Train Accuracy                    0.93              0.89             0.90   \n",
       "Train F1                          0.59              0.50             0.51   \n",
       "Train Precision                   0.49              0.38             0.39   \n",
       "Train Recall                      0.73              0.73             0.74   \n",
       "Validation Accuracy               0.91              0.88             0.88   \n",
       "Validation F1                     0.48              0.40             0.40   \n",
       "Validation Precision              0.39              0.29             0.29   \n",
       "Validation Recall                 0.62              0.62             0.61   \n",
       "\n",
       "                      TFIDF_NGRAM_2_3  \n",
       "Train Accuracy                   0.91  \n",
       "Train F1                         0.29  \n",
       "Train Precision                  0.32  \n",
       "Train Recall                     0.26  \n",
       "Validation Accuracy              0.90  \n",
       "Validation F1                    0.18  \n",
       "Validation Precision             0.19  \n",
       "Validation Recall                0.17  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest: compare vectorizers with class weight balances + lemmatizing \n",
    "rfc_metrics_bal = \\\n",
    "wrapper_compare_vectorizations(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 10, \n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10), \n",
    "                                   vectorization_list, apply_smote=False)\n",
    "pd.DataFrame(rfc_metrics_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT_VECTORIZER</th>\n",
       "      <th>TFIDF_VECTORIZER</th>\n",
       "      <th>TFIDF_NGRAM_1_2</th>\n",
       "      <th>TFIDF_NGRAM_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation F1</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Precision</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Recall</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      COUNT_VECTORIZER  TFIDF_VECTORIZER  TFIDF_NGRAM_1_2  \\\n",
       "Train Accuracy                    0.90              0.87             0.86   \n",
       "Train F1                          0.89              0.86             0.85   \n",
       "Train Precision                   0.94              0.93             0.93   \n",
       "Train Recall                      0.85              0.80             0.79   \n",
       "Validation Accuracy               0.91              0.91             0.91   \n",
       "Validation F1                     0.49              0.47             0.47   \n",
       "Validation Precision              0.38              0.37             0.37   \n",
       "Validation Recall                 0.68              0.63             0.64   \n",
       "\n",
       "                      TFIDF_NGRAM_2_3  \n",
       "Train Accuracy                   0.63  \n",
       "Train F1                         0.43  \n",
       "Train Precision                  0.97  \n",
       "Train Recall                     0.28  \n",
       "Validation Accuracy              0.93  \n",
       "Validation F1                    0.28  \n",
       "Validation Precision             0.48  \n",
       "Validation Recall                0.19  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest: compare vectorizers with upsampling + lemmatizing \n",
    "rfc_metrics_up = \\\n",
    "wrapper_compare_vectorizations(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 20, \n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10), \n",
    "                                   vectorization_list, apply_smote=False)\n",
    "pd.DataFrame(rfc_metrics_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNT_VECTORIZER</th>\n",
       "      <th>TFIDF_VECTORIZER</th>\n",
       "      <th>TFIDF_NGRAM_1_2</th>\n",
       "      <th>TFIDF_NGRAM_2_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train Accuracy</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train F1</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Precision</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Recall</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation F1</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Precision</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Recall</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      COUNT_VECTORIZER  TFIDF_VECTORIZER  TFIDF_NGRAM_1_2  \\\n",
       "Train Accuracy                    0.95              0.94             0.94   \n",
       "Train F1                          0.69              0.63             0.65   \n",
       "Train Precision                   0.62              0.54             0.56   \n",
       "Train Recall                      0.79              0.77             0.77   \n",
       "Validation Accuracy               0.93              0.90             0.90   \n",
       "Validation F1                     0.54              0.44             0.44   \n",
       "Validation Precision              0.48              0.35             0.36   \n",
       "Validation Recall                 0.62              0.58             0.59   \n",
       "\n",
       "                      TFIDF_NGRAM_2_3  \n",
       "Train Accuracy                   0.44  \n",
       "Train F1                         0.19  \n",
       "Train Precision                  0.11  \n",
       "Train Recall                     0.94  \n",
       "Validation Accuracy              0.41  \n",
       "Validation F1                    0.16  \n",
       "Validation Precision             0.09  \n",
       "Validation Recall                0.86  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest: compare vectorizers with SMOTE + lemmatizing  \n",
    "rfc_metrics_smote  = \\\n",
    "wrapper_compare_vectorizations(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 20, \n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10), \n",
    "                                   vectorization_list, apply_smote=False)\n",
    "pd.DataFrame(rfc_metrics_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_transformed, metrics_dict, val_confusion_matrix, y_val_pred, y_val_prob, pred_df = \\\n",
    "\\\n",
    "single_vector_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                    RandomForestClassifier(max_depth= 20, \n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10), count_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-Searching For Best Fit for Count Vectorizer + Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# rfc = RandomForestClassifier(n_estimators=60, max_depth=6, random_state=10, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countvect =  count_vect.fit_transform(X_train.lem_tweet)\n",
    "X_val_countvect =  count_vect.transform(X_val.lem_tweet)\n",
    "X_test_countvect = count_vect.transform(X_test.lem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=10)\n",
    "\n",
    "parameters = {'n_estimators' : [40, 60, 80, 100],\n",
    "'max_leaf_nodes' : [200, 400, 600],\n",
    "'random_state' : [10],\n",
    "'max_depth': [5, 7, 10, 20],\n",
    " 'verbose' : [0],\n",
    "'class_weight': ['balanced', 'balanced_subsample']}\n",
    "          \n",
    "rfc_gs = GridSearchCV(rfc, param_grid=parameters, cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=None,\n",
       "                                              oob_score=False, random_state=10,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'class_weight': ['balanced', 'balanced_subsample'],\n",
       "                         'max_depth': [5, 7, 10, 20],\n",
       "                         'max_leaf_nodes': [200, 400, 600],\n",
       "                         'n_estimators': [40, 60, 80, 100],\n",
       "                         'random_state': [10], 'verbose': [0]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_gs.fit(X_train_countvect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'max_depth': 20,\n",
       " 'max_leaf_nodes': 200,\n",
       " 'n_estimators': 100,\n",
       " 'random_state': 10,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=20, max_features='auto',\n",
       "                       max_leaf_nodes=200, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                       random_state=10, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc2 = RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
    "                       criterion='gini', max_depth=20, max_features='auto',\n",
    "                       max_leaf_nodes=200, min_impurity_decrease=0.0,\n",
    "                       min_impurity_split=None, min_samples_leaf=1,\n",
    "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                       n_estimators=100, n_jobs=None, oob_score=False,\n",
    "                       random_state=10, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=20, max_features='auto',\n",
       "                       max_leaf_nodes=200, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                       random_state=10, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc2.fit(X_train_countvect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7278624716185533"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_predict = rfc2.predict(X_train_countvect)\n",
    "metrics.f1_score(y_train, y_train_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_predict = rfc2.predict(X_val_countvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5694249649368864"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_val, y_val_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "columns not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-61d9ab0f621b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Feature\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplot_feature_importances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-106-61d9ab0f621b>\u001b[0m in \u001b[0;36mplot_feature_importances\u001b[0;34m(rfc2)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_countvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Feature importance\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Feature\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: columns not found"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_feature_importances(rfc2):\n",
    "    n_features = X_val_countvect.shape[1]\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.barh(range(n_features), rfc2.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), countvect.values) \n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "\n",
    "plot_feature_importances(rfc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.tokenized_tweet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train.tokenized_tweet[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X-train pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.tokenized_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train['tokenized_tweet']= X_train['tokenized_tweet'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_list = list(X_train.tokenized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_token_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_sumlist = sum(X_train_token_list,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique number of words in the training dataset is: 28638\n"
     ]
    }
   ],
   "source": [
    "X_train_unique_tokens = set(X_train_token_sumlist)\n",
    "print('The unique number of words in the training dataset is: {}'.format(len(X_train_unique_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X-val pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique number of words in the validation dataset is: 11739\n"
     ]
    }
   ],
   "source": [
    "X_val_token_list = list(X_val['tokenized_tweet'])\n",
    "X_val_token_sumlist = sum(X_val_token_list,[])\n",
    "X_val_unique_tokens = set(X_val_token_sumlist)\n",
    "\n",
    "print('The unique number of words in the validation dataset is: {}'.format(len(X_val_unique_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X-test pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique number of words in the training dataset is: 13677\n"
     ]
    }
   ],
   "source": [
    "X_test_token_list = list(X_test['tokenized_tweet'])\n",
    "X_test_token_sumlist = sum(X_test_token_list,[])\n",
    "\n",
    "X_test_unique_tokens = set(X_test_token_sumlist)\n",
    "print('The unique number of words in the training dataset is: {}'.format(len(X_test_unique_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.07 mins\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t = time()\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(X_train_token_list, sg=1, min_count=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0726 11:51:26.661659 140735734276992 base_any2vec.py:1182] Effective 'alpha' higher than previous training cycles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1059046, 1270695)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(X_train_token_list, total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save('data/w2v.model')\n",
    "\n",
    "w2v = gensim.models.Word2Vec.load('data/w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20455"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocab= w2v.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28638"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28638, 100)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0741628 ,  0.25726736, -0.28648418,  0.584742  ,  0.6637516 ,\n",
       "       -0.323804  ,  0.15741855,  0.10598789,  0.65175503,  0.58140874,\n",
       "       -0.6858151 ,  0.27742994, -0.7756751 ,  0.384504  ,  0.17189607,\n",
       "       -0.0779872 ,  0.720414  , -0.14792329,  0.3027056 ,  0.26341632,\n",
       "        0.564876  , -0.01558565, -0.2851384 , -0.5719765 ,  0.10701799,\n",
       "       -0.04947015, -0.13864604, -0.15518494, -0.53318757, -0.05348621,\n",
       "       -0.02617207, -0.3777389 , -0.5651622 ,  0.23554018, -0.5019818 ,\n",
       "        0.28997305, -0.623897  , -0.12971206, -0.18043801, -0.18210743,\n",
       "        0.1676428 , -0.13844441, -0.2695112 , -0.19630684,  0.00210546,\n",
       "        0.4490902 ,  0.22908258,  0.66579163,  0.39319432,  0.03842583,\n",
       "        0.07071815,  0.35224882, -0.01833265,  0.30844247,  0.573281  ,\n",
       "       -0.31231984,  0.16718751,  0.06626698, -0.52159023, -0.05834407,\n",
       "        0.06671209, -0.52523416,  0.00899767, -0.05377226, -0.14505701,\n",
       "       -0.6625283 , -0.15970175,  0.3465464 , -0.11873607,  0.6024351 ,\n",
       "       -0.09394083, -0.39062175,  0.14463694, -0.48668283, -0.5271387 ,\n",
       "        0.09678972,  0.1084594 ,  0.454414  , -0.61435306, -0.29905105,\n",
       "       -0.11387684,  0.52043086, -0.41144788,  0.7426081 , -0.52662545,\n",
       "       -0.625695  ,  0.40188828, -0.4948679 ,  0.84673065, -0.0125394 ,\n",
       "       -0.10079043, -0.19086188,  0.16118406,  0.2970656 , -0.09066874,\n",
       "        0.11023952,  0.00478813, -0.03985578, -0.41531447,  0.38738012],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv['trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paladino', 0.8283085823059082),\n",
       " ('carl', 0.8134002685546875),\n",
       " ('obama', 0.8051496148109436),\n",
       " ('donald', 0.7897615432739258),\n",
       " ('clinton', 0.7894325852394104),\n",
       " ('remarks', 0.7865539789199829),\n",
       " ('blm', 0.7865400314331055),\n",
       " ('comments', 0.7766364812850952),\n",
       " ('obamas', 0.7705139517784119),\n",
       " ('ally', 0.7704936265945435)]"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(['trump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clinton', 0.8898996114730835),\n",
       " ('kkk', 0.8775207996368408),\n",
       " ('bigot', 0.8729462623596191),\n",
       " ('paladino', 0.8713377714157104),\n",
       " ('misogynist', 0.8632591366767883),\n",
       " ('carl', 0.8615664839744568),\n",
       " ('homophobic', 0.8582882881164551),\n",
       " ('comments', 0.858141303062439),\n",
       " ('non', 0.8573181629180908),\n",
       " ('liar', 0.85687255859375)]"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(['racist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('brown', 0.9132056832313538),\n",
       " ('dating', 0.9076129794120789),\n",
       " ('minded', 0.907611608505249),\n",
       " ('fuckin', 0.9014075398445129),\n",
       " ('bills', 0.8978642225265503),\n",
       " ('deplorable', 0.8956817984580994),\n",
       " ('proving', 0.8955982327461243),\n",
       " ('feeding', 0.8938329815864563),\n",
       " ('letterkenny', 0.8933387994766235),\n",
       " ('besides', 0.8898042440414429)]"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=['lazy','black'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x1a4b048518>"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Word2VecKeyedVectors.get_keras_embedding of <gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x1a4b048518>>"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.get_keras_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_X = w2v.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = X_train_token_list[1]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = np.empty((20455, 100))\n",
    "for sentence in X_train_token_list:\n",
    "    np.append(X_train_w2v, np.mean([w2v[w] for w in sentence if w in w2v]\n",
    "                   or [np.zeros(100)], axis=0))\n",
    "\n",
    "X_val_w2v = np.empty((5114, 100))\n",
    "for sentence in X_val_token_list:\n",
    "    np.append(X_val_w2v, np.mean([w2v[w] for w in sentence if w in w2v]\n",
    "                   or [np.zeros(100)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_w2v[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create average vector for train and test from model\n",
    "#returned list of numpy arrays are then stacked \n",
    "\n",
    "X_train_w2v_2 = np.concatenate([avg_word_vectors(word, w2v) for word in X_train_token_list])\n",
    "\n",
    "X_val_w2v_2 = np.concatenate([avg_word_vectors(word, w2v) for word in X_val_token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.28823341,  0.07312309, -0.20851161,  0.06421986,  0.3664251 ,\n",
       "       -0.06542463,  0.00562458,  0.05938577,  0.02248305,  0.02420388,\n",
       "       -0.05223551,  0.09545344, -0.200593  ,  0.05784301,  0.01856432,\n",
       "       -0.14747404,  0.1177594 , -0.46701664, -0.13469652,  0.06984268,\n",
       "       -0.09360238,  0.23719825, -0.68535973, -0.15376803,  0.32805413,\n",
       "       -0.07435187,  0.36213338, -0.10601687,  0.17945274, -0.43607683,\n",
       "       -0.26756533,  0.32542747, -0.17665357,  0.15005742, -0.19501098,\n",
       "        0.11132066, -0.03504308, -0.12257827,  0.05847587, -0.05714871,\n",
       "        0.27701876,  0.00147894, -0.14153895, -0.08310635,  0.02631015,\n",
       "        0.07375977, -0.20184338, -0.02832522, -0.02476832, -0.08871662,\n",
       "        0.05824629,  0.15230941,  0.05233768,  0.08011377,  0.55853388,\n",
       "        0.20630752, -0.08700915,  0.15207756, -0.12323522, -0.17290489,\n",
       "       -0.12458454, -0.2057261 , -0.43881546,  0.12705594, -0.25701744,\n",
       "       -0.2308612 , -0.19199194, -0.11990811,  0.16399261,  0.03320788,\n",
       "        0.01793683, -0.15430561,  0.22409219, -0.32258257,  0.14086155,\n",
       "        0.11200625, -0.0046262 ,  0.12578172, -0.0213434 , -0.13176388,\n",
       "        0.24250354,  0.23884157,  0.20026546,  0.21086903, -0.00130135,\n",
       "       -0.09595749,  0.29260252, -0.39647596,  0.28873768,  0.06751546,\n",
       "       -0.02591443, -0.0640447 ,  0.36502416,  0.50302316, -0.08778854,\n",
       "       -0.04135202, -0.30395537, -0.1561361 , -0.40979241,  0.33322317])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_w2v_2[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-6d16e52ab1dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmote_w2v_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-50107401b501>\u001b[0m in \u001b[0;36msmote_w2v_model\u001b[0;34m(X_train_w2v, y_train, X_val_w2v, y_val, classifier)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_w2v_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \"\"\"\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_resample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 X, y, fitted_transformer = fit_resample_one_cached(\n\u001b[0;32m--> 201\u001b[0;31m                     cloned_transformer, X, y, **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    202\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_resample_one\u001b[0;34m(sampler, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fit_resample_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m     \u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         self.sampling_strategy_ = check_sampling_strategy(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicate_one_vs_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    720\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "smote_w2v_model(X_train_w2v, y_train, X_val_w2v, y_val, LogisticRegression(solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.84\n",
      "Train Precision: 0.3\n",
      "Train Recall: 0.86\n",
      "Train F1: 0.44\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.85\n",
      "Validation Precision: 0.28\n",
      "Validation Recall: 0.79\n",
      "Validation F1: 0.41\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4104</td>\n",
       "      <td>680</td>\n",
       "      <td>4784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68</td>\n",
       "      <td>262</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4172</td>\n",
       "      <td>942</td>\n",
       "      <td>5114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0    1   All\n",
       "Actual                    \n",
       "0          4104  680  4784\n",
       "1            68  262   330\n",
       "All        4172  942  5114"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smote_w2v_model(X_train_w2v_2, y_train, X_val_w2v_2, y_val, LogisticRegression(solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\n",
      "Train Precision: 0.99\n",
      "Train Recall: 1.0\n",
      "Train F1: 0.99\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.94\n",
      "Validation Precision: 0.51\n",
      "Validation Recall: 0.5\n",
      "Validation F1: 0.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4625</td>\n",
       "      <td>159</td>\n",
       "      <td>4784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165</td>\n",
       "      <td>165</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4790</td>\n",
       "      <td>324</td>\n",
       "      <td>5114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0    1   All\n",
       "Actual                    \n",
       "0          4625  159  4784\n",
       "1           165  165   330\n",
       "All        4790  324  5114"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smote_w2v_model(X_train_w2v_2, y_train, X_val_w2v_2, y_val, RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.twitter.27B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-407-ab131508714c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mglove_input_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'glove.twitter.27B.100d.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mglove_output_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/glove.txt.word2vec'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mglove2word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_input_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_output_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/scripts/glove2word2vec.py\u001b[0m in \u001b[0;36mglove2word2vec\u001b[0;34m(glove_input_file, word2vec_output_file)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mnum_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_glove_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_input_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"converting %i vectors from %s to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_input_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_output_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_output_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gensim/scripts/glove2word2vec.py\u001b[0m in \u001b[0;36mget_glove_info\u001b[0;34m(glove_file_name)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mnum_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.twitter.27B.100d.txt'"
     ]
    }
   ],
   "source": [
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# glove_input_file = 'data/glove.twitter.27B.100d.txt'\n",
    "# glove_output_file = 'data/glove.txt.word2vec'\n",
    "# glove2word2vec(glove_input_file, glove_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = KeyedVectors.load_word2vec_format('data/glove.txt.word2vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('white', 0.8178365230560303),\n",
       " ('non', 0.7734473943710327),\n",
       " ('woman', 0.7680543661117554),\n",
       " ('asian', 0.7646945714950562),\n",
       " ('hispanic', 0.7526468634605408),\n",
       " ('brown', 0.7498205304145813),\n",
       " ('racist', 0.7466655969619751),\n",
       " ('tag', 0.740607500076294),\n",
       " ('venusexchange', 0.7382000088691711),\n",
       " ('scum', 0.7306535243988037)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar('black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('white', 0.9390966892242432),\n",
       " ('blue', 0.8325154781341553),\n",
       " ('red', 0.7988946437835693),\n",
       " ('pink', 0.7783992290496826),\n",
       " ('green', 0.7753273844718933),\n",
       " ('purple', 0.7709735035896301),\n",
       " ('brown', 0.7628934383392334),\n",
       " ('yellow', 0.758215606212616),\n",
       " ('dark', 0.756466269493103),\n",
       " ('grey', 0.7383605241775513)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar('black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.8371e-01, -2.7318e-01, -4.6840e-02,  7.0274e-01, -5.6301e-01,\n",
       "       -1.7214e-01,  8.1775e-01, -6.6641e-02,  2.0224e-02, -2.3878e-01,\n",
       "       -1.6122e-02, -1.7875e-01, -4.9281e+00, -4.0272e-01, -4.1190e-01,\n",
       "       -1.4570e-01,  1.8508e-01, -1.9116e-01, -8.4608e-01,  6.5378e-01,\n",
       "        1.2370e-01,  4.8835e-02, -3.3709e-01,  2.5834e-01,  4.7729e-01,\n",
       "       -8.8145e-01, -1.3083e-01,  7.9499e-01,  8.6216e-02, -9.1803e-01,\n",
       "       -9.6671e-01, -4.8309e-01, -4.0963e-01,  1.6920e-01,  2.3241e-01,\n",
       "       -2.0524e-02,  2.2267e-01, -2.6763e-01,  1.4378e-01, -4.6547e-02,\n",
       "       -1.1726e+00, -3.5298e-02,  2.8347e-01,  4.3446e-01,  1.8472e-01,\n",
       "        3.3065e-01,  6.7848e-02, -7.6485e-01,  1.8351e-01, -6.4361e-01,\n",
       "       -7.5015e-01, -4.2583e-02,  2.4262e-01, -6.4108e-01,  6.9741e-01,\n",
       "       -2.3133e-01, -2.1454e-01,  3.1040e-01,  2.3161e-01,  6.8397e-02,\n",
       "        8.1428e-02, -3.9331e-01, -1.0927e+00, -4.2618e-02, -9.6970e-02,\n",
       "        3.1243e-01, -1.0463e+00,  2.8207e-01, -7.2446e-01,  3.1422e-01,\n",
       "        5.9191e-01, -2.7716e-01, -8.3745e-02, -3.0487e-01,  4.0743e-01,\n",
       "        1.6906e-02, -5.3021e-01, -4.0651e-01, -2.8332e-01,  3.5181e-01,\n",
       "        1.4475e+00,  1.0493e-01, -9.7307e-02, -1.4306e-01,  2.9544e-03,\n",
       "        5.3336e-01,  3.9376e-02,  5.1001e-01,  8.3593e-02,  1.3237e-01,\n",
       "        8.8598e-01,  2.8876e-01, -7.1663e-02,  3.3424e-01, -8.8094e-02,\n",
       "        4.1479e-02, -1.8501e-01, -3.2992e-01,  2.7373e-01, -1.5416e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'i',\n",
       " 'love',\n",
       " 'u',\n",
       " 'take',\n",
       " 'with',\n",
       " 'u',\n",
       " 'all',\n",
       " 'the',\n",
       " 'time',\n",
       " 'in',\n",
       " 'ur']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_token_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove2 = np.empty((20455, 100))\n",
    "for sentence in X_train_token_list:\n",
    "    np.append(X_train_glove2, np.mean([glove_model[w] for w in sentence if w in glove_model]\n",
    "                   or [np.zeros(100)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove = np.concatenate([avg_word_vectors(w, glove_model) for w in X_train_token_list])\n",
    "X_val_glove = np.concatenate([avg_word_vectors(w, glove_model) for w in X_val_token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_w2v[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.028505  , -0.04374214,  0.18586859, -0.01696486, -0.47875792,\n",
       "        0.01426772, -0.25949828,  0.34069775, -0.01975948, -0.3021883 ,\n",
       "        0.05335021,  0.33508971, -0.37468172, -0.15916829,  0.33115939,\n",
       "        0.23021236,  0.10092731, -0.01055496, -0.22143245, -0.2765775 ,\n",
       "       -0.11729609,  0.59136347,  0.0837363 ,  0.22692289,  0.14306551,\n",
       "        0.17185371, -0.31483988,  0.04305273, -0.1257824 , -0.38313468,\n",
       "        0.02947778,  0.0759358 ,  0.08734205, -0.01308978,  0.04350968,\n",
       "        0.10200389,  0.14046779, -0.45173085, -0.07938617, -0.34419653,\n",
       "       -0.16534324,  0.14472922,  0.28601627,  0.13695804,  0.16271638,\n",
       "       -0.12347816,  0.13635907, -0.36715094, -0.00888368,  0.11414939,\n",
       "       -0.15993796,  0.0929606 , -0.41119229, -0.02858371, -0.12876672,\n",
       "       -0.03553657, -0.13776215,  0.31243446,  0.28799529, -0.03698096,\n",
       "        0.05387536,  0.2913464 , -0.40524991, -0.08885277, -0.24015333,\n",
       "       -0.11776452, -0.12920914, -0.20915868, -0.19259807,  0.35950353,\n",
       "       -0.22930713, -0.12574733,  0.01357989, -0.12591634, -0.48147522,\n",
       "        0.11510526, -0.16485232,  0.05467255, -0.50577471,  0.02670927,\n",
       "       -0.00295293, -0.14812569, -0.23803934,  0.10216917, -0.04794974,\n",
       "       -0.27021588,  0.20741428,  0.05237857, -0.23213364,  0.19121008,\n",
       "       -0.02326109,  0.0261383 ,  0.03599726,  0.13088754,  0.09788964,\n",
       "       -0.09242559,  0.11011475, -0.07399436,  0.13657168, -0.09666208])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_w2v_2[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove[255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.88\n",
      "Train Precision: 0.01\n",
      "Train Recall: 0.01\n",
      "Train F1: 0.01\n",
      "\n",
      "\n",
      "Validation Accuracy: 0.89\n",
      "Validation Precision: 0.02\n",
      "Validation Recall: 0.01\n",
      "Validation F1: 0.01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4537</td>\n",
       "      <td>247</td>\n",
       "      <td>4784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>326</td>\n",
       "      <td>4</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4863</td>\n",
       "      <td>251</td>\n",
       "      <td>5114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     0    1   All\n",
       "Actual                    \n",
       "0          4537  247  4784\n",
       "1           326    4   330\n",
       "All        4863  251  5114"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, LogisticRegression (class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Testing Scraped Trump Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>no_hash_tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lem_tweet</th>\n",
       "      <th>stem_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>So Robert Mueller has now asked for his long t...</td>\n",
       "      <td>Robert Mueller asked long time Never Trumper l...</td>\n",
       "      <td>Robert Mueller asked long time Never Trumper l...</td>\n",
       "      <td>['Robert', 'Mueller', 'asked', 'long', 'time',...</td>\n",
       "      <td>['robert', 'mueller', 'ask', 'long', 'time', '...</td>\n",
       "      <td>['Robert', 'Mueller', 'asked', 'long', 'time',...</td>\n",
       "      <td>Robert Mueller asked long time Never Trumper l...</td>\n",
       "      <td>robert mueller asked long time never trumper l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The young leaders here today (@TPUSA) are part...</td>\n",
       "      <td>young leaders here today part movement unlike ...</td>\n",
       "      <td>young leaders here today part movement unlike ...</td>\n",
       "      <td>['young', 'leaders', 'here', 'today', 'part', ...</td>\n",
       "      <td>['young', 'leader', 'here', 'today', 'part', '...</td>\n",
       "      <td>['young', 'leader', 'here', 'today', 'part', '...</td>\n",
       "      <td>young leaders here today part movement unlike ...</td>\n",
       "      <td>young leaders here today part movement unlike ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Congratulations to our new Secretary of Defens...</td>\n",
       "      <td>Congratulations Secretary Defense Mark twitter</td>\n",
       "      <td>Congratulations Secretary Defense Mark twitter</td>\n",
       "      <td>['Congratulations', 'Secretary', 'Defense', 'M...</td>\n",
       "      <td>['congratul', 'secretari', 'defens', 'mark', '...</td>\n",
       "      <td>['Congratulations', 'Secretary', 'Defense', 'M...</td>\n",
       "      <td>Congratulations Secretary Defense Mark twitter</td>\n",
       "      <td>congratulations secretary defense mark twitt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Oh well, we still have the great @seanhannity ...</td>\n",
       "      <td>well still have great hear really strong show ...</td>\n",
       "      <td>well still have great hear really strong show ...</td>\n",
       "      <td>['well', 'still', 'have', 'great', 'hear', 're...</td>\n",
       "      <td>['well', 'still', 'have', 'great', 'hear', 're...</td>\n",
       "      <td>['well', 'still', 'have', 'great', 'hear', 're...</td>\n",
       "      <td>well still have great hear really strong show ...</td>\n",
       "      <td>well still have great hear really strong show ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>THANK YOU @TPUSA! #MAGApic.twitter.com/1eqR2Im8dQ</td>\n",
       "      <td>THANK #MAGApic twitter</td>\n",
       "      <td>THANK MAGApic twitter</td>\n",
       "      <td>['THANK', 'MAGApic', 'twitter']</td>\n",
       "      <td>['thank', 'magap', 'twitter']</td>\n",
       "      <td>['THANK', 'MAGApic', 'twitter']</td>\n",
       "      <td>THANK MAGApic twitter</td>\n",
       "      <td>thank magapic twitt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              tweet  \\\n",
       "0           0  So Robert Mueller has now asked for his long t...   \n",
       "1           1  The young leaders here today (@TPUSA) are part...   \n",
       "2           2  Congratulations to our new Secretary of Defens...   \n",
       "3           3  Oh well, we still have the great @seanhannity ...   \n",
       "4           4  THANK YOU @TPUSA! #MAGApic.twitter.com/1eqR2Im8dQ   \n",
       "\n",
       "                                          tidy_tweet  \\\n",
       "0  Robert Mueller asked long time Never Trumper l...   \n",
       "1  young leaders here today part movement unlike ...   \n",
       "2     Congratulations Secretary Defense Mark twitter   \n",
       "3  well still have great hear really strong show ...   \n",
       "4                             THANK #MAGApic twitter   \n",
       "\n",
       "                                       no_hash_tweet  \\\n",
       "0  Robert Mueller asked long time Never Trumper l...   \n",
       "1  young leaders here today part movement unlike ...   \n",
       "2     Congratulations Secretary Defense Mark twitter   \n",
       "3  well still have great hear really strong show ...   \n",
       "4                              THANK MAGApic twitter   \n",
       "\n",
       "                                     tokenized_tweet  \\\n",
       "0  ['Robert', 'Mueller', 'asked', 'long', 'time',...   \n",
       "1  ['young', 'leaders', 'here', 'today', 'part', ...   \n",
       "2  ['Congratulations', 'Secretary', 'Defense', 'M...   \n",
       "3  ['well', 'still', 'have', 'great', 'hear', 're...   \n",
       "4                    ['THANK', 'MAGApic', 'twitter']   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  ['robert', 'mueller', 'ask', 'long', 'time', '...   \n",
       "1  ['young', 'leader', 'here', 'today', 'part', '...   \n",
       "2  ['congratul', 'secretari', 'defens', 'mark', '...   \n",
       "3  ['well', 'still', 'have', 'great', 'hear', 're...   \n",
       "4                      ['thank', 'magap', 'twitter']   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  ['Robert', 'Mueller', 'asked', 'long', 'time',...   \n",
       "1  ['young', 'leader', 'here', 'today', 'part', '...   \n",
       "2  ['Congratulations', 'Secretary', 'Defense', 'M...   \n",
       "3  ['well', 'still', 'have', 'great', 'hear', 're...   \n",
       "4                    ['THANK', 'MAGApic', 'twitter']   \n",
       "\n",
       "                                           lem_tweet  \\\n",
       "0  Robert Mueller asked long time Never Trumper l...   \n",
       "1  young leaders here today part movement unlike ...   \n",
       "2     Congratulations Secretary Defense Mark twitter   \n",
       "3  well still have great hear really strong show ...   \n",
       "4                              THANK MAGApic twitter   \n",
       "\n",
       "                                          stem_tweet  \n",
       "0  robert mueller asked long time never trumper l...  \n",
       "1  young leaders here today part movement unlike ...  \n",
       "2       congratulations secretary defense mark twitt  \n",
       "3  well still have great hear really strong show ...  \n",
       "4                                thank magapic twitt  "
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_df= pd.read_csv('data/cleaned-trump-tweet.csv')\n",
    "trump_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countvect =  count_vect.fit_transform(X_train_up.lem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train_countvect, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trump = count_vect.transform(trump_df.lem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trump = X_trump.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 28610)"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trump.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trump_predict = logreg.predict(X_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trump_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df['predictions'] = y_trump_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trump_predict_prob = logreg.predict_proba(X_trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trump_predict_prob = pd.DataFrame(y_trump_predict_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df['predict_probability'] = y_trump_predict_prob[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df = trump_df[['tweet','predictions', 'predict_probability']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump0 = trump_df[trump_df.predictions == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     So Robert Mueller has now asked for his long time Never Trumper lawyer to sit beside him and help with answers. What’s this all about? His lawyer represented the “basement server guy” who got off free in the Crooked Hillary case. This should NOT be allowed. Rigged Witch Hunt!                        \n",
       "1     The young leaders here today (@TPUSA) are part of a movement unlike anything in the history of our nation. It is a movement about reclaiming YOUR future, rebuilding YOUR Country, restoring YOUR destiny, and renewing the values that are the true source of American GREATNESS!pic.twitter.com/unURpLS8Lc\n",
       "2     Congratulations to our new Secretary of Defense, Mark @EsperDoD!pic.twitter.com/aT3D1ZX5KK                                                                                                                                                                                                                  \n",
       "3     Oh well, we still have the great @seanhannity who I hear has a really strong show tonight. 9:00 P.M.                                                                                                                                                                                                        \n",
       "4     THANK YOU @TPUSA! #MAGApic.twitter.com/1eqR2Im8dQ                                                                                                                                                                                                                                                           \n",
       "5     Just watched Rep. Eric Swalwell be asked endless softball questions by @marthamaccallum on @FoxNews about the phony Witch Hunt. He was just forced out of the Democrat Presidential Primary because he polled at ZERO. Fox sure ain’t what it used to be. Too bad!                                          \n",
       "6     Just got back only to hear of a last minute change allowing a Never Trumper attorney to help Robert Mueller with his testimony before Congress tomorrow. What a disgrace to our system. Never heard of this before. VERY UNFAIR, SHOULD NOT BE ALLOWED. A rigged Witch Hunt!                                \n",
       "8     ....I got to know him over his many years as Chairman of the Police Athletic League, for which he devoted so much time and energy. Bob Morgenthau, a legend, will be greatly missed!                                                                                                                        \n",
       "9     I was saddened to learn of the recent passing of Bob Morgenthau, a truly great man! Bob served as a Naval Officer in World War II, was an extraordinary US Attorney, Manhattan District Attorney, and always a warrior for our Country that he loved so dearly....                                          \n",
       "10    Leaving for Turning Point USA. Will be speaking to some of the greatest and smartest young people on the planet. See you there!                                                                                                                                                                             \n",
       "13    ....Tariffs, Remittance Fees, or all of the above. Guatemala has not been good. Big U.S. taxpayer dollars going to them was cut off by me 9 months ago.                                                                                                                                                     \n",
       "14    Guatemala, which has been forming Caravans and sending large numbers of people, some with criminal records, to the United States, has decided to break the deal they had with us on signing a necessary Safe Third Agreement. We were ready to go. Now we are looking at the “BAN,”....                     \n",
       "15    Farmers are starting to do great again, after 15 years of a downward spiral. The 16 Billion Dollar China “replacement” money didn’t exactly hurt!                                                                                                                                                           \n",
       "17    Congratulations to Boris Johnson on becoming the new Prime Minister of the United Kingdom. He will be great!                                                                                                                                                                                                \n",
       "18    ....Best and Newest Military (almost totally rebuilt from the depleted military I took over) in History, Best V.A. in History (Choice), and MUCH, MUCH MORE. Gee, let’s impeach the President. The “Squad” (AOC Plus 3) and other Dems suffer from Trump Derangement Syndrome. Crazy!                       \n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump0.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>predictions</th>\n",
       "      <th>predict_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So Robert Mueller has now asked for his long time Never Trumper lawyer to sit beside him and help with answers. What’s this all about? His lawyer represented the “basement server guy” who got off free in the Crooked Hillary case. This should NOT be allowed. Rigged Witch Hunt!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The young leaders here today (@TPUSA) are part of a movement unlike anything in the history of our nation. It is a movement about reclaiming YOUR future, rebuilding YOUR Country, restoring YOUR destiny, and renewing the values that are the true source of American GREATNESS!pic.twitter.com/unURpLS8Lc</td>\n",
       "      <td>0</td>\n",
       "      <td>0.870890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Congratulations to our new Secretary of Defense, Mark @EsperDoD!pic.twitter.com/aT3D1ZX5KK</td>\n",
       "      <td>0</td>\n",
       "      <td>0.978122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oh well, we still have the great @seanhannity who I hear has a really strong show tonight. 9:00 P.M.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THANK YOU @TPUSA! #MAGApic.twitter.com/1eqR2Im8dQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0.937782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Just watched Rep. Eric Swalwell be asked endless softball questions by @marthamaccallum on @FoxNews about the phony Witch Hunt. He was just forced out of the Democrat Presidential Primary because he polled at ZERO. Fox sure ain’t what it used to be. Too bad!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.953525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Just got back only to hear of a last minute change allowing a Never Trumper attorney to help Robert Mueller with his testimony before Congress tomorrow. What a disgrace to our system. Never heard of this before. VERY UNFAIR, SHOULD NOT BE ALLOWED. A rigged Witch Hunt!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Budget Deal gives great victories to our Military and Vets, keeps out Democrat poison pill riders. Republicans and Democrats in Congress need to act ASAP and support this deal.</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>....I got to know him over his many years as Chairman of the Police Athletic League, for which he devoted so much time and energy. Bob Morgenthau, a legend, will be greatly missed!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.977587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I was saddened to learn of the recent passing of Bob Morgenthau, a truly great man! Bob served as a Naval Officer in World War II, was an extraordinary US Attorney, Manhattan District Attorney, and always a warrior for our Country that he loved so dearly....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.840154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Leaving for Turning Point USA. Will be speaking to some of the greatest and smartest young people on the planet. See you there!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.972643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>When an old Wall at the Southern Border, that is crumbling and falling over, built in an important section to keep out problems, is replaced with a brand new 30 foot high steel and concrete Wall, the Media says no new Wall has been built. Fake News! Building lots of Wall!</td>\n",
       "      <td>1</td>\n",
       "      <td>0.053194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>In 2016 I almost won Minnesota. In 2020, because of America hating anti-Semite Rep. Omar, &amp; the fact that Minnesota is having its best economic year ever, I will win the State! “We are going to be a nightmare to the President,” she say. No, AOC Plus 3 are a Nightmare for America!</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>....Tariffs, Remittance Fees, or all of the above. Guatemala has not been good. Big U.S. taxpayer dollars going to them was cut off by me 9 months ago.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.917712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Guatemala, which has been forming Caravans and sending large numbers of people, some with criminal records, to the United States, has decided to break the deal they had with us on signing a necessary Safe Third Agreement. We were ready to go. Now we are looking at the “BAN,”....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Farmers are starting to do great again, after 15 years of a downward spiral. The 16 Billion Dollar China “replacement” money didn’t exactly hurt!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.958040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>KEEP AMERICA GREAT!</td>\n",
       "      <td>1</td>\n",
       "      <td>0.370960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Congratulations to Boris Johnson on becoming the new Prime Minister of the United Kingdom. He will be great!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>....Best and Newest Military (almost totally rebuilt from the depleted military I took over) in History, Best V.A. in History (Choice), and MUCH, MUCH MORE. Gee, let’s impeach the President. The “Squad” (AOC Plus 3) and other Dems suffer from Trump Derangement Syndrome. Crazy!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.554673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Newest Poll: Only 11% in favor of starting ridiculous impeachment hearings. Well, let’s see: We have the Best Economy in History, the Best Employment Numbers in History, Most People Working in History, Highest Stock Market in History, Biggest Tax and Regulation Cuts in History,..</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                           tweet  \\\n",
       "0   So Robert Mueller has now asked for his long time Never Trumper lawyer to sit beside him and help with answers. What’s this all about? His lawyer represented the “basement server guy” who got off free in the Crooked Hillary case. This should NOT be allowed. Rigged Witch Hunt!                           \n",
       "1   The young leaders here today (@TPUSA) are part of a movement unlike anything in the history of our nation. It is a movement about reclaiming YOUR future, rebuilding YOUR Country, restoring YOUR destiny, and renewing the values that are the true source of American GREATNESS!pic.twitter.com/unURpLS8Lc   \n",
       "2   Congratulations to our new Secretary of Defense, Mark @EsperDoD!pic.twitter.com/aT3D1ZX5KK                                                                                                                                                                                                                     \n",
       "3   Oh well, we still have the great @seanhannity who I hear has a really strong show tonight. 9:00 P.M.                                                                                                                                                                                                           \n",
       "4   THANK YOU @TPUSA! #MAGApic.twitter.com/1eqR2Im8dQ                                                                                                                                                                                                                                                              \n",
       "5   Just watched Rep. Eric Swalwell be asked endless softball questions by @marthamaccallum on @FoxNews about the phony Witch Hunt. He was just forced out of the Democrat Presidential Primary because he polled at ZERO. Fox sure ain’t what it used to be. Too bad!                                             \n",
       "6   Just got back only to hear of a last minute change allowing a Never Trumper attorney to help Robert Mueller with his testimony before Congress tomorrow. What a disgrace to our system. Never heard of this before. VERY UNFAIR, SHOULD NOT BE ALLOWED. A rigged Witch Hunt!                                   \n",
       "7   Budget Deal gives great victories to our Military and Vets, keeps out Democrat poison pill riders. Republicans and Democrats in Congress need to act ASAP and support this deal.                                                                                                                               \n",
       "8   ....I got to know him over his many years as Chairman of the Police Athletic League, for which he devoted so much time and energy. Bob Morgenthau, a legend, will be greatly missed!                                                                                                                           \n",
       "9   I was saddened to learn of the recent passing of Bob Morgenthau, a truly great man! Bob served as a Naval Officer in World War II, was an extraordinary US Attorney, Manhattan District Attorney, and always a warrior for our Country that he loved so dearly....                                             \n",
       "10  Leaving for Turning Point USA. Will be speaking to some of the greatest and smartest young people on the planet. See you there!                                                                                                                                                                                \n",
       "11  When an old Wall at the Southern Border, that is crumbling and falling over, built in an important section to keep out problems, is replaced with a brand new 30 foot high steel and concrete Wall, the Media says no new Wall has been built. Fake News! Building lots of Wall!                               \n",
       "12  In 2016 I almost won Minnesota. In 2020, because of America hating anti-Semite Rep. Omar, & the fact that Minnesota is having its best economic year ever, I will win the State! “We are going to be a nightmare to the President,” she say. No, AOC Plus 3 are a Nightmare for America!                       \n",
       "13  ....Tariffs, Remittance Fees, or all of the above. Guatemala has not been good. Big U.S. taxpayer dollars going to them was cut off by me 9 months ago.                                                                                                                                                        \n",
       "14  Guatemala, which has been forming Caravans and sending large numbers of people, some with criminal records, to the United States, has decided to break the deal they had with us on signing a necessary Safe Third Agreement. We were ready to go. Now we are looking at the “BAN,”....                        \n",
       "15  Farmers are starting to do great again, after 15 years of a downward spiral. The 16 Billion Dollar China “replacement” money didn’t exactly hurt!                                                                                                                                                              \n",
       "16  KEEP AMERICA GREAT!                                                                                                                                                                                                                                                                                            \n",
       "17  Congratulations to Boris Johnson on becoming the new Prime Minister of the United Kingdom. He will be great!                                                                                                                                                                                                   \n",
       "18  ....Best and Newest Military (almost totally rebuilt from the depleted military I took over) in History, Best V.A. in History (Choice), and MUCH, MUCH MORE. Gee, let’s impeach the President. The “Squad” (AOC Plus 3) and other Dems suffer from Trump Derangement Syndrome. Crazy!                          \n",
       "19  Newest Poll: Only 11% in favor of starting ridiculous impeachment hearings. Well, let’s see: We have the Best Economy in History, the Best Employment Numbers in History, Most People Working in History, Highest Stock Market in History, Biggest Tax and Regulation Cuts in History,..                       \n",
       "\n",
       "    predictions  predict_probability  \n",
       "0   0            0.955340             \n",
       "1   0            0.870890             \n",
       "2   0            0.978122             \n",
       "3   0            0.999288             \n",
       "4   0            0.937782             \n",
       "5   0            0.953525             \n",
       "6   0            0.999638             \n",
       "7   1            0.032784             \n",
       "8   0            0.977587             \n",
       "9   0            0.840154             \n",
       "10  0            0.972643             \n",
       "11  1            0.053194             \n",
       "12  1            0.008696             \n",
       "13  0            0.917712             \n",
       "14  0            0.999320             \n",
       "15  0            0.958040             \n",
       "16  1            0.370960             \n",
       "17  0            0.955888             \n",
       "18  0            0.554673             \n",
       "19  1            0.019639             "
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>predictions</th>\n",
       "      <th>predict_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So Robert Mueller has now asked for his long time Never Trumper lawyer to sit beside him and help with answers. What’s this all about? His lawyer represented the “basement server guy” who got off free in the Crooked Hillary case. This should NOT be allowed. Rigged Witch Hunt!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The young leaders here today (@TPUSA) are part of a movement unlike anything in the history of our nation. It is a movement about reclaiming YOUR future, rebuilding YOUR Country, restoring YOUR destiny, and renewing the values that are the true source of American GREATNESS!pic.twitter.com/unURpLS8Lc</td>\n",
       "      <td>0</td>\n",
       "      <td>0.870890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Congratulations to our new Secretary of Defense, Mark @EsperDoD!pic.twitter.com/aT3D1ZX5KK</td>\n",
       "      <td>0</td>\n",
       "      <td>0.978122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oh well, we still have the great @seanhannity who I hear has a really strong show tonight. 9:00 P.M.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THANK YOU @TPUSA! #MAGApic.twitter.com/1eqR2Im8dQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0.937782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Just watched Rep. Eric Swalwell be asked endless softball questions by @marthamaccallum on @FoxNews about the phony Witch Hunt. He was just forced out of the Democrat Presidential Primary because he polled at ZERO. Fox sure ain’t what it used to be. Too bad!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.953525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Just got back only to hear of a last minute change allowing a Never Trumper attorney to help Robert Mueller with his testimony before Congress tomorrow. What a disgrace to our system. Never heard of this before. VERY UNFAIR, SHOULD NOT BE ALLOWED. A rigged Witch Hunt!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>....I got to know him over his many years as Chairman of the Police Athletic League, for which he devoted so much time and energy. Bob Morgenthau, a legend, will be greatly missed!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.977587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I was saddened to learn of the recent passing of Bob Morgenthau, a truly great man! Bob served as a Naval Officer in World War II, was an extraordinary US Attorney, Manhattan District Attorney, and always a warrior for our Country that he loved so dearly....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.840154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Leaving for Turning Point USA. Will be speaking to some of the greatest and smartest young people on the planet. See you there!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.972643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>....Tariffs, Remittance Fees, or all of the above. Guatemala has not been good. Big U.S. taxpayer dollars going to them was cut off by me 9 months ago.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.917712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Guatemala, which has been forming Caravans and sending large numbers of people, some with criminal records, to the United States, has decided to break the deal they had with us on signing a necessary Safe Third Agreement. We were ready to go. Now we are looking at the “BAN,”....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Farmers are starting to do great again, after 15 years of a downward spiral. The 16 Billion Dollar China “replacement” money didn’t exactly hurt!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.958040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Congratulations to Boris Johnson on becoming the new Prime Minister of the United Kingdom. He will be great!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>....Best and Newest Military (almost totally rebuilt from the depleted military I took over) in History, Best V.A. in History (Choice), and MUCH, MUCH MORE. Gee, let’s impeach the President. The “Squad” (AOC Plus 3) and other Dems suffer from Trump Derangement Syndrome. Crazy!</td>\n",
       "      <td>0</td>\n",
       "      <td>0.554673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                           tweet  \\\n",
       "0   So Robert Mueller has now asked for his long time Never Trumper lawyer to sit beside him and help with answers. What’s this all about? His lawyer represented the “basement server guy” who got off free in the Crooked Hillary case. This should NOT be allowed. Rigged Witch Hunt!                           \n",
       "1   The young leaders here today (@TPUSA) are part of a movement unlike anything in the history of our nation. It is a movement about reclaiming YOUR future, rebuilding YOUR Country, restoring YOUR destiny, and renewing the values that are the true source of American GREATNESS!pic.twitter.com/unURpLS8Lc   \n",
       "2   Congratulations to our new Secretary of Defense, Mark @EsperDoD!pic.twitter.com/aT3D1ZX5KK                                                                                                                                                                                                                     \n",
       "3   Oh well, we still have the great @seanhannity who I hear has a really strong show tonight. 9:00 P.M.                                                                                                                                                                                                           \n",
       "4   THANK YOU @TPUSA! #MAGApic.twitter.com/1eqR2Im8dQ                                                                                                                                                                                                                                                              \n",
       "5   Just watched Rep. Eric Swalwell be asked endless softball questions by @marthamaccallum on @FoxNews about the phony Witch Hunt. He was just forced out of the Democrat Presidential Primary because he polled at ZERO. Fox sure ain’t what it used to be. Too bad!                                             \n",
       "6   Just got back only to hear of a last minute change allowing a Never Trumper attorney to help Robert Mueller with his testimony before Congress tomorrow. What a disgrace to our system. Never heard of this before. VERY UNFAIR, SHOULD NOT BE ALLOWED. A rigged Witch Hunt!                                   \n",
       "8   ....I got to know him over his many years as Chairman of the Police Athletic League, for which he devoted so much time and energy. Bob Morgenthau, a legend, will be greatly missed!                                                                                                                           \n",
       "9   I was saddened to learn of the recent passing of Bob Morgenthau, a truly great man! Bob served as a Naval Officer in World War II, was an extraordinary US Attorney, Manhattan District Attorney, and always a warrior for our Country that he loved so dearly....                                             \n",
       "10  Leaving for Turning Point USA. Will be speaking to some of the greatest and smartest young people on the planet. See you there!                                                                                                                                                                                \n",
       "13  ....Tariffs, Remittance Fees, or all of the above. Guatemala has not been good. Big U.S. taxpayer dollars going to them was cut off by me 9 months ago.                                                                                                                                                        \n",
       "14  Guatemala, which has been forming Caravans and sending large numbers of people, some with criminal records, to the United States, has decided to break the deal they had with us on signing a necessary Safe Third Agreement. We were ready to go. Now we are looking at the “BAN,”....                        \n",
       "15  Farmers are starting to do great again, after 15 years of a downward spiral. The 16 Billion Dollar China “replacement” money didn’t exactly hurt!                                                                                                                                                              \n",
       "17  Congratulations to Boris Johnson on becoming the new Prime Minister of the United Kingdom. He will be great!                                                                                                                                                                                                   \n",
       "18  ....Best and Newest Military (almost totally rebuilt from the depleted military I took over) in History, Best V.A. in History (Choice), and MUCH, MUCH MORE. Gee, let’s impeach the President. The “Squad” (AOC Plus 3) and other Dems suffer from Trump Derangement Syndrome. Crazy!                          \n",
       "\n",
       "    predictions  predict_probability  \n",
       "0   0            0.955340             \n",
       "1   0            0.870890             \n",
       "2   0            0.978122             \n",
       "3   0            0.999288             \n",
       "4   0            0.937782             \n",
       "5   0            0.953525             \n",
       "6   0            0.999638             \n",
       "8   0            0.977587             \n",
       "9   0            0.840154             \n",
       "10  0            0.972643             \n",
       "13  0            0.917712             \n",
       "14  0            0.999320             \n",
       "15  0            0.958040             \n",
       "17  0            0.955888             \n",
       "18  0            0.554673             "
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_df[trump_df.predictions == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump1 = trump_df[trump_df.predictions == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7     Budget Deal gives great victories to our Military and Vets, keeps out Democrat poison pill riders. Republicans and Democrats in Congress need to act ASAP and support this deal.                                                                                                        \n",
       "11    When an old Wall at the Southern Border, that is crumbling and falling over, built in an important section to keep out problems, is replaced with a brand new 30 foot high steel and concrete Wall, the Media says no new Wall has been built. Fake News! Building lots of Wall!        \n",
       "12    In 2016 I almost won Minnesota. In 2020, because of America hating anti-Semite Rep. Omar, & the fact that Minnesota is having its best economic year ever, I will win the State! “We are going to be a nightmare to the President,” she say. No, AOC Plus 3 are a Nightmare for America!\n",
       "16    KEEP AMERICA GREAT!                                                                                                                                                                                                                                                                     \n",
       "19    Newest Poll: Only 11% in favor of starting ridiculous impeachment hearings. Well, let’s see: We have the Best Economy in History, the Best Employment Numbers in History, Most People Working in History, Highest Stock Market in History, Biggest Tax and Regulation Cuts in History,..\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump1.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
